\documentclass[dvipsnames,12pt]{article}
%\VignetteIndexEntry{epigrowthfit-vignette}
%\VignetteEngine{knitr::knitr}


% Page layout
\usepackage[top=1in,bottom=1.5in,left=1in,right=1in]{geometry}
\usepackage{lineno} % line numbering
\renewcommand{\linenumberfont}{\normalfont\tiny\sffamily\color[gray]{0.5}}
\hfuzz=1in % tolerate \hbox fullness
\vbadness=\maxdimen % tolerate \vbox badness

% Text layout
%\usepackage{setspace} % \onehalfspacing, \doublespacing
%\raggedright
\usepackage[T1]{fontenc} % words with accented characters can be hyphenated

% Title page setup
\title{\Large The \pkg{epigrowthfit} package}
\author{Mikael Jagan, Benjamin M.\ Bolker, Junling Ma,\\ David J.\ D.\ Earn, Jonathan Dushoff}
\date{\today}

% Deferred execution
% front matter
\usepackage{etoolbox} % document hooks
\AfterEndPreamble{%
  \pagenumbering{roman}
  \maketitle
  \tableofcontents
  \thispagestyle{empty}
  \clearpage
  \pagenumbering{arabic}
  \linenumbers
}
% back matter
\AtEndDocument{%
  \bibliographystyle{vancouver}
  \bibliography{epigrowthfit-vignette}
}

% Math
\usepackage{amsmath,mathtools}
\usepackage{amssymb,bm,bbm}
\allowdisplaybreaks % page breaks in display style math mode

% Code
\usepackage[scaled=0.85]{DejaVuSansMono}
% Match inline code highlighting to custom knitr theme
% specified in `knit_theme.css`
\definecolor{background}{HTML}{f4f4f4} % background
\definecolor{num}{HTML}{aa4499} % numeric, logical, NA
\definecolor{str}{HTML}{999933} % character
\definecolor{com}{HTML}{999999} % comment
\definecolor{opt}{HTML}{555555} % !?
\definecolor{std}{HTML}{555555} % variable name, operator, delimiter
\definecolor{kwa}{HTML}{aa4499} % function, if, else, for, in, while, NULL
\definecolor{kwb}{HTML}{555555} % assignment operator
\definecolor{kwc}{HTML}{555555} % function argument
\definecolor{kwd}{HTML}{3a9183} % function name
\usepackage{listings} % \lstinline
\lstset{%
  basicstyle=\color{std}\ttfamily,%
  breaklines=true,%
  moredelim=[is][\color{num}]{`1}{`1},
  moredelim=[is][\color{str}]{`2}{`2},
  moredelim=[is][\color{com}\itshape]{`3}{`3},
  moredelim=[is][\color{opt}]{`4}{`4},
  moredelim=[is][\color{std}]{`5}{`5},
  moredelim=[is][\color{kwa}\bfseries]{`6}{`6},
  moredelim=[is][\color{kwb}]{`7}{`7},
  moredelim=[is][\color{kwc}]{`8}{`8},
  moredelim=[is][\color{kwd}]{`9}{`9}
}

% Float captions
\usepackage{caption}
\captionsetup{%
  aboveskip=8pt,%
  labelfont=bf,%
  labelsep=period,%
  justification=raggedright,%
  singlelinecheck=false%
}
\renewcommand{\figurename}{Fig}

% Float placement
\usepackage{float} % \begin{figure}[H]
\usepackage[section]{placeins} % \FloatBarrier

% Lists
\usepackage{enumitem}
\setlist[enumerate]{label=(\roman*)}
\setlist[itemize]{label=\tiny$\blacksquare$}

% Tables
\usepackage{booktabs} % \toprule, \midrule, \bottomrule, \addlinespace
\usepackage{array}
% columns with variable width, top alignment
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

% Citation
\usepackage{cite}
\makeatletter
\renewcommand{\@biblabel}[1]{\quad #1.} % \@biblabel format
\makeatother

% Ref hyperlinks
\usepackage[colorlinks=true,allcolors=magenta]{hyperref}
\usepackage[nameinlink,capitalize]{cleveref}
% equation
\crefformat{equation}{#2Eq~#1#3}
\crefmultiformat{equation}{#2Eqs~#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{equation}{#3Eqs~#1#4--#5#2#6}
\crefformat{blankequation}{#2#1#3}
\crefmultiformat{blankequation}{#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{blankequation}{#3#1#4--#5#2#6}
\crefalias{blankequation}{equation}
\crefformat{pluralequation}{#2Eqs~#1#3}
\crefalias{pluralequation}{equation}
% figure
\crefformat{figure}{#2Fig~#1#3}
\crefmultiformat{figure}{#2Figs~#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{figure}{#3Figs~#1#4--#5#2#6}
% section
\crefformat{section}{#2\S#1#3}
\crefmultiformat{section}{#2\S\S#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{section}{#3\S\S#1#4--#5#2#6}
% table
\crefformat{Table}{#2Table~#1#3}
\crefmultiformat{table}{#2Tables~#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{table}{#3Tables~#1#4--#5#2#6}

% More macros
% laziness
\let\tops\texorpdfstring
% fonts
\renewcommand{\vec}[1]{\bm{\mathrm{#1}}}
% diacritics
\let\wh\widehat
\let\wt\widetilde
% delimiters
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
% operators
\DeclareMathOperator*{\argmin}{arg\,min}
% symbols
\newcommand{\thalf}{t_\text{\normalfont half}}
\newcommand{\R}{\mathcal{R}}
% abbreviations
\newcommand{\cf}{\textit{cf}.~}
\newcommand{\eg}{\textit{e}.\textit{g}.,~}
\newcommand{\ie}{\textit{i}.\textit{e}.,~}
\newcommand{\etc}{\textit{etc}.}
\newcommand{\etal}{\textit{et al}.}
% code
\newcommand{\code}[1]{\lstinline|#1|}
\newcommand{\fun}[1]{\code{`9#1`9()}}
\let\pkg\textbf
% comments
\newcommand{\comment}[3]{\textcolor{#1}{\textbf{[#2: }\textit{#3}\textbf{]}}}
\newcommand{\david}[1]{\comment{magenta}{DE}{#1}}
\newcommand{\mikael}[1]{\comment{blue}{MJ}{#1}}

%%%%%%%%%%%%%%%%
%% START HERE %%
%%%%%%%%%%%%%%%%

\begin{document}
\setlength{\parskip}{0.5mm}
\setlength{\parindent}{7mm}

<<set-chunk-defaults, echo=FALSE>>=
library(knitr)
## Make some substitutions in the .tex output in order to:
## * Prevent lineno from messing up breaking of chunks over pages.
## * Prevent automatic indentation after chunks.
## * Dispense with compile errors due to knitr-xcolor interaction.
##   See https://tex.stackexchange.com/questions/148188/.
knit_hooks$set(document = function(x) {
  x <- sub("\\begin{knitrout}", "\\nolinenumbers\\begin{knitrout}", x, fixed = TRUE)
  x <- sub("\\end{knitrout}", "\\end{knitrout}\\linenumbers\\noindent", x, fixed = TRUE)
  sub("\\usepackage[]{color}", "\\usepackage{xcolor}", x, fixed = TRUE)
})
## Set number of digits printed in chunk output with chunk option "digits"
knit_hooks$set(digits = function(before, options, envir) {
  if (before) {
    options(digits = options$digits)
  }
})
## Set character width of chunk with chunk option "width"
## (can be used to prevent intrusions into right margin)
knit_hooks$set(char.width = function(before, options, envir) {
  if (before) {
    options(width = options$char.width)
  }
})
## Use custom palette for code highlighting
knit_theme$set(knit_theme$get("knit_theme.css"))
## Set chunk defaults
opts_chunk$set(
  cache = TRUE, # if `FALSE`, chunk is evaluated from scratch with compile
  echo = TRUE, # if `FALSE`, chunk is not displayed
  eval = TRUE, # if `FALSE`, chunk is not evaluated
  include = TRUE, # if `FALSE`, chunk output is not displayed and `error = FALSE`
  error = TRUE, # if `FALSE`, evaluation stops on errors
  warning = TRUE, # if `FALSE`, warnings printed in console, not document
  message = FALSE, # if `FALSE`, messages printed in console, not document
  digits = 7, # number of digits printed in chunk output
  char.width = 74, # character width of chunks
  fig.pos = "H", # figure position in document
  fig.align = "center", # figure alignment
  dev = "pdf", # plotting device
  dev.args = list(pointsize = 10), # base point size in plots
  eval.after = "fig.cap", # chunk options to be evaluated after chunk
  strip.white = TRUE # reduce white space around chunks
)
@

\section{Introduction}
\label{sec:intro}

R package \pkg{epigrowthfit} implements methods for estimating
parameters associated with epidemic growth. \pkg{epigrowthfit}
was initially developed to support the analysis of Earn
\etal~\cite{Earn+20}, based on the methodology of Ma
\etal~\cite{Ma+14}, but now extends their methods and provides
additional useful machinery.

This document was built using \Sexpr{R.Version()$version.string}
and these R package versions:

<<package-versions, echo=FALSE>>=
package_list <- c(
  "epigrowthfit",
  "TMB",     # automatic differentiation of objective functions
  "emdbook", # tools for ecological modeling
  "knitr",   # integration of R code and LaTeX
  "shape"    # arrows in plots
)
print(installed.packages()[package_list, "Version"], quote = FALSE)
library(epigrowthfit)
@

\subsection{Installation}

\pkg{epigrowthfit} can be installed from a
\href{https://github.com/davidearn/epigrowthfit/}{GitHub repository}
using function \fun{install_github} from the \pkg{remotes} package.

<<installation, eval=FALSE>>=
if (!require(remotes)) {
  install.packages("remotes")
}
remotes::install_github("davidearn/epigrowthfit",
  ref = "devel",
  dependencies = TRUE,
  build_vignettes = TRUE
)
library(epigrowthfit)
@
%
A list of data sets and functions exported by \pkg{epigrowthfit}
can be retrieved with \fun{data} and \fun{ls}.

<<namespace1>>=
# Data sets
(dnames <- data(package = "epigrowthfit")$results[, "Item"])

# Functions
(fnames <- setdiff(ls("package:epigrowthfit"), dnames))
@

Functions \fun{egf_init} and \fun{egf} do most of the work in
fitting models of epidemic growth. They define the \code{egf_init}
and \code{egf} classes, for which there are a number of useful
S3 methods.
%
<<namespace2>>=
(mnames <- setdiff(ls(getNamespace("epigrowthfit")), c(dnames, fnames)))
@
%
These should not be called directly, as S3 methods are found
automatically by R when the corresponding generic functions
(the names before the dot) are called. For example, if
\code{`9plot`9(x)} is run and \code{x} is an \code{egf} object,
then R evaluates \code{`9plot.egf`9(x)}. \fun{egf_init}, \fun{egf},
and the associated methods are demonstrated in \cref{sec:example1}.

The remaining exported functions serve primarily to enable estimation
of the basic reproduction number $\R_0$ from initial epidemic growth
rates $r$ obtained by \fun{egf}. These functions are demonstrated in
\cref{sec:example2}.


\subsection{Documentation}

Package documentation can be accessed as follows:

<<documentation, eval=FALSE>>=
## This vignette
vignette("epigrowthfit-vignette")

## Help pages
?"epigrowthfit-package" # package
?data_name              # data set "data_name"
?function_name          # function "function_name"
?"class_name-methods"   # S3 methods for class "class_name"
                        # ("egf_init" or "egf")
@


\section{Data requirements}
\label{sec:data}

All that is required to use \pkg{epigrowthfit} is an interval incidence
time series. That is, one must have times $t_0 < t_1 < \cdots < t_n$
and know the number $x_i$ of cases observed between times $t_{i-1}$
and $t_i$ for $i = 1,\ldots,n$. Interval incidence can be derived from
\emph{cumulative} incidence by differencing. That is, if one knows the
number $c_i$ cases observed up to time $t_i$ for $i = 0,\ldots,n$, then
one derives interval incidence as $x_i = c_i - c_{i-1}$.
\cref{fig:data-types} displays the relationship between the two types
of incidence.

<<data-types, fig.width=4, fig.height=3, fig.cap=paste0("A sketch of the relationship between cumulative and interval incidence. Cumulative incidence $c_i$ is observed at times $t_i$ for $i = 0,\\ldots,", n, "$. Interval incidence $x_i$ is computed as $x_i = c_i - c_{i-1}$ for $i = 1,\\ldots,", n, "$. When the times $t_i$ are roughly equally spaced, as in this sketch, interval incidence peaks around the inflection point in cumulative incidence (black square), where the curvature changes sign."), echo=FALSE>>=
r <- 1
c0 <- 1
K <- 10
logistic <- function(time) K / (1 + (K / c0 - 1) * exp(-r * time))

n <- 5
time <- 0:n
cum_inc <- logistic(time)
int_inc <- diff(cum_inc)
l <- length(time)
curve_time <- seq(0, n, by = 0.2)
curve_cum_inc <- logistic(curve_time)
inflection_time <- -(1 / r) * (log(c0) - log(K - c0))
inflection_cum_inc <- logistic(inflection_time)

par(mar = c(3, 3, 1, 1), mgp = c(3, 0.7, 0), las = 1)

xax_labels <- parse(text = paste0("t[", time, "]"))
yax_labels <- parse(text = paste0("c[", time, "]"))
arr_labels <- parse(text = paste0("x[", time[-1], "]"))

plot.new()
plot.window(xlim = range(time), ylim = c(0, max(cum_inc) * 1.1),
            yaxs = "i")
segments(x0 = rep(par("usr")[1], 4), y0 = cum_inc,
         x1 = time, y1 = cum_inc,
         lty = 3, lwd = 2, col = "grey80")
lines(curve_time, curve_cum_inc, lwd = 2, col = "grey80")
points(time, cum_inc)
points(inflection_time, inflection_cum_inc, pch = 15)
shape::Arrows(x0 = time[-1], y0 = cum_inc[-l],
              x1 = time[-1], y1 = cum_inc[-1],
              arr.length = 0.16, arr.width = 0.08, arr.adj = 1,
              col = "grey30")
text(x = time[-1] + 0.2, y = cum_inc[-l] + 0.4 * int_inc,
     labels = arr_labels, xpd = NA)
box(bty = "l")
axis(side = 1, at = time, labels = xax_labels)
axis(side = 2, at = cum_inc, labels = yax_labels)
title(xlab = "time", line = 2)
title(ylab = "cumulative incidence", line = 2)
@

Here, ``cases'' is used loosely and usually means one of three things:
(i) infections, (ii) reported infections, or (iii) reported deaths
from disease. Under certain assumptions, the data type one uses makes
no difference to the initial epidemic growth rate. To make this
precise, let $c(t)$ be the expected number of infections occurring up
to time $t$ (expected cumulative incidence), and let $\wt{c}(t)$ be
the expected number of infections (or disease deaths) \emph{reported}
up to to time $t$ (expected cumulative reported incidence). If, at
the start of an epidemic, cumulative incidence can be modeled as an
exponential function, so that
%
\begin{linenomath}
\begin{equation}
\label[blankequation]{bleq:data-1}
c(t) \sim c_0 e^{rt}\,,
\end{equation}
\end{linenomath}
%
and if cumulative reported incidence is proportional to cumulative
incidence at an earlier time, so that
%
\begin{linenomath}
\begin{equation}
\label[blankequation]{bleq:data-2}
\wt{c}(t) \propto c(t - t_\text{delay})\,,
\end{equation}
\end{linenomath}
%
then both $c(t)$ and $\wt{c}(t)$ grow exponentially with rate $r$.
In fact, the same is true for interval incidence $c(t) - c(t-\Delta t)$
and interval reported incidence $\wt{c}(t) - \wt{c}(t - \Delta t)$.
Hence, for the purpose of estimating $r$ for a given epidemic, it
is sufficient to study reported incidence and not necessary to know
incidence, at least under assumptions \cref{bleq:data-1,bleq:data-2}.
(Going forward, ``cases'' and ``incidence'' are used as general terms
referring to infections, reported infections, or reported deaths from
disease.)

In the context of early historical epidemics, one often observes
deaths due to all causes, but not disease deaths. \pkg{epigrowthfit}
handles this additional use case by assuming a model for how all
causes mortality is decomposed into disease mortality and mortality
due to other causes (see \cref{sec:models-baseline}).


\section{Models of epidemic growth}
\label{sec:models}

The models of epidemic growth implemented in \pkg{epigrowthfit}
consist of a phenomenological model and an observation model.
The phenomenological model formulates what an incidence curve is
expected to look like, while the observation model expresses how
what we observe varies randomly from this expectation. Below is
a brief outline of these models.

\subsection{Models of expected cumulative incidence}
\label{sec:models-expected}

Let $c(t)$ be the expected number of cases observed up to
$t$ days since a reference date, and let $c(0) = c_0 > 0$.

\paragraph{Exponential model.} If $c(t)$ follows
%
\begin{linenomath}
\begin{equation}
\label{eq:exponential-de}
c'(t) = r c(t)\,,\qquad r > 0\,,
\end{equation}
\end{linenomath}
%
then $c(t)$ grows exponentially as
%
\begin{linenomath}
\begin{equation}
\label{eq:exponential}
c(t) = c_0 e^{r t}\,.
\end{equation}
\end{linenomath}
%
Two parameters must be fit to observed data: the exponential
growth rate $r$ and initial value $c_0$.

The exponential model ignores depletion of susceptible individuals
and implies unbounded exponential growth of $c(t)$. It can agree
with epidemic data only during the (typically short) initial
exponential growth phase. Indeed, Ma \etal~\cite{Ma+14} show that
estimates of $r$ obtained from the exponential model are highly
sensitive to the choice of fitting window. More robust and realistic
fits to epidemic data are obtained with the logistic and Richards
models, which allow $c(t)$ to saturate asymptotically (see below).

\paragraph{Logistic model.} If $c(t)$ follows
%
\begin{linenomath}
\begin{equation}
\label{eq:logistic-de}
c'(t) = r c(t)\bigg(1 - \frac{c(t)}{K}\bigg)\,,\qquad r, K > 0\,,
\end{equation}
\end{linenomath}
%
and if $c_0 \in (0,K)$, then $c(t)$ grows logistically
%
\begin{linenomath}
\begin{equation}
c(t) = \frac{K}{1 + \big(\frac{K}{c_0} - 1\big) e^{-r t}}
\end{equation}
\end{linenomath}
%
and increases to $K$ as $t \to \infty$. The logistic model can
be reparametrized as
%
\begin{linenomath}
\begin{equation}
\label{eq:logistic}
c(t) = \frac{K}{1 + e^{-r (t - \thalf)}}\,,
\end{equation}
\end{linenomath}
%
where $\thalf$ is the time at which cumulative incidence attains
half its final size, satisfying $c(\thalf) = \frac{K}{2}$. The
reparametrized logistic model requires fitting $r$, $K$, and $\thalf$
to observed data.

In the logistic model, $r$ represents the \emph{initial} exponential
growth rate, as \cref{eq:logistic-de} gives $c'(t) \sim r c(t)$ for
$c(t) / K \ll 1$. That is, at the start of an epidemic, when $c(t)$
is very small compared to $K$, $c(t)$ grows roughly exponentially
with rate $r$.

\paragraph{Richards model~\cite{Rich59}.} If $c(t)$ follows
%
\begin{linenomath}
\begin{equation}
\label{eq:richards-de}
c'(t) = r c(t)\bigg(1 - \bigg(\frac{c(t)}{K}\bigg)^p\bigg)\,,\qquad r, K, p > 0\,,
\end{equation}
\end{linenomath}
%
and if $c_0 \in (0,K)$, then $c(t)$ grows as
%
\begin{linenomath}
\begin{equation}
c(t) = \frac{K}{\big[1 + \big(\big(\frac{K}{c_0}\big)^p - 1\big) e^{-r p t}\big]^{1/p}}
\end{equation}
\end{linenomath}
%
and increases to $K$ as $t \to \infty$. Here, $p$ is a shape parameter
determining how quickly $c(t)$ saturates. (The logistic model is
recovered in the special case $p = 1$.) The Richards model can be
reparametrized as
%
\begin{linenomath}
\begin{equation}
\label{eq:richards}
c(t) = \frac{K}{\big[1 + (2^p - 1) e^{-r p (t - \thalf)}\big]^{1/p}}
\end{equation}
\end{linenomath}
%
where, as with the logistic model, $\thalf$ satisfies
$c(\thalf) = \frac{K}{2}$. The reparametrized Richards model
requires fitting $r$, $K$, $\thalf$, and $p$ to observed data.
Here, again, $r$ represents the initial epidemic growth rate.

\cref{fig:models-expected} compares cumulative and interval
incidence curves generated by the exponential, logistic, and
Richards models.

<<models-expected, fig.height=2.5, fig.cap=paste0("\\textbf{[Left]} Cumulative incidence, $c(t)$. Displayed are exponential and Richards ($p = ", paste0(p, collapse = ", "), "$) curves with $r = ", r, "$, $c_0 = ", c0, "$, and $K = ", K, "$. The Richards curve with $p = 1$ corresponds to a logistic curve. A dashed line is drawn at $c = K / 2 = ", K / 2, "$ to show that $\\thalf$ is a decreasing function of $p$. Units of time are characteristic ($1 / r$). \\textbf{[Right]} Interval incidence, $c(t) - c(t - 1)$."), echo=FALSE>>=
r <- 1
c0 <- 1
K <- 1000
p <- c(1.3, 1, 0.7)
time <- seq(0, 16, by = 1)
richards <- function(p) K / (1 + ((K / c0)^p - 1) * exp(-r * p * time))^(1 / p)
cum_inc <- c(list(c0 * exp(r * time)), lapply(p, richards))
int_inc <- lapply(cum_inc, diff)

## Setup
ltys <- c(1, 2, 1, 3)
cols <- c("grey80", rep("grey30", 3))
labs <- c("exponential", paste0("Richards (p = ", p, ")"))
par(mfrow = c(1, 2), mar = c(3, 4, 1, 3), oma = c(0, 0, 0, 5),
    mgp = c(3, 0.7, 0), las = 1)

## Panel 1
plot.new()
plot.window(xlim = range(time),
            ylim = c(0, max(unlist(cum_inc[-1])) * 1.1),
            xaxs = "i", yaxs = "i")
abline(h = K / 2, lty = 2, col = "grey30")
for (i in 1:4) {
  lines(time, cum_inc[[i]], lty = ltys[i], lwd = 3, col = cols[i])
}
box(bty = "l")
axis(side = 1)
axis(side = 2)
title(xlab = "time", line = 2)
title(ylab = "cumulative incidence")

## Panel 2
plot.new()
plot.window(xlim = range(time), ylim = c(0, max(unlist(int_inc[-1])) * 1.1),
            xaxs = "i", yaxs = "i")
for (i in 1:4) {
  lines(time[-1], int_inc[[i]], lty = ltys[i], lwd = 3, col = cols[i])
}
box(bty = "l")
axis(side = 1)
axis(side = 2)
title(xlab = "time", line = 2)
title(ylab = "interval incidence")
legend(x = par("usr")[2] * 1.05, y = mean(par("usr")[3:4]),
       xpd = NA, yjust = 0.5, bty = "n", seg.len = 3, cex = 0.7,
       legend = labs, lty = ltys, lwd = 3, col = cols)
@


\subsection{Models of observed interval incidence}
\label{sec:models-observed}

Let $x_i = x(t_{i-1},t_i) = c(t_i) - c(t_{i-1})$ be the expected number
of cases observed between times $t_{i-1}$ and $t_i > t_{i-1}$ (expected
interval incidence). Let $X_i = X(t_{i-1},t_i)$ be the number that is
actually observed.

\paragraph{Poisson model.} $X_i$ is modeled as a Poisson-distributed
random variable with mean $x_i$:
%
\begin{linenomath}
\begin{equation}
X_i \sim \mathrm{Poisson}\big(x_i\big)\,.
\end{equation}
\end{linenomath}

\paragraph{Negative binomial model.} $X_i$ is modeled as a negative
binomial-distributed random variable with mean $x_i$ and dispersion
$k > 0$:
%
\begin{linenomath}
\begin{equation}
X_i \sim \mathrm{NegativeBinomial}\big(x_i,k\big)\,.
\end{equation}
\end{linenomath}
%
This requires that $k$ is fit in addition to other model parameters.

It is worth noting that a negative binomial distribution with
mean $x_i$ and dispersion $k$ is well approximated by the Poisson
distribution with mean $x_i$ if $x_i / k \ll 1$~\cite{Bolk08}.
(Negative binomial variance exceeds Poisson variance by $x_i^2 / k$.
Relative to the mean $x_i$, the excess variance is $x_i / k$.)
Indeed,
%
\begin{linenomath}
\begin{equation}
\mathrm{NegativeBinomial}\big(x_i,k\big) \quad\xrightarrow{k \to \infty}\quad \mathrm{Poisson}(x_i)\,.
\end{equation}
\end{linenomath}
%
This means that if the fitted value of $x_i / k$ is less
than roughly $1/10$ for all $i$, then one should consider
switching to the Poisson observation model.
\david{Some citations needed in this section.}
\mikael{Do we know a textbook that proves convergence
in distribution to Poisson? There is a sketch on Wikipedia\ldots}

\cref{fig:models-observed} compares negative binomial and Poisson
distributions with a common mean of 20.

<<models-observed, fig.height=2.5, fig.cap=paste0("Probability mass functions of three negative binomial random variables ($k = ", paste0(k, collapse = ", "), "$) and a Poisson random variable, all with mean ", mu, "."), echo=FALSE>>=
val <- 0:60
mu <- 20
k <- 10^{c(0:2)}
pmf <- c(lapply(k, function(x) dnbinom(val, mu = mu, size = x)),
         list(dpois(val, lambda = mu)))

## Setup
pchs <- c(2, 4, 16, 1)
cols <- c("#DDDDDD", "#44BB99", "#BBCC33", "#EE8866")
labs <- c(paste0("NegativeBinomial (", mu, ", ", k, ")"),
          paste0("Poisson(", mu, ")"))
par(mar = c(3, 4, 1, 1), mgp = c(3, 0.7, 0), las = 1)

## Plot
plot.new()
plot.window(xlim = range(val), ylim = c(0, 0.1),
            xaxs = "i", yaxs = "i")
for (i in 1:4) {
  points(val, pmf[[i]], pch = pchs[i], col = cols[i])
}
box(bty = "l")
axis(side = 1)
axis(side = 2)
title(xlab = "Value", line = 2)
title(ylab = "Probability")
legend("topright", bty = "n", cex = 0.7,
       legend = labs, pch = pchs, col = cols)
@


\subsection{Baseline growth}
\label{sec:models-baseline}

For many historical epidemics, available data count deaths due
to all causes, not deaths to disease of interest specifically.
Growth in disease mortality over time can still be understood
phenomenologically provided that it is modeled separately from
baseline mortality (deaths to causes other than the disease).
To account for baseline mortality, it is assumed that deaths
in the absence of disease occur at a constant rate $b > 0$.
Then, for example, the logistic model given by \cref{eq:logistic}
becomes
%
\begin{linenomath}
\begin{equation}
\label{eq:logistic-baseline}
c(t) = b t + \frac{K}{1 + e^{-r (t - \thalf)}}\,,
\end{equation}
\end{linenomath}
%
where $c(t)$ is to be interpreted as expected cumulative all
causes mortality instead of expected cumulative disease mortality.
Accounting for baseline mortality requires that $b$ is fit in
addition to other model parameters.

\cref{fig:models-baseline} displays the effect of baseline growth
on the logistic curve.

<<models-baseline, fig.height=2.5, fig.cap=paste0("\\textbf{[Left]} Cumulative all causes mortality, $c(t)$. Displayed are logistic curves with $r = ", r, "$, $c_0 = ", c0, "$, and $K = ", K, "$ and a linear baseline ($b = ", paste0(b, collapse = ", "), "$; see \\cref{eq:logistic-baseline}). Units of time are characteristic ($1 / r$). \\textbf{[Right]} Interval all causes mortality, $c(t) - c(t - 1)$."), echo=FALSE>>=
r <- 1
c0 <- 1
K <- 1000
b <- c(50, 25, 0)
time <- seq(0, 16, by = 1)
logistic <- function(b) {
  b * time + K / (1 + (K / c0 - 1) * exp(-r * time))
}
cum_inc <- lapply(b, logistic)
int_inc <- lapply(cum_inc, diff)

## Setup
cols <- c("grey30", "grey30", "grey80")
ltys <- c(3, 2, 1)
labs <- paste0("b = ", b)
par(mfrow = c(1, 2), mar = c(3, 4, 1, 3), oma = c(0, 0, 0, 5),
    mgp = c(3, 0.7, 0), las = 1)

## Panel 1
plot.new()
plot.window(xlim = range(time), ylim = c(0, max(unlist(cum_inc)) * 1.1),
            xaxs = "i", yaxs = "i")
for (i in 1:3) {
  lines(time, cum_inc[[i]], lty = ltys[i], lwd = 3, col = cols[i])
}
box(bty = "l")
axis(side = 1)
axis(side = 2)
title(xlab = "time", line = 2)
title(ylab = "cumulative AC mortality")

## Panel 2
plot.new()
plot.window(xlim = range(time), ylim = c(0, max(unlist(int_inc)) * 1.1),
            xaxs = "i", yaxs = "i")
for (i in 1:3) {
  lines(time[-1], int_inc[[i]], lty = ltys[i], lwd = 3, col = cols[i])
}
box(bty = "l")
axis(side = 1)
axis(side = 2)
title(xlab = "time", line = 2)
title(ylab = "interval AC mortality")
legend(x = par("usr")[2] * 1.05, y = mean(par("usr")[3:4]),
       xpd = NA, yjust = 0.5, bty = "n", seg.len = 3, cex = 0.7,
       legend = labs, lty = ltys, lwd = 3, col = cols)
@


\section{Maximum likelihood estimation}
\label{sec:mle}

For $i = 1,\ldots,n$, let $x_i$ be the number of cases observed
between times $t_{i-1}$ and $t_i$. The likelihood of a model
parameter vector $\vec{\theta}$ given data $\vec{x}$ is the
probability of observing the data given the model, \ie
%
\begin{linenomath}
\begin{equation}
\mathcal{L}(\vec{\theta}|\vec{x}) = \prod_{i=1}^{n} f_i(x_i|\vec{\theta})\,,
\end{equation}
\end{linenomath}
%
where $f_i$ is the density function of $X(t_{i-1},t_i)$
(see \cref{sec:models-observed}). Hence the negative log
likelihood is
%
\begin{linenomath}
\begin{equation}
-\ell(\vec{\theta}|\vec{x})
= -\log \mathcal{L}(\vec{\theta}|\vec{x})
= -\sum_{i=1}^{n} \log f_i(x_i|\vec{\theta})\,.
\end{equation}
\end{linenomath}
%
\pkg{epigrowthfit} writes $-\ell$ in a C++ template and uses package
\pkg{TMB} to carry out automatic differentiation of $-\ell$ with
respect to log-transformed parameters. (All parameters are fit on an
unconstrained logarithmic scale. Hence, for example, \pkg{epigrowthfit}
fits $\log r$ on the interval $(-\infty,\infty)$, instead of $r$ on
the interval $(0,\infty)$.) Function \fun{egf} calls gradient-based
optimizer \fun{nlminb} (alternatively \fun{nlm} or one of the
optimizers provided through \fun{optim}) from base package \pkg{stats}
to search for the parameter vector $\vec{\wh{\theta}}$ minimizing
$-\ell$, namely the maximum likelihood estimate of $\vec{\theta}$.

The iterative optimizers are susceptible to various numerical problems
preventing convergence near to $\vec{\wh{\theta}}$. Poor fits to the
data by \fun{nlminb} may be corrected by trying a different optimizer,
such as the Nelder-Mead algorithm available through \fun{optim}.
Nelder-Mead is considered robust, but, as it makes no use of gradients,
it is relatively slow. This may not be a concern in practice, as the
models implemented in \pkg{epigrowthfit} have at most 6 parameters.


\section{Example 1: Fitting a model of epidemic growth}
\label{sec:example1}


\subsection{Loading an epidemic time series}

\pkg{epigrowthfit} comes with a number of infectious disease
data sets. One of them is \code{canadacovid}, a data frame
listing daily confirmations of COVID-19 in Canadian provinces
and territories from February 14, 2020 to June 21, 2020. You
can load \code{canadacovid} into your working environment
using \fun{data}.

<<example1-1-1>>=
library(epigrowthfit)
data(canadacovid)
@
%
Documentation for the data set can be accessed by running
\code{?canadacovid}, but you can directly examine the data
frame as follows.

<<example1-1-2>>=
dim(canadacovid) # dimensions
head(canadacovid, 10) # first 10 rows
levels(canadacovid$province) # province and territory labels
sapply(canadacovid, class) # variable classes
@
%
Let's consider just the data from Ontario.

<<example1-1-3>>=
ontario <- subset(canadacovid, province == "ON")
dim(ontario)
@
%
\pkg{epigrowthfit} does not tolerate missing values in the interval
incidence time series. You can remove rows containing \code{`1NA`1}
using \fun{na.omit}.

<<example1-1-4>>=
ontario <- na.omit(ontario)
dim(ontario)
sum(is.na(ontario$new_confirmations)) # check that no NA remain
@
%
Just removing a date with a missing value is sufficient only if
the missing number of cases is carried over to the next observation
time. Here, we assume this to be true, but if that is not the case,
then imputation is necessary (unless the missing number is expected
to be negligible).


\subsection{Initializing the fitting machinery with \tops{\protect\fun{egf_init}}{egf\_init()}}

We will fit a Richards model (see \cref{sec:models-expected}) with
negative binomial observations (see \cref{sec:models-observed}) to
the \code{ontario} data. The optimization (see \cref{sec:mle}) is
carried out by function \fun{egf}, but requires initialization.
Before calling \fun{egf}, we use function \fun{egf_init} to define
(i) a fitting window and
(ii) initial estimates of all model parameters.

\paragraph{Fitting window.} The ``fitting window'' is the subset of
data (\ie the range of dates) considered when fitting a model.
Ma \etal~\cite{Ma+14} examine the sensitivity of the fitted initial
exponential growth rate to the endpoints of the fitting window.
A reasonable initial approach for the logistic and Richards models,
based on their analysis, is to start at the time of the first
observed case and to end at the time of the peak in interval
incidence (equivalently, the inflection point in cumulative incidence;
see \cref{fig:data-types}). If the peak has not occurred (\eg when
fitting in real time, during a growing epidemic), then end at the
last observation time.

\fun{egf_init} implements the above approach by default. However,
for an exponential model, it results in underestimation of $r$,
because epidemic growth becomes subexponential much earlier than
the peak in interval incidence. When fitting an exponential model,
it is typically best to choose the window where interval incidence
is roughly linear on a logarithmic scale.

Regardless of the model being fit, one may need to experiment with
different fitting windows. Optional \fun{egf_init} arguments, namely
\code{min_wlen}, \code{peak}, \code{first}, \code{first_level}, and
\code{skip_zero}, can be set explicitly to override the default window
selection algorithm. For details, run \code{?egf_init}.

\paragraph{Initial parameter estimates.} The optimization algorithms
employed by \pkg{epigrowthfit} (see \cref{sec:mle}) are iterative and
require an initial estimate $\vec{\theta}^{(0)}$ of parameter vector
$\vec{\theta}$. The exact choice of $\vec{\theta}^{(0)}$ is typically
not critical, but a good initial guess can support convergence.
Reasonable initial estimates of $r$ and $c_0$ are $\beta_1$ and
$e^{\beta_0}$, respectively, where $\beta_1$ and $\beta_0$ are
the slope and intercept of a linear model fit to the logarithm of
cumulative incidence, using only observations in the fitting window.
If the epidemic curve is roughly symmetric, then an estimate of
$\thalf$ is supplied by the time of the peak in interval incidence.
Certainly, $K$ should be no less than the number of cases observed
to date and no more than the population size. When fitting a Richards
model, one can initially assume a logistic model by estimating $p = 1$.
When including baseline growth in a model
(see \cref{sec:models-baseline}), one can use the average mortality
rate in the years preceding and following the (presumably historical)
epidemic as an initial estimate of $b$.

With the exception of $b$, \fun{egf_init} employs these initial
estimates (using the lower bound on $K$) by default. For simplicity,
the default initial estimates of parameters $b$ and $k$ are 1
day$^{-1}$ and 1, respectively. Optional argument \code{theta0}
can be set explicitly to override any of these defaults. For details,
run \code{?egf_init}.

\,\\

Below, we pass to \fun{egf_init} a Date vector \code{date}
listing times $t_0 < t_1 < \cdots < t_n$ and a numeric vector
\code{cases} specifying interval incidence $x_i$ for
$i = 1,\ldots,n$, where $x_i$ is the number of cases observed
between times $t_{i-1}$ and $t_i$. We also indicate the model
we want to fit using arguments:
%
\par\vspace{6pt}
\begin{tabular}{rl}
\code{curve} & options \code{`2"exponential"`2}, \code{`2"logistic"`2} (default), or \code{`2"richards"`2} \\
\code{distr} & options \code{`2"pois"`2} or \code{`2"nbinom"`2} (default) \\
\code{include_baseline} & options \code{`1TRUE`1} or \code{`1FALSE`1} (default)
\end{tabular}
\vspace{6pt}\par\noindent
%
We accept the default values of all other arguments, letting
\fun{egf_init} select a fitting window and initial parameter
estimates for us as described above.

<<example1-2-1>>=
init <- egf_init(
  date = ontario$date,
  cases = ontario$new_confirmations[-1],
  curve = "richards",
  distr = "nbinom",
  include_baseline = FALSE
)
@

The output of the initialization is an \code{egf_init} object---%
a list with \Sexpr{length(init)} elements containing information
used by \fun{egf} and methods for class \code{egf_init}.

<<example1-2-2>>=
class(init)
names(init)
@
%
Elements \code{date}, \code{cases}, \code{curve}, \code{distr}, and
\code{include_baseline} are copies of the arguments of \fun{egf_init}.
Element \code{time} expresses each date listed in \code{date} as a
number of days since \code{date[`11`1]}.

<<example1-2-3>>=
with(init, identical(time, as.numeric(date - date[1])))
@
%
Elements \code{first} and \code{last} define the selected fitting
window. They are integer indices such that \code{cases[first]}
and \code{cases[last]} are the first and last observations in the
window.
%
Element \code{theta0} is a numeric vector listing initial parameter
estimates, which, in this example, were all selected by \fun{egf_init}.
Element \code{log_theta0} gives the log-transformed estimates.
To retrieve \code{theta0} and \code{log_theta0}, you can use the
\fun{coef} method for class \code{egf_init}.

<<example1-2-4>>=
coef(init) # same as `init$theta0`
coef(init, log = TRUE) # same as `init$log_theta0`
@
%
Here, $r$ is expressed in units per day and $\thalf$ in days.
The doubling time corresponding to an initial epidemic growth
rate $r$ is $(\log 2) / r$, hence the doubling time corresponding
to \code{theta0[[`2"r"`2]]} in this example is
\Sexpr{log(2) / init$theta0[["r"]]} days.

Element \code{cum_inc} is a closure (function) with a numeric
argument \code{time} specifying times in days since \code{date[`11`1]}.
\code{`9cum_inc`9(time)} evaluates the expected cumulative incidence
curve (\cref{eq:richards} in this example) at \code{time} days,
conditional on parameter vector \code{theta0}.
Element \code{int_inc} is a closure such that \code{int_inc(time)}
evaluates \code{`9diff`9(`9cum_inc`9(time))}, the interval incidence
time series corresponding to \code{`9cum_inc`9(time)}.
The \fun{predict} method for class \code{egf_init} provides
a convenient interface to these two functions.

<<example1-2-5>>=
predict(init, init$time[1:10])
@

Finally, element \code{call} contains the call to \fun{egf_init},
so that the returned \code{egf_init} object is reproducible with
\code{`9eval`9(call)}.


\subsection{Plotting and printing \tops{\protect\code{egf_init}}{egf\_init} objects}

The \fun{plot} method for class \code{egf_init} displays together
all of the salient components of an \code{egf_init} object. It
takes an argument \code{inc} indicating the type of incidence to
plot: \code{`2"cumulative"`2} (default) or \code{`2"interval"`2}.

<<example1-3-1, fig.height=7, fig.cap='Result of plotting an \\code{egf_init} object: \\textbf{[Top]} cumulative incidence (\\code{inc = `2"cumulative"`2}) and \\textbf{[Bottom]} interval incidence (\\code{inc = `2"interval"`2}). Points are observed incidence. Lines are expected incidence conditional on initial parameter estimates listed at the bottom of the right margin. Initial parameter estimates are guesses, hence the lines are \\emph{not} fitted to the data. Incidence is plotted on a logarithmic scale, with zeros plotted directly on the horizontal axis. Vertical dashed lines indicate the selected fitting window. For interval incidence, a coloured point at time $t_i$ indicates that the observation interval $t_i-t_{i-1}$ was more than 2.5\\% longer than the median observation interval.'>>=
par(mfrow = c(2, 1)) # create a two-panel plot
plot(init, inc = "cumulative")
plot(init, inc = "interval")
@

The exact endpoints of the fitting window selected by \fun{egf_init}
are not obvious from \cref{fig:example1-3-1}. The \fun{print} method
for class \code{egf_init} will tell you more about the window and
remind you that actually fitting a model requires a call to \fun{egf}.

<<example1-3-2>>=
print(init) # or just `init`
@


\subsection{Obtaining a fit with \tops{\protect\fun{egf}}{egf()}}

Function \fun{egf} takes an optional argument \code{method},
used to indicate an optimization algorithm. The options are
\code{`2"nlminb"`2} (default) \code{`2"nlm"`2},
\code{`2"Nelder-Mead"`2}, \code{`2"BFGS"`2},
\code{`2"L-BFGS-S"`2}, and \code{`2"CG"`2}.
If the default produces a bad fit to the data, then try
different algorithms and choose the fit with the greatest
likelihood.

Below, we pass \code{init} to \fun{egf}, accepting the
default \code{method = `2"nlminb"`2}.

<<example1-4-1>>=
fit <- egf(init)
class(fit)
@

\fun{egf} returns an \code{egf} object---a list with
\Sexpr{length(init)} elements containing information
about the fit that it obtained and other information
used by methods for class \code{egf_init}.

<<example1-4-2>>=
class(fit)
names(fit)
@

<<example-4-3>>=
plot(fit, inc = "interval")
plot(fit, inc = "cumulative")
@

\section{Example 2: Estimating the basic reproduction number \tops{$\R_0$}{R0}}
\label{sec:example2}

Under construction!

\end{document}

