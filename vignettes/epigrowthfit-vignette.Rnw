\documentclass[dvipsnames,12pt]{article}
%\VignetteIndexEntry{epigrowthfit-vignette}
%\VignetteEngine{knitr::knitr}


% Page layout
\usepackage[top=1in,bottom=1.5in,left=1in,right=1in]{geometry}
\usepackage{lineno} % line numbering
\renewcommand{\linenumberfont}{\normalfont\tiny\sffamily\color[gray]{0.5}}
\hfuzz=1in % tolerate \hbox fullness
\vbadness=\maxdimen % tolerate \vbox badness

% Text layout
%\usepackage{setspace} % \onehalfspacing, \doublespacing
%\raggedright
\usepackage[T1]{fontenc} % words with accented characters can be hyphenated

% Title page setup
\title{\Large The \pkg{epigrowthfit} package}
\author{Mikael Jagan, Benjamin M.\ Bolker, Junling Ma,\\ David J.\ D.\ Earn, Jonathan Dushoff}
\date{\today}

% Deferred execution
% front matter
\usepackage{etoolbox} % document hooks
\AfterEndPreamble{%
  \pagenumbering{roman}
  \maketitle
  \tableofcontents
  \thispagestyle{empty}
  \clearpage
  \pagenumbering{arabic}
  \linenumbers
}
% back matter
\AtEndDocument{%
  \bibliographystyle{vancouver}
  \bibliography{epigrowthfit-vignette}
  \addcontentsline{toc}{section}{References}
}

% Math
\usepackage{amsmath,mathtools}
\usepackage{amssymb,bm,bbm}
\allowdisplaybreaks % page breaks in display style math mode

% Code
\usepackage[scaled=0.85]{DejaVuSansMono}
% Match inline code highlighting to custom knitr theme
% specified in `knit_theme.css`
\definecolor{background}{HTML}{f4f4f4} % background
\definecolor{num}{HTML}{aa4499} % numeric, logical, NA
\definecolor{str}{HTML}{999933} % character
\definecolor{com}{HTML}{999999} % comment
\definecolor{opt}{HTML}{555555} % !?
\definecolor{std}{HTML}{555555} % variable name, operator, delimiter
\definecolor{kwa}{HTML}{aa4499} % function, if, else, for, in, while, NULL
\definecolor{kwb}{HTML}{555555} % assignment operator
\definecolor{kwc}{HTML}{555555} % function argument
\definecolor{kwd}{HTML}{3a9183} % function name
\usepackage{listings} % \lstinline
\lstset{%
  basicstyle=\color{std}\ttfamily,%
  breaklines=true,%
  moredelim=[is][\color{num}]{`1}{`1},
  moredelim=[is][\color{str}]{`2}{`2},
  moredelim=[is][\color{com}\itshape]{`3}{`3},
  moredelim=[is][\color{opt}]{`4}{`4},
  moredelim=[is][\color{std}]{`5}{`5},
  moredelim=[is][\color{kwa}\bfseries]{`6}{`6},
  moredelim=[is][\color{kwb}]{`7}{`7},
  moredelim=[is][\color{kwc}]{`8}{`8},
  moredelim=[is][\color{kwd}]{`9}{`9}
}

% Float captions
\usepackage{caption}
\captionsetup{%
  aboveskip=8pt,%
  labelfont=bf,%
  labelsep=period,%
  justification=raggedright,%
  singlelinecheck=false%
}
\renewcommand{\figurename}{Fig}

% Float placement
\usepackage{float} % \begin{figure}[H]
\usepackage[section]{placeins} % \FloatBarrier

% Lists
\usepackage{enumitem}
\setlist[enumerate]{label=(\roman*)}
\setlist[itemize]{label=\tiny$\blacksquare$}

% Tables
\usepackage{booktabs} % \toprule, \midrule, \bottomrule, \addlinespace
\usepackage{array}
% columns with variable width, top alignment
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

% Citation
\usepackage{cite}
\makeatletter
\renewcommand{\@biblabel}[1]{\quad #1.} % \@biblabel format
\makeatother

% Ref hyperlinks
\usepackage[colorlinks=true,allcolors=magenta]{hyperref}
\usepackage[nameinlink,capitalize]{cleveref}
% equation
\crefformat{equation}{#2Eq~#1#3}
\crefmultiformat{equation}{#2Eqs~#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{equation}{#3Eqs~#1#4--#5#2#6}
\crefformat{blankequation}{#2#1#3}
\crefmultiformat{blankequation}{#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{blankequation}{#3#1#4--#5#2#6}
\crefalias{blankequation}{equation}
\crefformat{pluralequation}{#2Eqs~#1#3}
\crefalias{pluralequation}{equation}
% figure
\crefformat{figure}{#2Fig~#1#3}
\crefmultiformat{figure}{#2Figs~#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{figure}{#3Figs~#1#4--#5#2#6}
% section
\crefformat{section}{#2\S#1#3}
\crefmultiformat{section}{#2\S\S#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{section}{#3\S\S#1#4--#5#2#6}
% table
\crefformat{Table}{#2Table~#1#3}
\crefmultiformat{table}{#2Tables~#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{table}{#3Tables~#1#4--#5#2#6}

% More macros
% laziness
\let\tops\texorpdfstring
% fonts
\renewcommand{\vec}[1]{\bm{\mathrm{#1}}}
% diacritics
\let\wh\widehat
\let\wt\widetilde
% delimiters
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
% operators
\DeclareMathOperator*{\argmin}{arg\,min}
% symbols
\newcommand{\thalf}{t_\text{\normalfont half}}
\newcommand{\R}{\mathcal{R}}
% abbreviations
\newcommand{\cf}{\textit{cf}.~}
\newcommand{\eg}{\textit{e}.\textit{g}.,~}
\newcommand{\ie}{\textit{i}.\textit{e}.,~}
\newcommand{\etc}{\textit{etc}.}
\newcommand{\etal}{\textit{et al}.}
% code
\newcommand{\code}[1]{\mbox{\lstinline|#1|}}
\newcommand{\fun}[1]{\code{`9#1`9()}}
\let\pkg\textbf
% comments
\newcommand{\comment}[3]{\textcolor{#1}{\textbf{[#2: }\textit{#3}\textbf{]}}}
\newcommand{\david}[1]{\comment{magenta}{DE}{#1}}
\newcommand{\mikael}[1]{\comment{blue}{MJ}{#1}}

%%%%%%%%%%%%%%%%
%% START HERE %%
%%%%%%%%%%%%%%%%

\begin{document}
\setlength{\parskip}{0.5mm}
\setlength{\parindent}{7mm}

<<set-chunk-defaults, echo=FALSE>>=
library(knitr)
## Make some substitutions in the .tex output in order to:
## * Prevent lineno from messing up breaking of chunks over pages.
## * Prevent automatic indentation after chunks.
## * Dispense with compile errors due to knitr-xcolor interaction.
##   See https://tex.stackexchange.com/questions/148188/.
knit_hooks$set(document = function(x) {
  x <- sub("\\begin{knitrout}", "\\nolinenumbers\\begin{knitrout}", x, fixed = TRUE)
  x <- sub("\\end{knitrout}", "\\end{knitrout}\\linenumbers\\noindent", x, fixed = TRUE)
  sub("\\usepackage[]{color}", "\\usepackage{xcolor}", x, fixed = TRUE)
})
## Set number of digits printed in chunk output with chunk option "digits"
knit_hooks$set(digits = function(before, options, envir) {
  if (before) {
    options(digits = options$digits)
  }
})
## Set character width of chunk with chunk option "width"
## (can be used to prevent intrusions into right margin)
knit_hooks$set(char.width = function(before, options, envir) {
  if (before) {
    options(width = options$char.width)
  }
})
## Use custom palette for code highlighting
knit_theme$set(knit_theme$get("knit_theme.css"))
## Set chunk defaults
opts_chunk$set(
  cache = TRUE, # if `FALSE`, chunk is evaluated from scratch with compile
  echo = TRUE, # if `FALSE`, chunk is not displayed
  eval = TRUE, # if `FALSE`, chunk is not evaluated
  include = TRUE, # if `FALSE`, chunk output is not displayed and `error = FALSE`
  error = TRUE, # if `FALSE`, evaluation stops on errors
  warning = TRUE, # if `FALSE`, warnings printed in console, not document
  message = FALSE, # if `FALSE`, messages printed in console, not document
  digits = 7, # number of digits printed in chunk output
  char.width = 74, # character width of chunks
  fig.pos = "H", # figure position in document
  fig.align = "center", # figure alignment
  dev = "pdf", # plotting device
  dev.args = list(pointsize = 10), # base point size in plots
  eval.after = "fig.cap", # chunk options to be evaluated after chunk
  strip.white = TRUE # reduce white space around chunks
)
@

\section{Introduction}
\label{sec:intro}

R package \pkg{epigrowthfit} implements methods for estimating
parameters associated with epidemic growth, including the initial
exponential growth rate $r$. \pkg{epigrowthfit} was initially
developed to support the analysis of Earn \etal~\cite{Earn+20},
based on the methodology of Ma \etal~\cite{Ma+14}, but now extends
their methods and provides additional useful machinery.

This document was built using \Sexpr{R.Version()$version.string}
and these R package versions:

<<package-versions, echo=FALSE>>=
package_list <- c(
  "epigrowthfit",
  "TMB",     # automatic differentiation of objective functions
  "emdbook", # tools for ecological modeling
  "knitr",   # integration of R code and LaTeX
  "shape",   # arrows in plots
  "scales"   # colour transparency in plots
)
print(installed.packages()[package_list, "Version"], quote = FALSE)
library(epigrowthfit)
@

\subsection{Installation}

\pkg{epigrowthfit} can be installed from a
\href{https://github.com/davidearn/epigrowthfit/}{GitHub repository}
using function \fun{install_github} from the \pkg{remotes} package.

<<installation, eval=FALSE>>=
if (!require(remotes)) {
  install.packages("remotes")
}
remotes::install_github("davidearn/epigrowthfit",
  ref = "devel",
  dependencies = TRUE,
  build_vignettes = TRUE
)
library(epigrowthfit)
@

A list of data sets and functions exported by \pkg{epigrowthfit}
can be retrieved using \fun{data} and \fun{ls}.

<<namespace1>>=
# Data sets
(dnames <- data(package = "epigrowthfit")$results[, "Item"])

# Functions
(fnames <- setdiff(ls("package:epigrowthfit"), dnames))
@
%
Functions \fun{egf_init} and \fun{egf} do all of the work in fitting
models of epidemic growth. They define the \code{egf_init} and
\code{egf} classes, for which there are a number of useful S3 methods
(see below). The remaining functions enable estimation of parameters
that are not explicitly fitted, from fitted initial exponential growth
rates $r$. These include the
epidemic doubling time,
basic reproduction number, and
epidemic final size (see \cref{sec:extras}).

A list of implemented S3 methods can retrieved as follows.

<<namespace2>>=
(mnames <- setdiff(ls(getNamespace("epigrowthfit")), c(dnames, fnames)))
@
%
These should not be called directly, as S3 methods are found
automatically by R when the corresponding generic functions
(the names before the dot) are called. For example, if
\code{`9plot`9(x)} is run and \code{x} is an \code{egf} object,
then R evaluates \code{`9plot.egf`9(x)}.

\cref{sec:ontario} demonstrates the entire \pkg{epigrowthfit}
machinery for a COVID-19 data set from Ontario, Canada.


\subsection{Documentation}

Package documentation can be accessed as follows:

<<documentation, eval=FALSE>>=
## This vignette
vignette("epigrowthfit-vignette")

## Help pages
?"epigrowthfit-package" # package
?data_set_name          # data set "data_set_name"
?function_name          # function "function_name"
?"class_name-methods"   # S3 methods for class "class_name"
                        # ("egf_init" or "egf")
@


\section{Data requirements}
\label{sec:data}

All that is required to use \pkg{epigrowthfit} is an interval incidence
time series. That is, one must have times $t_0 < t_1 < \cdots < t_n$
and know the number $x_i$ of cases observed between times $t_{i-1}$
and $t_i$ for $i = 1,\ldots,n$. Interval incidence can be derived from
\emph{cumulative} incidence by differencing. That is, if one knows the
number $c_i$ cases observed up to time $t_i$ for $i = 0,\ldots,n$, then
one derives interval incidence as $x_i = c_i - c_{i-1}$.
\cref{fig:data-types} displays the relationship between the two types
of incidence.

<<data-types, fig.width=4, fig.height=3, fig.cap=paste0("A sketch of the relationship between cumulative and interval incidence. Cumulative incidence $c_i$ is observed at times $t_i$ for $i = 0,\\ldots,", n, "$. Interval incidence $x_i$ is computed as $x_i = c_i - c_{i-1}$ for $i = 1,\\ldots,", n, "$. When the times $t_i$ are roughly equally spaced, as in this sketch, interval incidence peaks around the inflection point in cumulative incidence (black square), where the curvature changes sign."), echo=FALSE>>=
r <- 1
c0 <- 1
K <- 10
logistic <- function(time) K / (1 + (K / c0 - 1) * exp(-r * time))

n <- 5
time <- 0:n
cum_inc <- logistic(time)
int_inc <- diff(cum_inc)
l <- length(time)
curve_time <- seq(0, n, by = 0.2)
curve_cum_inc <- logistic(curve_time)
inflection_time <- -(1 / r) * (log(c0) - log(K - c0))
inflection_cum_inc <- logistic(inflection_time)

par(mar = c(3, 3, 1, 1), mgp = c(3, 0.7, 0), las = 1)

xax_labels <- parse(text = paste0("t[", time, "]"))
yax_labels <- parse(text = paste0("c[", time, "]"))
arr_labels <- parse(text = paste0("x[", time[-1], "]"))

plot.new()
plot.window(xlim = range(time), ylim = c(0, max(cum_inc) * 1.04),
            yaxs = "i")
segments(x0 = rep(par("usr")[1], 4), y0 = cum_inc,
         x1 = time, y1 = cum_inc,
         lty = 3, lwd = 2, col = "grey80")
lines(curve_time, curve_cum_inc, lwd = 2, col = "grey80")
points(time, cum_inc)
points(inflection_time, inflection_cum_inc, pch = 15)
shape::Arrows(x0 = time[-1], y0 = cum_inc[-l],
              x1 = time[-1], y1 = cum_inc[-1],
              arr.length = 0.16, arr.width = 0.08, arr.adj = 1,
              col = "grey30")
text(x = time[-1] + 0.2, y = cum_inc[-l] + 0.4 * int_inc,
     labels = arr_labels, xpd = NA)
box(bty = "l")
axis(side = 1, at = time, labels = xax_labels)
axis(side = 2, at = cum_inc, labels = yax_labels)
title(xlab = "time", line = 2)
title(ylab = "cumulative incidence", line = 2)
@

Here, ``cases'' is used loosely and usually means one of three things:
(i)~infections, (ii)~reported infections, or (iii)~reported deaths
from disease. Under certain assumptions, the data type one uses makes
no difference to the initial epidemic growth rate. To make this
precise, let $c(t)$ be the expected number of infections occurring up
to time $t$ (expected cumulative incidence), and let $\wt{c}(t)$ be
the expected number of infections (or disease deaths) \emph{reported}
up to to time $t$ (expected cumulative reported incidence). If, at
the start of an epidemic, cumulative incidence can be modeled as an
exponential function, so that
%
\begin{linenomath}
\begin{equation}
\label{eq:data-1}
c(t) \sim c_0 e^{rt}\,,
\end{equation}
\end{linenomath}
%
and if cumulative reported incidence is proportional to cumulative
incidence at an earlier time, so that
%
\begin{linenomath}
\begin{equation}
\label{eq:data-2}
\wt{c}(t) \propto c(t - t_\text{delay})\,,
\end{equation}
\end{linenomath}
%
then both $c(t)$ and $\wt{c}(t)$ grow exponentially with rate $r$.
In fact, the same is true for interval incidence $c(t) - c(t-\Delta t)$
and interval reported incidence $\wt{c}(t) - \wt{c}(t - \Delta t)$.
Hence, for the purpose of estimating $r$ for a given epidemic, it
is sufficient to study reported incidence and not necessary to know
incidence (assuming that \cref{eq:data-1,eq:data-2} are valid).
Going forward, ``cases'' and ``incidence'' are used as general terms
referring to infections, reported infections, or reported deaths
from disease.

In the study of historical epidemics, one often observes deaths
due to multiple causes including the disease of interest, rather
than disease deaths alone. \pkg{epigrowthfit} handles this
additional use case by assuming a model for how multiple causes
mortality is decomposed into disease mortality and mortality due
to other causes (see \cref{sec:models-baseline}).


\section{Models of epidemic growth}
\label{sec:models}

The models of epidemic growth implemented in \pkg{epigrowthfit}
consist of a phenomenological model and an observation model.
The phenomenological model formulates what an incidence curve is
expected to look like, while the observation model expresses how
what we observe varies randomly from this expectation. Below is
a brief outline of these models.

\subsection{Models of expected cumulative incidence}
\label{sec:models-expected}

Let $c(t)$ be the expected number of cases observed up to
$t$ days since a reference date, and let $c(0) = c_0 > 0$.

\paragraph{Exponential model.} If $c(t)$ follows
%
\begin{linenomath}
\begin{equation}
\label{eq:exponential-de}
c'(t) = r c(t)\,,\qquad r > 0\,,
\end{equation}
\end{linenomath}
%
then $c(t)$ grows exponentially as
%
\begin{linenomath}
\begin{equation}
\label{eq:exponential}
c(t) = c_0 e^{r t}\,.
\end{equation}
\end{linenomath}
%
Two parameters must be fit to observed data: the exponential
growth rate $r$ and initial cumulative incidence $c_0$.

The exponential model ignores depletion of susceptible individuals
and implies unbounded exponential growth of $c(t)$. It can agree
with epidemic data only during the (typically short) initial
exponential growth phase. Indeed, Ma \etal~\cite{Ma+14} show that
estimates of $r$ obtained from the exponential model are highly
sensitive to the choice of fitting window. More robust and realistic
fits to epidemic data are obtained with the logistic and Richards
models, which allow $c(t)$ to saturate asymptotically (see below).

\paragraph{Logistic model.} If $c(t)$ follows
%
\begin{linenomath}
\begin{equation}
\label{eq:logistic-de}
c'(t) = r c(t)\bigg(1 - \frac{c(t)}{K}\bigg)\,,\qquad r, K > 0\,,
\end{equation}
\end{linenomath}
%
and if $c_0 \in (0,K)$, then $c(t)$ grows logistically as
%
\begin{linenomath}
\begin{equation}
c(t) = \frac{K}{1 + \big(\frac{K}{c_0} - 1\big) e^{-r t}}
\end{equation}
\end{linenomath}
%
and increases to $K$ as $t \to \infty$. The logistic model can
be reparametrized as
%
\begin{linenomath}
\begin{equation}
\label{eq:logistic}
c(t) = \frac{K}{1 + e^{-r (t - \thalf)}}\,,
\end{equation}
\end{linenomath}
%
where $\thalf$ is the time at which cumulative incidence attains
half its final size, satisfying $c(\thalf) = \frac{K}{2}$. The
reparametrized logistic model requires fitting $r$, $K$, and $\thalf$
to observed data.

In the logistic model, $r$ represents the \emph{initial} exponential
growth rate, as \cref{eq:logistic-de} gives $c'(t) \sim r c(t)$ for
$c(t) / K \ll 1$. That is, at the start of an epidemic, when $c(t)$
is very small compared to $K$, $c(t)$ grows roughly exponentially
with rate $r$.

\paragraph{Richards model~\cite{Rich59}.} If $c(t)$ follows
%
\begin{linenomath}
\begin{equation}
\label{eq:richards-de}
c'(t) = r c(t)\bigg(1 - \bigg(\frac{c(t)}{K}\bigg)^p\bigg)\,,\qquad r, K, p > 0\,,
\end{equation}
\end{linenomath}
%
and if $c_0 \in (0,K)$, then $c(t)$ grows as
%
\begin{linenomath}
\begin{equation}
c(t) = \frac{K}{\big[1 + \big(\big(\frac{K}{c_0}\big)^p - 1\big) e^{-r p t}\big]^{1/p}}
\end{equation}
\end{linenomath}
%
and increases to $K$ as $t \to \infty$. Here, $p$ is a shape parameter
determining how quickly $c(t)$ saturates, and setting $p = 1$ recovers
the logistic model. The Richards model can be reparametrized as
%
\begin{linenomath}
\begin{equation}
\label{eq:richards}
c(t) = \frac{K}{\big[1 + (2^p - 1) e^{-r p (t - \thalf)}\big]^{1/p}}
\end{equation}
\end{linenomath}
%
where $\thalf$ satisfies $c(\thalf) = \frac{K}{2}$, as in
\cref{eq:logistic}. The reparametrized Richards model requires
fitting $r$, $K$, $\thalf$, and $p$ to observed data. Here,
as in \cref{eq:logistic}, $r$ represents the \emph{initial}
epidemic growth rate.

\cref{fig:models-expected} compares cumulative and interval
incidence curves generated by the exponential, logistic, and
Richards models.

<<models-expected, fig.height=2.5, fig.cap=paste0("\\textbf{[Left]} Cumulative incidence, $c(t)$. Displayed are exponential and Richards ($p = ", paste0(p, collapse = ", "), "$) curves with $r = ", r, "$, $c_0 = ", c0, "$, and $K = ", K, "$. The Richards curve with $p = 1$ corresponds to a logistic curve. A dashed line is drawn at $c = K / 2 = ", K / 2, "$. This line intersects the Richards curves at time $\\thalf$ and shows that $\\thalf$ is a decreasing function of $p$. Units of time are characteristic ($1 / r$). \\textbf{[Right]} Interval incidence, $c(t) - c(t - 1)$."), echo=FALSE>>=
r <- 1
c0 <- 1
K <- 1000
p <- c(1.3, 1, 0.7)
time <- seq(0, 16, by = 1)
richards <- function(p) K / (1 + ((K / c0)^p - 1) * exp(-r * p * time))^(1 / p)
cum_inc <- c(list(c0 * exp(r * time)), lapply(p, richards))
int_inc <- lapply(cum_inc, diff)

## Setup
ltys <- c(1, 2, 1, 3)
cols <- c("grey80", rep("grey30", 3))
labs <- c("exponential", paste0("Richards (p = ", p, ")"))
par(mfrow = c(1, 2), mar = c(3, 4, 1, 3), oma = c(0, 0, 0, 5),
    mgp = c(3, 0.7, 0), las = 1)

## Panel 1
plot.new()
plot.window(xlim = range(time),
            ylim = c(0, max(unlist(cum_inc[-1])) * 1.04),
            xaxs = "i", yaxs = "i")
abline(h = K / 2, lty = 2, col = "grey30")
for (i in 1:4) {
  lines(time, cum_inc[[i]], lty = ltys[i], lwd = 3, col = cols[i])
}
box(bty = "l")
axis(side = 1)
axis(side = 2)
title(xlab = "time", line = 2)
title(ylab = "cumulative incidence")

## Panel 2
plot.new()
plot.window(xlim = range(time), ylim = c(0, max(unlist(int_inc[-1])) * 1.1),
            xaxs = "i", yaxs = "i")
for (i in 1:4) {
  lines(time[-1], int_inc[[i]], lty = ltys[i], lwd = 3, col = cols[i])
}
box(bty = "l")
axis(side = 1)
axis(side = 2)
title(xlab = "time", line = 2)
title(ylab = "interval incidence")
legend(x = par("usr")[2] * 1.05, y = mean(par("usr")[3:4]),
       xpd = NA, yjust = 0.5, bty = "n", seg.len = 3, cex = 0.7,
       legend = labs, lty = ltys, lwd = 3, col = cols)
@


\subsection{Models of observed interval incidence}
\label{sec:models-observed}

Let $x_i = x(t_{i-1},t_i) = c(t_i) - c(t_{i-1})$ be the expected number
of cases observed between times $t_{i-1}$ and $t_i > t_{i-1}$ (expected
interval incidence). Let $X_i = X(t_{i-1},t_i)$ be the number that is
actually observed.

\paragraph{Poisson model.} $X_i$ is modeled as a Poisson-distributed
random variable with mean $x_i$:
%
\begin{linenomath}
\begin{equation}
X_i \sim \mathrm{Poisson}\big(x_i\big)\,.
\end{equation}
\end{linenomath}

\paragraph{Negative binomial model.} $X_i$ is modeled as a negative
binomial-distributed random variable with mean $x_i$ and dispersion
$k > 0$:
%
\begin{linenomath}
\begin{equation}
X_i \sim \mathrm{NegativeBinomial}\big(x_i,k\big)\,.
\end{equation}
\end{linenomath}
%
This requires that $k$ is fit in addition to other model parameters.

It is worth noting that a negative binomial distribution with
mean $x_i$ and dispersion $k$ is well approximated by the Poisson
distribution with mean $x_i$ if $x_i / k \ll 1$~\cite[p.\ 124]{Bolk08}.
(Negative binomial variance exceeds Poisson variance by $x_i^2 / k$.
Relative to the mean $x_i$, the excess variance is $x_i / k$.)
Indeed,
%
\begin{linenomath}
\begin{equation}
\mathrm{NegativeBinomial}\big(x_i,k\big) \quad\xrightarrow{k \to \infty}\quad \mathrm{Poisson}(x_i)\,.
\end{equation}
\end{linenomath}
%
This means that if the fitted value of $x_i / k$ is less
than roughly $1/10$ for all $i$, then one should consider
switching to the Poisson observation model.

\cref{fig:models-observed} compares negative binomial and Poisson
distributions with a common mean of 20.

<<models-observed, fig.height=2.5, fig.cap=paste0("Probability mass functions of three negative binomial random variables ($k = ", paste0(k, collapse = ", "), "$) and a Poisson random variable, all with mean ", mu, "."), echo=FALSE>>=
val <- 0:60
mu <- 20
k <- 10^{c(0:2)}
pmf <- c(lapply(k, function(x) dnbinom(val, mu = mu, size = x)),
         list(dpois(val, lambda = mu)))

## Setup
pchs <- c(2, 4, 16, 1)
cols <- c("#DDDDDD", "#44BB99", "#BBCC33", "#EE8866")
labs <- c(paste0("NegativeBinomial (", mu, ", ", k, ")"),
          paste0("Poisson(", mu, ")"))
par(mar = c(3, 4, 1, 1), mgp = c(3, 0.7, 0), las = 1)

## Plot
plot.new()
plot.window(xlim = range(val), ylim = c(0, 0.1),
            xaxs = "i", yaxs = "i")
for (i in 1:4) {
  points(val, pmf[[i]], pch = pchs[i], col = cols[i])
}
box(bty = "l")
axis(side = 1)
axis(side = 2)
title(xlab = "value", line = 2)
title(ylab = "probability")
legend("topright", bty = "n", cex = 0.7,
       legend = labs, pch = pchs, col = cols)
@


\subsection{Baseline growth}
\label{sec:models-baseline}

For many historical epidemics, available data count deaths due to
multiple causes including the disease of interest, rather
than disease deaths alone. For example, during the 1918 influenza
pandemic, many individuals contracted pneumonia secondary to an
infection with influenza. As a result, many deaths due to influenza
were recorded as deaths due to pneumonia. In this setting, one can
attempt to make inferences about influenza spread from aggregates
of pneumonia and influenza deaths.

Growth in disease mortality over time can be understood from multiple
causes mortality provided that disease deaths and deaths from
other causes (baseline mortality) are modeled separately. To account
for baseline mortality, one can assume that deaths due to causes
other than the disease of interest occur at a constant rate $b > 0$.
Then, for example, the logistic model given by \cref{eq:logistic}
becomes
%
\begin{linenomath}
\begin{equation}
\label{eq:logistic-baseline}
c(t) = b t + \frac{K}{1 + e^{-r (t - \thalf)}}\,,
\end{equation}
\end{linenomath}
%
where $c(t)$ is to be interpreted as cumulative multiple causes
mortality rather than cumulative disease mortality. Accounting for
baseline mortality in this way requires that $b$ is fit in addition
to other model parameters.

\cref{fig:models-baseline} displays the effect of baseline growth
on the logistic curve.

<<models-baseline, fig.height=2.5, fig.cap=paste0("\\textbf{[Left]} Cumulative multiple causes mortality, $c(t)$. Displayed are logistic curves with $r = ", r, "$, $c_0 = ", c0, "$, and $K = ", K, "$ and a linear baseline ($b = ", paste0(b, collapse = ", "), "$; see \\cref{eq:logistic-baseline}). Units of time are characteristic ($1 / r$). \\textbf{[Right]} Interval all causes mortality, $c(t) - c(t - 1)$."), echo=FALSE>>=
r <- 1
c0 <- 1
K <- 1000
b <- c(50, 25, 0)
time <- seq(0, 16, by = 1)
logistic <- function(b) {
  b * time + K / (1 + (K / c0 - 1) * exp(-r * time))
}
cum_inc <- lapply(b, logistic)
int_inc <- lapply(cum_inc, diff)

## Setup
cols <- c("grey30", "grey30", "grey80")
ltys <- c(3, 2, 1)
labs <- paste0("b = ", b)
par(mfrow = c(1, 2), mar = c(3, 4, 1, 3), oma = c(0, 0, 0, 5),
    mgp = c(3, 0.7, 0), las = 1)

## Panel 1
plot.new()
plot.window(xlim = range(time), ylim = c(0, max(unlist(cum_inc)) * 1.1),
            xaxs = "i", yaxs = "i")
for (i in 1:3) {
  lines(time, cum_inc[[i]], lty = ltys[i], lwd = 3, col = cols[i])
}
box(bty = "l")
axis(side = 1)
axis(side = 2)
title(xlab = "time", line = 2)
title(ylab = "cumulative MC mortality")

## Panel 2
plot.new()
plot.window(xlim = range(time), ylim = c(0, max(unlist(int_inc)) * 1.1),
            xaxs = "i", yaxs = "i")
for (i in 1:3) {
  lines(time[-1], int_inc[[i]], lty = ltys[i], lwd = 3, col = cols[i])
}
box(bty = "l")
axis(side = 1)
axis(side = 2)
title(xlab = "time", line = 2)
title(ylab = "interval MC mortality")
legend(x = par("usr")[2] * 1.05, y = mean(par("usr")[3:4]),
       xpd = NA, yjust = 0.5, bty = "n", seg.len = 3, cex = 0.7,
       legend = labs, lty = ltys, lwd = 3, col = cols)
@


\section{Maximum likelihood estimation}
\label{sec:mle}

For $i = 1,\ldots,n$, let $x_i$ be the number of cases observed
between times $t_{i-1}$ and $t_i$. The likelihood of a model
parameter vector $\vec{\theta}$ given data $\vec{x}$ is the
joint probability density of the data given the model, \ie
%
\begin{linenomath}
\begin{equation}
\mathcal{L}(\vec{\theta}|\vec{x}) = \prod_{i=1}^{n} f_i(x_i|\vec{\theta})\,,
\end{equation}
\end{linenomath}
%
where $f_i$ is the density function of $X(t_{i-1},t_i)$
(see \cref{sec:models-observed}). Hence the negative log
likelihood is
%
\begin{linenomath}
\begin{equation}
-\ell(\vec{\theta}|\vec{x})
= -\log \mathcal{L}(\vec{\theta}|\vec{x})
= -\sum_{i=1}^{n} \log f_i(x_i|\vec{\theta})\,.
\end{equation}
\end{linenomath}
%
\pkg{epigrowthfit} writes $-\ell$ in a C++ template and uses package
\pkg{TMB} to carry out automatic differentiation of $-\ell$ with
respect to log-transformed parameters. (All parameters are fit on an
unconstrained logarithmic scale. Hence, for example, \pkg{epigrowthfit}
fits $\log r$ on the interval $(-\infty,\infty)$, instead of $r$ on
the interval $(0,\infty)$.) Function \fun{egf} calls gradient-based
optimizer \fun{nlminb} (alternatively \fun{nlm} or one of the
optimizers provided through \fun{optim}) from base package \pkg{stats}
to search for the parameter vector $\vec{\wh{\theta}}$ minimizing
$-\ell$, namely the maximum likelihood estimate of $\vec{\theta}$.

The iterative optimizers are susceptible to various numerical problems
preventing convergence near to $\vec{\wh{\theta}}$. Poor fits to
the data by \fun{nlminb} are sometimes corrected by trying a different
optimizer, such as the Nelder-Mead algorithm available through
\fun{optim}. Nelder-Mead is considered robust, but as it makes no use
of gradients, it is relatively slow. Speed may not be a concern in
practice, as the models implemented in \pkg{epigrowthfit} have at most
six parameters.


\section{Epidemic parameters derived from \tops{$r$}{r}}
\label{sec:extras}


\subsection{Characteristic and doubling times}
\label{sec:doubling-time}

The characteristic time of an epidemic is the time required for
cumulative incidence to increase by a factor of $e$ at the start
of an epidemic, when cumulative incidence grows roughly exponentially
with rate $r$, \ie when $c(t) \sim c_0 e^{r t}$. The characteristic
time corresponding to an exponential growth rate $r$ is simply $1 / r$.

In practice, it is much easier to talk about the doubling time,
the time required for cumulative incidence to increase by a factor
of 2, which is computed as $(\log 2) / r$. \pkg{epigrowthfit}
function \fun{compute_doubling_time} can be used to compute
doubling times for given $r$. See \cref{sec:ontario-doubling-time}
for an example.


\subsection{Basic reproduction number}
\label{sec:R0}

The basic reproduction number of an infectious disease, commonly denoted
by $\R_0$, is the number of people that an infected person is expected
to infect in an otherwise completely susceptible population. Wallinga
and Lipsitch~\cite[Eq 2.7]{WallLips07} showed that $\R_0$ is determined
by the initial exponential growth rate $r$ and the distribution of the
disease generation interval $\tau_\text{gen}$, the time from infection
of a primary case to infection of a secondary case in a chain of
transmission:
%
\begin{linenomath}
\begin{equation}
\label{eq:R0-general}
\frac{1}{\R_0} = \int_{0}^{\infty} e^{-r s} g(s)\,\text{d}s\,,
\end{equation}
\end{linenomath}
%
where $g$ is the density function of $\tau_\text{gen}$.

In the special case where
%
\begin{enumerate}
\item $g$ is supported on the interval $(\tau_0,\tau_m]$, and
\item there are times $\tau_0 < \tau_1 < \cdots < \tau_m$
  such that $g$ assumes a constant value
  $g_i = p_i / (\tau_i - \tau_{i-1})$
  on the interval $(\tau_{i-1},\tau_i]$ for $i = 1,\ldots,m$,
\end{enumerate}
%
\cref{eq:R0-general} yields
%
\begin{linenomath}
\begin{equation}
\label{eq:R0}
\R_0 = \left. r \middle/ \bigg\{ \sum_{i=1}^{m} \frac{p_i (e^{-r \tau_{i-1}} - e^{-r \tau_i})}{\tau_i - \tau_{i-1}} \bigg\} \right.
\end{equation}
\end{linenomath}
%
This special case arises when $g$ is an empirical distribution,
and $p_i$ is the observed relative frequency of
$\tau_\text{gen} \in (\tau_{i-1},\tau_i]$~\cite[Eq 3.6]{WallLips07}.
\pkg{epigrowthfit} function \fun{compute_R0} evaluates the
right-hand side of \cref{eq:R0} for given $r$, $\tau_i$, and $p_i$,
allowing users who have estimated $r$ using the package's fitting
machinery to also estimate $\R_0$. See \cref{sec:ontario-R0} for
an example.

The special case described above also arises when one assumes
that the latent%
\footnote{The latent period is the time from infection to onset
of infectiousness.}
and infectious%
\footnote{The infectious period is the time from onset of
infectiousness to recovery or death from disease.}
periods $\tau_\text{lat}$ and $\tau_\text{inf}$ are discrete.
To be precise, let $f_\text{lat}$ and $f_\text{inf}$ be the
density functions of $\tau_\text{lat}$ and $\tau_\text{inf}$,
and let $F_\text{inf} = \int f_\text{inf}$ be the distribution
function corresponding to $f_\text{inf}$.
Assuming that infectiousness is constant over the infectious
period, Svensson~\cite[\S 5.1]{Sven07} showed that
%
\begin{linenomath}
\begin{equation}
g = f_\text{lat} * f_\text{wait}\,,\qquad f_\text{wait} = \scalebox{1.1}{$\mathbbm{1}$}_{(0,\infty)} \frac{1 - F_\text{inf}}{\mathbbm{E}[\tau_\text{inf}]}\,.
\end{equation}
\end{linenomath}
%
Here, $*$ denotes
\href{https://en.wikipedia.org/wiki/Convolution}{convolution} and
$f_\text{wait}$ is the density function of the infectious
\emph{waiting time} (the time from onset of infectiousness to
transmission). One can show that if $f_\text{lat}$ and $f_\text{inf}$
have discrete support, then $g$ satisfies the conditions under which
\cref{eq:R0} is valid. \pkg{epigrowthfit} functions \fun{dgi} and
\fun{pgi} evaluate the density and distribution functions of
$\tau_\text{gen}$ implied by distributions of $\tau_\text{lat}$ and
$\tau_\text{inf}$ supported on the positive integers (\ie $1,2,\ldots$
days).  These are helpful if one wants to use \fun{compute_R0} and has
latent and infectious period distributions, but not the corresponding
generation interval distribution.


\subsection{Epidemic final size}
\label{sec:final-size}

Once one has computed $\R_0$ as a function of $r$ (see \cref{sec:R0}),
one can compute the expected epidemic final size $Z$ as a function of
$\R_0$. $Z$ is the proportion of the population expected to be infected
over the course of the epidemic and is obtained from $\R_0$ as
%
\begin{linenomath}
\begin{equation}
\label{eq:final-size-general}
Z = S_0 + \frac{1}{\R_0} W\big(-\R_0 S_0 e^{-\R_0 (S_0 + I_0)} \big)\,,
\end{equation}
\end{linenomath}
%
where $W$ denotes the non-elementary
\href{https://en.wikipedia.org/wiki/Lambert_W_function}{Lambert $W$ function}
and $S_0,I_0 \in [0,1]$ are the proportion of the population
that is susceptible and infected, respectively, at the start
of the epidemic. If the epidemic is seeded in a large population
by one infected individual, then $S_0 \approx 1$ and $I_0 \approx 1$,
and \cref{eq:final-size-general} simplifies to
%
\begin{linenomath}
\begin{equation}
\label{eq:final-size}
Z = 1 + \frac{1}{\R_0} W\big(-\R_0 e^{-\R_0} \big)\,.
\end{equation}
\end{linenomath}
%
Ma and Earn~\cite{MaEarn06} discuss the history and generality
of \cref{eq:final-size-general} and show that it is valid for
a large class of epidemic models, including the SIR model with
multiple infectious stages and arbitrarily distributed infectious
stage durations. \pkg{epigrowthfit} function \fun{compute_final_size}
evaluates the right-hand side of \cref{eq:final-size-general}
for given $\R_0$, $S_0$ and $I_0$. See \cref{sec:ontario-final-size}
for an example.


\section{Example: COVID-19 in Ontario, Canada}
\label{sec:ontario}


\subsection{Loading an epidemic time series}
\label{sec:ontario-data}

\pkg{epigrowthfit} comes with a number of infectious disease
data sets. One of them is \code{canadacovid}, a data frame
listing daily confirmations of COVID-19 in Canadian provinces
and territories from February 14, 2020 to June 21, 2020. You
can load \code{canadacovid} into your global environment using
\fun{data}.

<<ontario-data-1>>=
library(epigrowthfit)
data(canadacovid)
@
%
Documentation for the data set can be accessed by running
\code{?canadacovid}. You can directly examine the data frame
as follows.

<<ontario-data-2>>=
dim(canadacovid) # dimensions
head(canadacovid, 10) # first 10 rows
levels(canadacovid$province) # province and territory labels
sapply(canadacovid, class) # variable classes
@
%
Let's consider just the data from Ontario.

<<ontario-data-3>>=
ontario <- subset(canadacovid, province == "ON")
dim(ontario)
@
%
\pkg{epigrowthfit} does not tolerate missing values in the interval
incidence time series. You can remove rows containing \code{`1NA`1}
using \fun{na.omit}.

<<ontario-data-4>>=
ontario <- na.omit(ontario)
dim(ontario)
sum(is.na(ontario$new_confirmations)) # check that no NA remain
@
%
Just removing a date with a missing value is sufficient only if the
missing number of cases is carried over to the next observation time.
Here, we assume that this is true, but if that is not the case, then
\href{https://en.wikipedia.org/wiki/Imputation_(statistics)}{imputation}
is necessary unless the missing number is expected to be negligible.


\subsection{Initializing the fitting machinery with \tops{\protect\fun{egf_init}}{egf\_init()}}
\label{sec:ontario-init}

We will fit a Richards model (see \cref{sec:models-expected}) with
negative binomial observations (see \cref{sec:models-observed}) to
the \code{ontario} data. The optimization (see \cref{sec:mle}) is
carried out by function \fun{egf}, but requires initialization.
Before calling \fun{egf}, we use function \fun{egf_init} to define
(i) a fitting window and
(ii) initial estimates of all model parameters.

\paragraph{Fitting window.} The ``fitting window'' is the subset of
data (\ie the range of dates) considered when fitting a model.
Ma \etal~\cite{Ma+14} examine the sensitivity of the fitted initial
exponential growth rate to the endpoints of the fitting window.
A reasonable initial approach for the logistic and Richards models,
based on their analysis, is to start at the time of the first
observed case and to end at the time of the peak in interval
incidence (equivalently, the inflection point in cumulative incidence;
see \cref{fig:data-types}). If the peak has not occurred (\eg when
fitting in real time, during a growing epidemic), then end at the
last observation time.

\fun{egf_init} implements the above approach by default. However,
for an exponential model, this results in underestimation of $r$,
because epidemic growth becomes subexponential much earlier than
the peak in interval incidence. When fitting an exponential model,
it is typically best to choose the window where interval incidence
is roughly linear on a logarithmic scale.

Regardless of the model being fit, one may need to experiment with
different fitting windows. Optional \fun{egf_init} arguments, namely
\code{min_wlen}, \code{peak}, \code{first}, \code{first_level}, and
\code{skip_zero}, can be set explicitly to override the default window
selection algorithm. For details, run \code{?egf_init}.

\paragraph{Initial parameter estimates.} The optimization algorithms
employed by \pkg{epigrowthfit} (see \cref{sec:mle}) are iterative and
require an initial estimate $\vec{\theta}^{(0)}$ of parameter vector
$\vec{\theta}$. The exact choice of $\vec{\theta}^{(0)}$ is typically
not critical, but a reasonable initial guess can promote convergence.
Reasonable initial estimates of $r$
(\cref{eq:exponential,eq:logistic,eq:richards})
and $c_0$ (\cref{eq:exponential}) are $\beta_1$ and $e^{\beta_0}$,
respectively, where $\beta_1$ and $\beta_0$ are the slope and intercept
of a linear model fit to the logarithm of cumulative incidence, using
only observations in the fitting window. If the interval incidence
curve is roughly symmetric, then an estimate of $\thalf$
(\cref{eq:logistic,eq:richards}) is supplied by the time of its peak.
$K$ (\cref{eq:logistic,eq:richards}) should be estimated to be no less
than the number of cases observed to date and no more than the
population size. When fitting a Richards model (\cref{eq:richards}),
one can initially assume logistic growth by estimating that $p$ is 1.
When including baseline growth in a model
(see \cref{sec:models-baseline}), one can use the average mortality
rate in the years preceding and following the epidemic as an initial
estimate of $b$.

With the exception of $b$, \fun{egf_init} employs these initial
estimates (taking the lower bound on $K$) by default. For simplicity,
the default initial estimates of parameters $b$ and $k$ are 1
day$^{-1}$ and 1, respectively. Optional \fun{egf_init} argument
\code{theta0} can be set explicitly to override any of these defaults.
For details, run \code{?egf_init}.

\par\nolinenumbers\,\par\linenumbers

Below, we pass to \fun{egf_init} a Date vector \code{date}
listing times $t_0 < t_1 < \cdots < t_n$ and a numeric vector
\code{cases} with length \code{`9length`9(date)-`11`1}
specifying interval incidence $x_i$ for $i = 1,\ldots,n$,
where $x_i$ is the number of cases observed between times
$t_{i-1}$ and $t_i$. We indicate the model we want to fit
using these arguments:

\nolinenumbers\par\vspace{6pt}
\begin{tabular}{rl}
\code{curve} & options \code{`2"exponential"`2}, \code{`2"logistic"`2} (default), or \code{`2"richards"`2} \\
\code{distr} & options \code{`2"pois"`2} or \code{`2"nbinom"`2} (default) \\
\code{include_baseline} & options \code{`1TRUE`1} or \code{`1FALSE`1} (default)
\end{tabular}
\vspace{6pt}\par\linenumbers\noindent
%
We accept the default values of all other arguments, letting
\fun{egf_init} select a fitting window and initial parameter
estimates for us as described above.

<<ontario-init-1>>=
init <- egf_init(
  date = ontario$date,
  cases = ontario$new_confirmations[-1],
  curve = "richards",
  distr = "nbinom",
  include_baseline = FALSE
)
@

The output of the initialization is an \code{egf_init} object---%
a list with \Sexpr{length(init)} elements containing information
used by \fun{egf} and methods for class \code{egf_init}.

<<ontario-init-2>>=
class(init)
names(init)
@
%
Elements \code{date}, \code{cases}, \code{curve}, \code{distr}, and
\code{include_baseline} are copies of the arguments of \fun{egf_init}.
Element \code{time} expresses each date listed in \code{date} as a
number of days since \code{date[`11`1]}.

<<ontario-init-3>>=
with(init, identical(time, as.numeric(date - date[1])))
@
%
Elements \code{first} and \code{last} define the selected fitting
window. They are integer indices such that \code{cases[first]}
and \code{cases[last]} are the first and last observations in the
window. Element \code{theta0} is a numeric vector listing initial
parameter estimates, which in this example were all selected by
\fun{egf_init}. Element \code{log_theta0} gives the log-transformed
estimates. To retrieve \code{theta0} and \code{log_theta0}, you can
use the \fun{coef} method for class \code{egf_init}.

<<ontario-init-4>>=
coef(init) # same as `init$theta0`
coef(init, log = TRUE) # same as `init$log_theta0`
@
%
Here, $r$ is expressed in units per day and $\thalf$ in days.

Element \code{cum_inc} is a closure%
%
\footnote{A closure is formally defined as a pairing of a function
and an environment binding free variables---variables not defined
among the formal arguments or in the body of the function. By this
definition, nearly every function in R is a closure. Nonetheless,
we use the word ``closure'' instead of ``function'' here
to emphasize that function \fun{cum_inc} is not self-contained:
its evaluation \emph{relies on} the definition of free variables
\code{theta0}, \code{curve}, and \code{include_baseline} (which
can be seen by printing \code{cum_inc}). \fun{cum_inc} retrieves
these variables from its enclosing environment,
\code{`9environment`9(cum_inc)}, which is the execution environment
of its parent function \fun{egf_init}.}
%
taking numeric arguments \code{time} and \code{theta} (defaults
are elements \code{time} and \code{theta0} of the \code{egf_init}
object) that specify times in days since \code{date[`11`1]} and
parameter values named as in \code{theta0}.
\code{`9cum_inc`9(time, theta)} evaluates the expected cumulative
incidence curve (in this case \cref{eq:richards}) at \code{time}
days conditional on parameter vector \code{theta}. The \fun{predict}
method for class \code{egf_init} provides a convenient interface
to \fun{cum_inc}.

<<ontario-init-5>>=
predict(init, time = init$time[1:10])
@
%
Here, \code{int_inc} is simply \code{`9diff`9(cum_inc)}, the
interval incidence time series associated with \code{cum_inc}.
This means that \code{`9length`9(int_inc)} is
\code{`9length`9(cum_inc)-`11`1}.

Finally, element \code{call} is the call to \fun{egf_init},
so that the returned \code{egf_init} object is reproducible
with \code{`9eval`9(call)}.


\subsection{Plotting and printing \tops{\protect\code{egf_init}}{egf\_init} objects}
\label{sec:ontario-init-plot}

The \fun{plot} method for class \code{egf_init} displays together
the main components of an \code{egf_init} object. It takes an
argument \code{inc} indicating the type of incidence to plot:
\code{`2"cumulative"`2} (default) or \code{`2"interval"`2}.

<<ontario-init-plot-1, fig.height=7, fig.cap='Result of \\code{`9plot`9(init, inc)}: \\textbf{[Top]} cumulative incidence (\\code{inc = `2"cumulative"`2}) and \\textbf{[Bottom]} interval incidence (\\code{inc = `2"interval"`2}). Points are observed incidence. Lines are expected incidence conditional on initial parameter estimates listed at the bottom of the right margin. Initial parameter estimates are guesses, hence the lines are \\emph{not} fitted to the data. Incidence is plotted on a logarithmic scale, with zeros plotted directly on the horizontal axis. Vertical dashed lines indicate the selected fitting window. For interval incidence, a coloured point at time $t_i$ indicates that the observation interval $t_i-t_{i-1}$ differs from the median observation interval (in this case 1 day).'>>=
par(mfrow = c(2, 1)) # create a two-panel plot
plot(init, inc = "cumulative")
plot(init, inc = "interval")
@

Plots of interval incidence should be interpreted with care.
The displayed curve is supported on a grid of equally spaced time
points, and the spacing chosen is the median observation interval.

<<ontario-init-plot-2>>=
## Grid spacing in days
dt <- diff(init$time)
median(dt)
@
%
However, the displayed data may not be equally spaced. This is the
case in the \code{ontario} data, where most observations are counts
over one day, but some are counts over two days.

<<ontario-init-plot-3>>=
## Frequency table
dtt <- table(dt)
names(dtt) <- paste0(names(dtt), "d")
dtt
@
%
Counts over fewer or more days than the median observation interval
will tend to deviate from the plotted curve not due to chance, but
due to scale. These exceptional counts should be noted when visually
assessing whether the plotted curve fits the data. The \fun{plot}
method for class \code{egf_init} highlights exceptional counts
(see \cref{fig:ontario-init-plot-1}) according to optional argument
`tol` (default is 0).

<<ontario-init-plot-4, eval=FALSE>>=
## Not run

## Highlight all exceptional counts
plot(init, inc = "interval", tol = 0)

## Highlight only if observation interval differs
## from median by more than 2.5 per cent
plot(init, inc = "interval", tol = 0.025)

## Disable highlighting
plot(init, inc = "interval", tol = Inf)
@

The exact endpoints of the fitting window selected by \fun{egf_init}
are not obvious from \cref{fig:ontario-init-plot-1}. The \fun{print}
method for class \code{egf_init} will tell you more about the window
and remind you that actually fitting a model requires a call to
\fun{egf} (see \cref{sec:ontario-fit}).

<<ontario-init-plot-5>>=
init # same as `print(init)`
@


\subsection{Fitting the model with \tops{\protect\fun{egf}}{egf()}}
\label{sec:ontario-fit}

\cref{fig:ontario-init-plot-1} shows that our initial guess
$\vec{\theta}^{(0)}$ of the model parameter vector $\vec{\theta}$
produces a poor fit to the \code{ontario} data. Function \fun{egf}
uses this initial guess as a starting point in an optimization
algorithm to obtain a superior estimate---specifically, a
numerical approximation of the maximum likelihood parameter
vector $\vec{\wh{\theta}}$ (see \cref{sec:mle}).

The first argument of \fun{egf} expects an \code{egf_init}
object. An optional second argument \code{method} lets you
specify an optimization algorithm. The options are
\code{`2"nlminb"`2} (default), \code{`2"nlm"`2},
\code{`2"Nelder-Mead"`2}, \code{`2"BFGS"`2},
\code{`2"L-BFGS-S"`2}, and \code{`2"CG"`2}.
Further optional arguments will be passed directly to
\fun{nlminb}, \fun{nlm}, or \fun{optim}, depending on
\code{method}. Below, we pass \code{init} to \fun{egf},
accepting the default \code{method = `2"nlminb"`2}.

<<ontario-fit-1>>=
fit <- egf(init)
@

\fun{egf} returns an \code{egf} object---a list with
\Sexpr{length(init)} elements specifying the fitted model
and other information used by methods for class \code{egf}.

<<ontario-fit-2>>=
class(fit)
names(fit)
@
%
Elements \code{init} and \code{method} are copies of the arguments
of \fun{egf}. Element \code{theta_hat} is a numeric vector listing
\emph{fitted} parameter estimates, and element \code{log_theta_hat}
gives the log-transformed estimates. To retrieve \code{theta_hat}
and \code{log_theta_hat}, you can use the \fun{coef} method for
class \code{egf}.

<<ontario-fit-3>>=
coef(fit) # same as `fit$theta_hat`
coef(fit, log = TRUE) # same as `fit$log_theta_hat`
@
%
Once again, $r$ is expressed in units per day and $\thalf$ in days.

Element \code{nll} is the negative log likelihood of
\code{log_theta_hat}.%
\footnote{Recall from \cref{sec:mle} that \pkg{epigrowthfit} fits
all model parameters on an unconstrained logarithmic scale.
As a result, it defines the negative log likelihood as a function
of the log-transformed parameters.}
Elements \code{nll_func} and \code{nll_grad} are closures taking
a single numeric argument \code{log_theta} (default is
\code{log_theta_hat}) specifying log-transformed parameter values
in the order used by \code{log_theta_hat}. They evaluate the
negative log likelihood function and its gradient with respect to
log-transformed parameters at \code{log_theta}.

<<ontario-fit-4>>=
## Temporarily add list subset to search path
attach(fit[c("log_theta_hat", "nll", "nll_func", "nll_grad")],
  name = "tmp"
)

nll_func(log_theta_hat)
nll_grad(log_theta_hat)
identical(nll, nll_func(log_theta_hat))
@
%
It is a good idea to check that:
%
\begin{enumerate}
\item the norm of the gradient evaluated at \code{log_theta_hat}
  is some orders of magnitude less than one;
\item that small, random perturbations of \code{log_theta_hat}
  have lower likelihood (greater negative log likelihood).
\end{enumerate}
%
These checks provide some assurance that \fun{theta_hat} approximates
a local maximum of the likelihood surface. They do not, however, rule
out the possibility of multiple local maxima. Hence \code{theta_hat}
may still not approximate the global maximum. Such apprehensions are
typically resolved by plotting the fitted model to assess goodness of
fit (see \cref{sec:ontario-fit-plot}).

<<ontario-fit-5>>=
## Check 1
norm(nll_grad(log_theta_hat))

## Check 2
n <- 1e04
ptb <- replicate(n, rnorm(log_theta_hat, mean = 0, sd = 0.05))
lth_ptb <- log_theta_hat + ptb
nll_ptb <- apply(lth_ptb, 2, nll_func)
head(nll_ptb)
sum(nll_ptb > nll) / n # should be close to 1

## Remove list subset from search path
detach("tmp")
@

Element \code{cum_inc} of the \code{egf} object is a closure
identical to that found in the \code{egf_init} object, except
that the default value of argument \code{theta} is \code{theta_hat},
not \code{theta0}. The \fun{predict} method for class \code{egf}
uses \fun{cum_inc} to evaluate the fitted model of expected
cumulative incidence at desired times.

<<ontario-fit-6>>=
predict(fit, time = init$time[1:10])
@
%
Once again, \code{int_inc} is simply \code{`9diff`9(cum_inc)},
the interval incidence time series associated with \code{cum_inc}.

Elements \code{madf_out} and \code{optim_out} save the output
of \pkg{TMB} function \fun{MakeADFun} and the \pkg{stats}
optimizer specified by \fun{egf} argument \code{method}
(\fun{nlminb}, \fun{nlm}, or \fun{optim}). These are retained
in the \code{egf} object to allow the user to diagnose poor
fits and warnings and errors issued by the optimizer.
For details, consult the relevant documentation
(\eg by running \code{?TMB::MakeADFun}).

Finally, element \code{call} contains the call to \fun{egf}.
The \fun{egf} object is reproducible with \code{`9eval`9(call)}.


\subsection{Plotting and printing \tops{\protect\code{egf}}{egf} objects}
\label{sec:ontario-fit-plot}

The \fun{plot} and \fun{print} methods for class \code{egf}
replicate the methods for class \code{egf_init}
(see \cref{sec:ontario-init-plot}), except that what is plotted
and printed is no longer an initial guess but a fit to the data
obtained by an optimization algorithm.

<<ontario-fit-plot-1, fig.height=7, fig.cap='Result of \\code{`9plot`9(fit, inc)}: \\textbf{[Top]} cumulative incidence (\\code{inc = `2"cumulative"`2}) and \\textbf{[Bottom]} interval incidence (\\code{inc = `2"interval"`2}). Points are observed incidence. Lines are expected incidence conditional on fitted parameter estimates listed at the bottom of the right margin. Incidence is plotted on a logarithmic scale, with zeros plotted directly on the horizontal axis. Vertical dashed lines indicate the selected fitting window. For interval incidence, a coloured point at time $t_i$ indicates that the observation interval $t_i-t_{i-1}$ differs from the median observation interval (in this case 1 day).'>>=
par(mfrow = c(2, 1)) # create a two-panel plot
plot(fit, inc = "cumulative")
plot(fit, inc = "interval")
@

<<ontario-fit-plot-2>>=
fit # same as `print(fit)`
@
%
Comparing \cref{fig:ontario-init-plot-1,fig:ontario-fit-plot-1}
shows that the fit is much better than the initial guess (as one
might hope!).

If you would like to hand-craft a plot (\eg one with a different
colour palette or without annotation), then do not rely on the
\fun{plot} method for \code{egf} objects. You can retrieve the
interval incidence data from \code{init} elements \code{date}
and \code{cases}, and you can evaluate the fitted cumulative
incidence curve at desired time points using the \fun{predict}
method for \code{egf} objects (see \cref{sec:ontario-fit}).

\subsection{Simulating the fitted model}
\label{sec:ontario-fit-simulate}

Simulations of a fitted model can be used to assess the robustness
of the model to observation error (see \cref{sec:models-observed}).
You can perform simulations using the \fun{simulate} method for class
\code{egf}. Arguments \code{nsim} and \code{seed} specify a number
of simulations and an optional seed for random number generation, used
to make simulations reproducible. You can specify time points with
optional argument \code{time}. The default is \code{object$init$time},
where \code{object} is the \code{egf} object. This matches the time
points in your simulations to the time points in your data, which is
typically what you want.

<<ontario-fit-simulate-1>>=
sim <- simulate(fit, nsim = 6, seed = 1257)
@

\fun{simulate} returns an \code{egf_sim} object---a list with elements
\code{fit} and \code{time} containing copies of \fun{simulate}
arguments \code{object} and \code{time}; an element \code{refdate}
storing reference date \code{object$init$date[`11`1]}; and elements
\code{int_inc} and \code{cum_inc}, both matrices storing simulations.

<<ontario-fit-simulate-2>>=
names(sim)
length(sim$time) # number of time points
t(sapply(sim[c("int_inc", "cum_inc")], dim)) # dimensions of each matrix
@

Matrix \code{int_inc} has \code{`9length`9(time)-`11`1} rows and
\code{nsim} columns. \code{int_inc[i, j]} is the number of cases
observed between \code{time[i]} and \code{time[i+`11`1]} in
simulation \code{j}. Each element of row vector \code{int_inc[i, ]}
is sampled independently from a Poisson or negative binomial
distribution (depending on \code{object$init$distr}, in this case
\code{`2"nbinom"`2}) with mean equal to
\code{`9predict`9(object, time)$int_inc[i]}.
The negative binomial dispersion parameter, if used, takes
its fitted value \code{object$theta_hat[[`2"nbdisp"`2]]}.

Matrix \code{cum_inc} has \code{`9length`9(time)} rows and
\code{nsim} columns. \code{cum_inc[i, j]} is the number of
cases observed up to \code{time[i]} in simulation \code{j}.
Column vector \code{cum_inc[, j]} is computed as
\code{c0 + `9cumsum`9(`9c`9(`10`1, int_inc[, j]))}, where
\code{c0 = `9predict`9(object, time)$cum_inc[`11`1]} is the
value of expected cumulative incidence at \code{time[`11`1]}
in the fitted model.

<<ontario-fit-simulate-3>>=
lapply(sim[c("time", "int_inc", "cum_inc")], head)
@

The \fun{plot} method for class \code{egf_sim} plots the simulated
time series in transparent grey and the predicted curve in colour.
It takes an optional argument \code{alpha}, whose value---a number
between 0 and 1 (default is 0.5)---controls the transparency. A
number closer to 0 (more transparency) tends to be desirable when
the number of simulations is large.

<<ontario-fit-simulate-4, fig.height=2.5, fig.cap='Result of \\code{`9plot`9(sim, inc)}: \\textbf{[Left]} cumulative incidence (\\code{inc = `2"cumulative"`2}) and \\textbf{[Right]} interval incidence (\\code{inc = `2"interval"`2}). Simulated time series are plotted in grey with transparency. Predicted curves are plotted in colour.'>>=
par(mfrow = c(1, 2))
plot(sim, inc = "cumulative", alpha = 0.5)
plot(sim, inc = "interval", alpha = 0.3)
@


\subsection{Epidemic parameters derived from \tops{$r$}{r}}
\label{sec:ontario-extras}

Here, we demonstrate \pkg{epigrowthfit} functions
\fun{compute_doubling_time}, \fun{compute_R0}, and
\fun{compute_final_size}, which can be used to compute
salient epidemic parameters (see \cref{sec:extras})
from initial exponential growth rates $r$. We will
make use of the \code{egf_init} and \code{egf} objects
obtained in \cref{sec:ontario-init,sec:ontario-fit},
namely \code{init} and \code{fit}. These contain our
initial and final estimates of $r$ for the Ontario data.


\subsubsection{Doubling time}
\label{sec:ontario-doubling-time}

\fun{compute_doubling_time} is a generic function taking a single
argument \code{x}, which can be a numeric vector listing initial
exponential growth rates $r$ in units day$^{-1}$, an \code{egf_init}
object, or an \code{egf} object. If \code{x} is a numeric vector,
then it returns \code{`9log`9(`12`1)/x}, a numeric vector listing
doubling times in days corresponding to the elements of \code{x}.
If \code{x} is an \code{egf_init} object, then it extracts the
fitted value of $r$ from the object and returns the corresponding
doubling time, \ie it returns
\code{`9log`9(`12`1)/object$theta0[[`2"r"`2]]}.
Finally, if \code{x} is an \code{egf} object, then it extracts
the initial value of $r$ from the object and returns the corresponding
doubling time, \ie it returns
\code{`9log`9(`12`1)/object$theta_hat[[`2"r"`2]]}.

<<ontario-doubling-time-1>>=
compute_doubling_time(10^(-2:0)) # method for class "numeric"
compute_doubling_time(init) # method for class "egf_init"
compute_doubling_time(fit) # method for class "egf"
@


\subsubsection{Basic reproduction number}
\label{sec:ontario-R0}

\fun{compute_R0} is a generic function that uses \cref{eq:R0}
to calculate the basic reproduction number $\R_0$ as a function
of the initial exponential growth rate $r$. This calculation
depends on interval endpoints $\tau_i$ and bin probabilities
$p_i$ defining a binned generation interval distribution
(see \cref{sec:R0}).

Function \fun{compute_R0} takes numeric arguments \code{x},
\code{breaks}, and \code{probs}, specifying $r$ in units
day$^{-1}$, $\tau_i$ in units days, and $p_i$, respectively.
It is vectorized in \code{x}, so \code{x} can have length
greater than 1. Alternatively, \code{x} can be an \code{egf_init}
or \code{egf} object, in which case a value for $r$ is
taken from \code{object$theta0[[`2"r"`2]]} or
\code{object$theta_hat[[`2"r"`2]]}, respectively.
\code{probs} must have length \code{`9length`9(breaks)-`11`1},
and \code{probs[i]} should define the probability that the
generation interval is between \code{breaks[i]} and
\code{breaks[i+1]} days.%
\footnote{It is actually sufficient for \code{probs} to give
probability \emph{weights}, because \fun{compute_R0} normalizes
for you, \ie it assigns \code{probs <- probs / `9sum`9(probs)}.
internally. Here, for clarity, we do the normalization up front.}

For the purpose of demonstration, we assume that the generation
interval in days follows a log-normal distribution with parameters
$\mu = \log 5$ and $\sigma^2 = 0.25^2$, truncated at 10
(see \cref{fig:dlnorm}).%
\footnote{A log-normal distribution with parameters $\mu$ and
$\sigma^2$ is the distribution of the natural logarithm of a
normally distributed random variable with mean $\mu$ and variance
$\sigma^2$, \ie it is the distribution of $Y = \log(X)$,
where $X \sim \mathrm{Normal(\mu,\sigma^2)}$.}
Then the probability $p_i$ that the generation interval is between
$i-1$ and $i$ days is given by
%
\begin{equation}
p_i = \frac{F(i) - F(i-1)}{F(10)}\,,\qquad i = 1,\ldots,10\,,
\end{equation}
%
where $F$ is the untruncated log-normal distribution function.
We calculate these $p_i$ as follows, using \pkg{stats} function
\fun{plnorm} to evaluate $F$.

<<ontario-R0-1>>=
breaks <- 0:10
probs <- diff(plnorm(breaks, meanlog = log(5), sdlog = 0.25))
probs <- probs / sum(probs)
@
%
We are now equipped to call \fun{compute_R0}.

<<ontario-R0-2>>=
compute_R0(10^(-2:0), breaks, probs) # method for class "numeric"
compute_R0(init, breaks, probs) # method for class "egf_init"
compute_R0(fit, breaks, probs) # method for class "egf"
@

<<dlnorm, fig.height=2.5, fig.cap=paste0("The density function of a random variable whose distribution is $\\mathrm{Lognormal}(\\mu = \\log ", exp_mu, ", \\sigma^2 = ", sigma, "^2)$ and truncated at ", end, "."), echo=FALSE>>=
end <- 10
x <- seq(0, end, by = 0.2)
mu <- log(5)
exp_mu <- exp(mu)
sigma <- 0.25
Fend <- plnorm(end, meanlog = mu, sdlog = sigma)
fx <- dlnorm(x, meanlog = mu, sdlog = sigma) / Fend

## Setup
par(mar = c(3, 4, 1, 1), mgp = c(3, 0.7, 0), las = 1)

## Plot
plot.new()
plot.window(xlim = c(0, end+1), ylim = c(0, max(fx) * 1.04),
            xaxs = "i", yaxs = "i")
lines(x, fx, lwd = 3, col = "grey80")
box(bty = "l")
axis(side = 1, at = 0:10)
axis(side = 2)
title(xlab = "value", line = 2)
title(ylab = "probability density")
@


\subsubsection{Epidemic final size}
\label{sec:ontario-final-size}

\fun{compute_final_size} uses \cref{eq:final-size-general}
to calculate the expected epidemic final size $Z$ as a
function of the basic reproduction number $\R_0$ and the initial
susceptible and infected proportions $S_0,I_0 \in [0,1]$.
Accordingly, it takes numeric arguments \code{R0}, \code{S0},
and \code{I0}. \fun{compute_final_size} is vectorized in every
argument. To make this possible, \code{R0}, \code{S0}, and
\code{I0} are recycled up to the length of the longest of the
three vectors, and $Z$ is computed element-wise.

Below, we accept the defaults for \code{S0} and \code{I0},
which are 1 and 0. These are valid for epidemics seeded in
a large population by one infected individual.

<<ontario-final-size-1>>=
R0 <- 10^seq(0, 1, by = 0.05)
compute_final_size(R0)
@

\section{Missing functionality (to be implemented soon)}

\begin{itemize}
\item Confidence intervals on fitted incidence curves
  and parameter estimates.
\item Mixed effects models accounting for heterogeneity between
  jurisdictions in a principled way.
\end{itemize}


\end{document}

