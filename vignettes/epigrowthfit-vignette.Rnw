\documentclass[dvipsnames,12pt]{article}
%\VignetteIndexEntry{epigrowthfit-vignette}
%\VignetteEngine{knitr::knitr}


% Page layout
\usepackage[top=1in,bottom=1.5in,left=1in,right=1in]{geometry}
\usepackage{lineno} % line numbering
\renewcommand{\linenumberfont}{\normalfont\tiny\sffamily\color[gray]{0.5}}
\hfuzz=1in % tolerate \hbox fullness
\vbadness=\maxdimen % tolerate \vbox badness

% Text layout
%\usepackage{setspace} % \onehalfspacing, \doublespacing
%\raggedright
\usepackage[T1]{fontenc} % words with accented characters can be hyphenated

% Title page setup
\usepackage{titling}
\usepackage{graphicx}
%\makeatletter % defines \thanks without mark
%\def\thanks#1{\protected@xdef\@thanks{\@thanks\protect\footnotetext{#1}}}
%\makeatother
\pretitle{\begin{center}\includegraphics[height=1.5cm]{logo.pdf}\thanks{Thanks to Julianne Guevara for the logo.}\\[1in]}
%\pretitle{\begin{center}\includegraphics[height=1.5cm]{logo.pdf}\\[1in]}
\title{\Large The \textbf{epigrowthfit} package}
\posttitle{\end{center}}
\author{Mikael Jagan, Benjamin M.\ Bolker, Junling Ma,\\ David J.\ D.\ Earn, Jonathan Dushoff}
\date{\today}

% Deferred execution
% front matter
\usepackage{etoolbox} % document hooks
\AfterEndPreamble{%
  \pagenumbering{roman}
  \maketitle
  \tableofcontents
  \thispagestyle{empty}
  \clearpage
  \pagenumbering{arabic}
  \linenumbers
}
% back matter
\AtEndDocument{%
  \bibliographystyle{vancouver}
  \bibliography{epigrowthfit-vignette}
  \addcontentsline{toc}{section}{References}
}

% Math
\usepackage{amsmath,mathtools}
\usepackage{amssymb,bm,bbm}
\allowdisplaybreaks % page breaks in display style math mode

% Code
\usepackage[scaled=0.85]{DejaVuSansMono}
% Match inline code highlighting to custom knitr theme
% specified in `knit_theme.css`
\definecolor{background}{HTML}{f4f4f4} % background
\definecolor{num}{HTML}{aa4499} % numeric, logical, NA
\definecolor{str}{HTML}{999933} % character
\definecolor{com}{HTML}{999999} % comment
\definecolor{opt}{HTML}{555555} % !?
\definecolor{std}{HTML}{555555} % variable name, operator, delimiter
\definecolor{kwa}{HTML}{aa4499} % function, if, else, for, in, while, NULL
\definecolor{kwb}{HTML}{555555} % assignment operator
\definecolor{kwc}{HTML}{555555} % function argument
\definecolor{kwd}{HTML}{3a9183} % function name
\usepackage{listings} % \lstinline
\lstset{%
  basicstyle=\color{std}\ttfamily,%
  breaklines=true,%
  moredelim=[is][\color{num}]{`1}{`1},
  moredelim=[is][\color{str}]{`2}{`2},
  moredelim=[is][\color{com}\itshape]{`3}{`3},
  moredelim=[is][\color{opt}]{`4}{`4},
  moredelim=[is][\color{std}]{`5}{`5},
  moredelim=[is][\color{kwa}\bfseries]{`6}{`6},
  moredelim=[is][\color{kwb}]{`7}{`7},
  moredelim=[is][\color{kwc}]{`8}{`8},
  moredelim=[is][\color{kwd}]{`9}{`9}
}

% Float captions
\usepackage{caption}
\captionsetup{%
  aboveskip=8pt,%
  labelfont=bf,%
  labelsep=period,%
  justification=raggedright,%
  singlelinecheck=false%
}
\renewcommand{\figurename}{Fig}

% Float placement
\usepackage{float} % \begin{figure}[H]
\usepackage[section]{placeins} % \FloatBarrier

% Lists
\usepackage{enumitem}
\setlist[enumerate]{label=(\roman*)}
\setlist[itemize]{label=\tiny$\blacksquare$}

% Tables
\usepackage{booktabs} % \toprule, \midrule, \bottomrule, \addlinespace
\usepackage{array}
% columns with variable width, top alignment
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

% Citation
\usepackage{cite}
\makeatletter
\renewcommand{\@biblabel}[1]{\quad #1.} % \@biblabel format
\makeatother

% Ref hyperlinks
\usepackage[colorlinks=true,linkcolor=magenta,citecolor=green,urlcolor=blue]{hyperref}
\usepackage[nameinlink,capitalize]{cleveref}
% equation
\crefformat{equation}{#2Eq~#1#3}
\crefmultiformat{equation}{#2Eqs~#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{equation}{#3Eqs~#1#4--#5#2#6}
\crefformat{blankequation}{#2#1#3}
\crefmultiformat{blankequation}{#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{blankequation}{#3#1#4--#5#2#6}
\crefalias{blankequation}{equation}
\crefformat{pluralequation}{#2Eqs~#1#3}
\crefalias{pluralequation}{equation}
% figure
\crefformat{figure}{#2Fig~#1#3}
\crefmultiformat{figure}{#2Figs~#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{figure}{#3Figs~#1#4--#5#2#6}
% section
\crefformat{section}{#2\S#1#3}
\crefmultiformat{section}{#2\S\S#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{section}{#3\S\S#1#4--#5#2#6}
% table
\crefformat{Table}{#2Table~#1#3}
\crefmultiformat{table}{#2Tables~#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{table}{#3Tables~#1#4--#5#2#6}

% More macros
% laziness
\let\tops\texorpdfstring
% fonts
\renewcommand{\vec}[1]{\bm{\mathrm{#1}}}
% diacritics
\let\wh\widehat
\let\wt\widetilde
% delimiters
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
% operators
\DeclareMathOperator*{\argmin}{arg\,min}
% symbols
\newcommand{\thalf}{t_\text{\normalfont half}}
\newcommand{\R}{\mathcal{R}}
% abbreviations
\newcommand{\cf}{\textit{cf}.~}
\newcommand{\eg}{\textit{e}.\textit{g}.,~}
\newcommand{\ie}{\textit{i}.\textit{e}.,~}
\newcommand{\etc}{\textit{etc}.}
\newcommand{\etal}{\textit{et al}.}
% code
\newcommand{\code}[1]{\mbox{\lstinline|#1|}}
\newcommand{\fun}[1]{\code{`9#1`9()}}
\let\pkg\textbf
\newcommand{\cran}[1]{\href{https://CRAN.R-project.org/package=#1}{\textbf{#1}}}
% comments
\newcommand{\comment}[3]{\textcolor{#1}{\textbf{[#2: }\textit{#3}\textbf{]}}}
\newcommand{\david}[1]{\comment{magenta}{DE}{#1}}
\newcommand{\mikael}[1]{\comment{blue}{MJ}{#1}}

%%%%%%%%%%%%%%%%
%% START HERE %%
%%%%%%%%%%%%%%%%

\begin{document}
\setlength{\parskip}{0.5mm}
\setlength{\parindent}{7mm}

<<set-chunk-defaults, echo=FALSE>>=
library("knitr")
## Make some substitutions in the .tex output in order to:
## * Prevent lineno from messing up breaking of chunks over pages.
## * Prevent automatic indentation after chunks.
## * Dispense with compile errors due to knitr-xcolor interaction.
##   See https://tex.stackexchange.com/questions/148188/.
knit_hooks$set(document = function(x) {
  x <- sub("\\begin{knitrout}", "\\nolinenumbers\\begin{knitrout}", x, fixed = TRUE)
  x <- sub("\\end{knitrout}", "\\end{knitrout}\\linenumbers\\noindent", x, fixed = TRUE)
  sub("\\usepackage[]{color}", "\\usepackage{xcolor}", x, fixed = TRUE)
})
## Set number of digits printed in chunk output with chunk option "digits"
knit_hooks$set(digits = function(before, options, envir) {
  if (before) {
    options(digits = options$digits)
  }
})
## Set character width of chunk with chunk option "width"
## (can be used to prevent intrusions into right margin)
knit_hooks$set(char.width = function(before, options, envir) {
  if (before) {
    options(width = options$char.width)
  }
})
## Use custom palette for code highlighting
knit_theme$set(knit_theme$get("knit_theme.css"))
## Set chunk defaults
opts_chunk$set(
  cache = TRUE, # if `FALSE`, chunk is evaluated from scratch with compile
  echo = TRUE, # if `FALSE`, chunk is not displayed
  eval = TRUE, # if `FALSE`, chunk is not evaluated
  include = TRUE, # if `FALSE`, chunk output is not displayed and `error = FALSE`
  error = TRUE, # if `FALSE`, evaluation stops on errors
  warning = TRUE, # if `FALSE`, warnings printed in console, not document
  message = FALSE, # if `FALSE`, messages printed in console, not document
  digits = 7, # number of digits printed in chunk output
  char.width = 74, # character width of chunks
  fig.pos = "H", # figure position in document
  fig.align = "center", # figure alignment
  dev = "pdf", # plotting device
  dev.args = list(pointsize = 10), # base point size in plots
  eval.after = "fig.cap", # chunk options to be evaluated after chunk
  strip.white = TRUE # reduce white space around chunks
)
@

\section{Introduction}
\label{sec:intro}

R package \pkg{epigrowthfit} implements methods for estimating
parameters associated with epidemic growth, including the initial
exponential growth rate $r$. \pkg{epigrowthfit} was initially
developed to support the analysis of Earn \etal~\cite{Earn+20},
based on the methodology of Ma \etal~\cite{Ma+14}, but now extends
their methods and provides additional useful machinery.

This document was built using
\Sexpr{sub(" \\(.*\\)$", "", R.Version()$version.string)}
and these R package versions:

<<package-versions, echo=FALSE>>=
package_list <- c(
  "epigrowthfit",
  "TMB",     # automatic differentiation of objective functions
  "emdbook", # tools for ecological modeling
  "knitr",   # integration of R code and LaTeX
  "shape"    # arrows in plots
)
print(installed.packages()[package_list, "Version"], quote = FALSE)
library("epigrowthfit")
@


\subsection{Package installation}


\subsubsection{Dependencies}

\pkg{epigrowthfit} depends on an installation of
\href{https://www.r-project.org/}{R} 3.5.0 or greater
and R package \cran{TMB}, and imports from R packages
\cran{mathjaxr}, \cran{Rdpack}, and \cran{emdbook}.
Building this vignette requires additional R packages
\cran{knitr} and \cran{shape}, as well as a
\href{https://www.latex-project.org/}{\LaTeX} distribution.

Windows users require an installation of
\href{https://cran.r-project.org/bin/windows/Rtools/}{Rtools}
compatible with their version of R. Rtools is needed to build
from source R packages like \pkg{epigrowthfit} that rely on
compiled C++ code. Once installed, Rtools must be added to
\code{PATH}. Users with Rtools 40 (compatible with R 4.0.0
or greater) can accomplish this by running

<<rtools-1, eval=FALSE>>=
write('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"',
  file = "~/.Renviron",
  append = TRUE,
  sep = "\n"
)
@
%
then restarting R. Users with Rtools 35 (compatible with R 3.3.0--3.6.3)
must set both \code{PATH} and \code{BINPREF} by running

<<rtools-2, eval=FALSE>>=
write('PATH="C:\\Rtools\\bin;${PATH}"',
  file = "~/.Renviron",
  append = TRUE,
  sep = "\n"
)
write('BINPREF="C:\\Rtools\\mingw_${WIN}\\bin"',
  file = "~/.Renviron",
  append = TRUE,
  sep = "\n"
)
@
%
then restarting R. The above assumes that Rtools 35 was installed to
\code{C:\\Rtools} (typically the default location).


\subsubsection{Installing from GitHub}

\pkg{epigrowthfit} can be installed from a
\href{https://github.com/davidearn/epigrowthfit/}{GitHub} repository
using function \fun{install_github} from the \cran{remotes} package.

<<installation-1, eval=FALSE>>=
if (!require("remotes")) {
  install.packages("remotes")
}
remotes::install_github("davidearn/epigrowthfit",
  ref = "devel",
  dependencies = TRUE,
  build_vignettes = TRUE
)
@
%
To disable automatic installation of dependencies,
set \code{dependencies = `1FALSE`1}. To avoid building this vignette,
which requires a \LaTeX\ distribution and can add a few minutes to the
installation time if many \LaTeX\ packages must also be installed, set
\code{dependencies = `1NA`1} and \code{build_vignettes = `1FALSE`1}.


\subsubsection{Installing from a local repository}

If you have cloned \pkg{epigrowthfit} from
\href{https://github.com/davidearn/epigrowthfit/}{GitHub},
then you can install it on the command line from your local
repository. Local installation requires R package \cran{devtools}:

<<installation-2, eval=FALSE>>=
if (!require("devtools")) {
  install.packages("devtools")
}
@
%
Enter the \code{devel} branch of the repository with
\code{git checkout devel}, then run \code{make} in the
root directory, namely \code{epigrowthfit/}. This will
install \pkg{epigrowthfit} and its dependencies and
build this vignette. You can verify that the installation
was successful by running \code{(`9require`9(epigrowthfit))}
in R.


\subsection{Package contents}

A list of data sets and functions exported by \pkg{epigrowthfit}
can be retrieved using \fun{data} and \fun{ls}.

<<namespace1>>=
library("epigrowthfit")

# Data sets
(dnames <- data(package = "epigrowthfit")$results[, "Item"])

# Functions
(fnames <- setdiff(ls("package:epigrowthfit"), dnames))
@
%
Functions \fun{egf_init} and \fun{egf} do much of the work in fitting
models of epidemic growth. The remaining functions either support this
work flow or enable estimation of parameters that are not explicitly
fitted, from fitted initial exponential growth rates $r$. These include
the epidemic doubling time, basic reproduction number, and epidemic
final size (see \cref{sec:extras}).

A list of implemented S3 methods can retrieved as follows.%
\footnote{
  These should not be called directly, as S3 methods are found
  automatically by the corresponding generic functions
  (the names before the dot). For example, if \code{`9plot`9(x)}
  is run and \code{x} is an \code{egf} object, then R evaluates
  \code{`9plot.egf`9(x)}.
}

<<namespace2>>=
(mnames <- grep("\\.", ls(getNamespace("epigrowthfit")), value = TRUE))
@

Usage of the \pkg{epigrowthfit} machinery is demonstrated
in \cref{sec:ontario}, where it is applied to a COVID-19
data set from Ontario, Canada.


\subsection{Package documentation}

Package documentation can be accessed as follows:

<<documentation, eval=FALSE>>=
## This vignette
vignette("epigrowthfit-vignette")

## Help pages
?"epigrowthfit-package"  # package
?data_set_name           # data set "data_set_name"
?function_name           # function "function_name"
?generic_name.class_name # S3 methods for class "class_name"
@


\section{Data requirements}
\label{sec:data}

All that is required to use \pkg{epigrowthfit} is an interval incidence
time series. That is, one must have times $t_0 < t_1 < \cdots < t_n$---%
which need not be equally spaced---and know the number $x_i$ of cases
observed between times $t_{i-1}$ and $t_i$ for $i = 1,\ldots,n$.
Interval incidence can be derived from \emph{cumulative} incidence by
differencing. That is, if one knows the number $c_i$ cases observed up
to time $t_i$ for $i = 0,\ldots,n$, then one derives interval incidence
as $x_i = c_i - c_{i-1}$. \cref{fig:data-types}
displays the relationship between the two types of incidence.

<<data-types, fig.width=4, fig.height=3, fig.cap=sprintf("A sketch of the relationship between cumulative and interval incidence. Cumulative incidence $c_i$ is observed at times $t_i$ for $i = 0,\\ldots,%d$. Interval incidence $x_i$ is computed as $x_i = c_i - c_{i-1}$ for $i = 1,\\ldots,%d$. When the times $t_i$ are roughly equally spaced, as in this sketch, interval incidence peaks around the inflection point in cumulative incidence (black square), where the curvature changes sign.", n, n), echo=FALSE>>=
r <- 1
c0 <- 1
K <- 10
logistic <- function(time) {
  K / (1 + (K / c0 - 1) * exp(-r * time))
}
n <- 5
time <- 0:n
cum_inc <- logistic(time)
int_inc <- diff(cum_inc)
curve_time <- seq(0, n, by = 0.2)
curve_cum_inc <- logistic(curve_time)
inflection_time <- -(1 / r) * (log(c0) - log(K - c0))
inflection_cum_inc <- logistic(inflection_time)

xax_labels <- parse(text = sprintf("t[%d]", time))
yax_labels <- parse(text = sprintf("c[%d]", time))
arr_labels <- parse(text = sprintf("x[%d]", time[-1]))

## Plot
par(
  mar = c(3, 3, 1, 1),
  mgp = c(3, 0.7, 0),
  las = 1,
  yaxs = "i",
  bty = "l"
)
plot.new()
plot.window(xlim = range(time), ylim = c(0, max(cum_inc) * 1.04))
segments(
  x0 = rep(par("usr")[1], 4),
  y0 = cum_inc,
  x1 = time,
  y1 = cum_inc,
  lty = 3, lwd = 2, col = "grey80"
)
lines(curve_time, curve_cum_inc, lwd = 2, col = "grey80")
points(time, cum_inc)
points(inflection_time, inflection_cum_inc, pch = 15)
shape::Arrows(
  x0 = time[-1],
  y0 = cum_inc[-(n+1)],
  x1 = time[-1],
  y1 = cum_inc[-1],
  arr.length = 0.16, arr.width = 0.08, arr.adj = 1,
  col = "grey30"
)
text(
  x = time[-1] + 0.2,
  y = cum_inc[-(n+1)] + 0.4 * int_inc,
  labels = arr_labels,
  xpd = NA
)
box()
axis(side = 1, at = time, labels = xax_labels)
axis(side = 2, at = cum_inc, labels = yax_labels)
title(xlab = "time", line = 2)
title(ylab = "cumulative incidence", line = 2)
@

Here, ``cases'' is used loosely and usually means one of three things:
(i)~infections, (ii)~reported infections, or (iii)~reported deaths
from disease. Under certain assumptions, the data type that one uses
makes no difference to the initial epidemic growth rate. To make this
precise, let $c(t)$ be the expected number of infections occurring up
to time $t$ (expected cumulative incidence), and let $\wt{c}(t)$ be
the expected number of infections (or disease deaths) \emph{reported}
up to to time $t$ (expected cumulative reported incidence). If, at
the start of an epidemic, cumulative incidence can be modeled as an
exponential function, so that
%
\begin{linenomath}
\begin{equation}
\label{eq:data-1}
c(t) \sim c_0 e^{rt}\,,
\end{equation}
\end{linenomath}
%
and if cumulative reported incidence is proportional to cumulative
incidence at an earlier time, so that
%
\begin{linenomath}
\begin{equation}
\label{eq:data-2}
\wt{c}(t) \propto c(t - t_\text{delay})\,,
\end{equation}
\end{linenomath}
%
then both $c(t)$ and $\wt{c}(t)$ grow exponentially with rate $r$.
In fact, the same is true for interval incidence $c(t) - c(t-\Delta t)$
and interval reported incidence $\wt{c}(t) - \wt{c}(t - \Delta t)$.
Hence, for the purpose of estimating $r$ for a given epidemic, it
is sufficient to study reported incidence and not necessary to know
incidence (assuming that \cref{eq:data-1,eq:data-2} are valid).
Going forward, ``cases'' and ``incidence'' are used as general terms
referring to infections, reported infections, or reported deaths from
disease.

In the study of historical epidemics, one often observes deaths
due to all causes including the disease of interest, rather than
disease deaths alone. \pkg{epigrowthfit} handles this additional
use case by assuming a model for how all causes mortality is
decomposed into disease mortality and mortality due to other
causes (see \cref{sec:models-baseline}).


\subsection{Handling common pathologies in incidence time series}


\subsubsection{Missing data}
\label{sec:data-missing}

\pkg{epigrowthfit} tolerates missing values in interval incidence
time series, except in the extreme case in which the number of
points to which an epidemic model is fit, excluding missing values,
is less than the number of model parameters (see \cref{sec:mle}).
Hence, in practice, users are not required to perform
\href{https://en.wikipedia.org/wiki/Imputation_(statistics)}{imputation}.
Nonetheless, it is a good idea to investigate likely causes for
missingness, as there may be straightforward ways to eliminate
missing values (see, \eg \cref{sec:data-weekends}).


\subsubsection{Under-reporting on weekends, over-reporting on Mondays}
\label{sec:data-weekends}

When incidence is reported daily, it not uncommon for weekend and
weekday counts to differ systematically. Due to reduced weekend
testing capacity, weekend counts are often systematically smaller
than weekday counts, and sometimes weekend counts are missing
altogether. The resulting backlog typically appears in the time
series as a spike on Mondays. In this situation, the time series
should be treated as unequally spaced: one observes Saturday,
Sunday, and Monday counts in aggregate (\ie a sum over 3 days),
but not the distribution of the aggregate over those three days.

For example, consider the week from Thursday, November 5, 2020, to
Wednesday, November 11, 2020, and suppose that on day $i$ the number
of new cases since day $i-1$ is reported. For simplicity, assume that
under constant testing capacity, 50 cases would be reported every
day, but due to weekend lags, 25 Saturday cases and 25 Sunday cases
are reported on Monday, so that Monday's count is 100 instead of 50.

<<weekends-1>>=
dc <- data.frame(
  date = as.Date(0:6, origin = "2020-11-05"),
  cases = c(50, 50, 25, 25, 100, 50, 50)
)
row.names(dc) <- weekdays(dc$date, abbreviate = TRUE)
dc
@
%
To correctly represent the fact that we do not truly observe Saturday,
Sunday, and Monday counts individually, but only in aggregate, we add
the weekend counts to the Monday count, then delete the weekend rows.

<<weekends-2>>=
dc$cases[5] <- sum(dc$cases[3:5])
dc <- dc[-(3:4), ]
dc
@
%
The resulting time series is no longer equally spaced---the Monday count
is an aggregate over 3 days, while other days' counts are aggregates
over 1 day---but this is no issue for \pkg{epigrowthfit}. As explained
at the start of this section (\cref{sec:data}), \pkg{epigrowthfit} asks
users for interval endpoints and counts within each interval, but does
not require the intervals to have the same length.


\section{Models of epidemic growth}
\label{sec:models}

The models of epidemic growth implemented in \pkg{epigrowthfit}
are phenomenological. They formulate (i) what an incidence curve
is expected to look like and (ii) how \emph{observed} incidence
varies randomly from this expectation. Below is a brief outline
of these models.


\subsection{Models of expected cumulative incidence}
\label{sec:models-expected}

Let $c(t)$ be the expected number of cases observed between the
start of an epidemic wave%
\footnote{
  The start of an epidemic wave is defined as the time at
  which interval incidence begins to grow roughly exponentially.
}
and $t$ time later, and let $c(0) = c_0 > 0$. $c(t)$ will be called
expected cumulative incidence, with the understanding that it does
not necessarily count cases since the very start of an epidemic.

\paragraph{Exponential model.} If $c(t)$ follows
%
\begin{linenomath}
\begin{equation}
\label{eq:exponential-de}
c'(t) = r c(t)\,,\qquad r > 0\,,
\end{equation}
\end{linenomath}
%
then $c(t)$ grows exponentially as
%
\begin{linenomath}
\begin{equation}
\label{eq:exponential}
c(t) = c_0 e^{r t}\,.
\end{equation}
\end{linenomath}
%
Two parameters must be fit to observed data: the exponential
growth rate $r$ and initial cumulative incidence $c_0$.

\paragraph{Logistic model.} If $c(t)$ follows
%
\begin{linenomath}
\begin{equation}
\label{eq:logistic-de}
c'(t) = r c(t)\bigg(1 - \frac{c(t)}{K}\bigg)\,,\qquad r, K > 0\,,
\end{equation}
\end{linenomath}
%
and if $c_0 \in (0,K)$, then $c(t)$ grows logistically as
%
\begin{linenomath}
\begin{equation}
c(t) = \frac{K}{1 + \big(\frac{K}{c_0} - 1\big) e^{-r t}}
\end{equation}
\end{linenomath}
%
and increases to $K$ as $t \to \infty$. The logistic model can
be reparametrized as
%
\begin{linenomath}
\begin{equation}
\label{eq:logistic}
c(t) = \frac{K}{1 + e^{-r (t - \thalf)}}\,,
\end{equation}
\end{linenomath}
%
where $\thalf$ is the time at which cumulative incidence attains
half its final size, satisfying $c(\thalf) = \frac{K}{2}$. This
formulation is desirable because $\thalf$ has a natural interpretation
and, unlike $c_0$, is unconstrained: $c(t) \in (0,K)$ for all $t$,
for all $\thalf \in (-\infty,\infty)$. The reparametrized logistic
model requires that $r$, $\thalf$, and $K$ are fit to observed data.

In the logistic model, $r$ represents the \emph{initial} exponential
growth rate, as \cref{eq:logistic-de} gives $c'(t) \sim r c(t)$ for
$c(t) / K \ll 1$. That is, at the start of an epidemic, when $c(t)$
is very small compared to $K$, $c(t)$ grows roughly exponentially
with rate $r$.

\paragraph{Richards model~\cite{Rich59}.} If $c(t)$ follows
%
\begin{linenomath}
\begin{equation}
\label{eq:richards-de}
c'(t) = r c(t)\bigg(1 - \bigg(\frac{c(t)}{K}\bigg)^p\bigg)\,,\qquad r, K, p > 0\,,
\end{equation}
\end{linenomath}
%
and if $c_0 \in (0,K)$, then $c(t)$ grows as
%
\begin{linenomath}
\begin{equation}
c(t) = \frac{K}{\big[1 + \big(\big(\frac{K}{c_0}\big)^p - 1\big) e^{-r p t}\big]^{1/p}}
\end{equation}
\end{linenomath}
%
and increases to $K$ as $t \to \infty$. Here, $p$ is a shape parameter
determining how quickly $c(t)$ saturates, and setting $p = 1$ recovers
the logistic model. The Richards model can be reparametrized as
%
\begin{linenomath}
\begin{equation}
\label{eq:richards}
c(t) = \frac{K}{\big[1 + (2^p - 1) e^{-r p (t - \thalf)}\big]^{1/p}}
\end{equation}
\end{linenomath}
%
where $\thalf$ satisfies $c(\thalf) = \frac{K}{2}$, as in
\cref{eq:logistic}. The reparametrized Richards model requires
that $r$, $\thalf$, $K$, and $p$ are fit to observed data.
Here, as in \cref{eq:logistic}, $r$ represents the initial
exponential growth rate.

\cref{fig:models-expected} compares cumulative and interval
incidence curves generated by the exponential, logistic, and
Richards models.

<<models-expected, fig.height=2.5, fig.cap=sprintf("\\textbf{[Left]} Cumulative incidence, $c(t)$. Displayed are exponential and Richards ($p = %s$) curves with $r = %g$, $c_0 = %g$, and $K = %g$. The Richards curve with $p = 1$ is a logistic curve. A dashed line is drawn at $c = K / 2 = %g$. This line intersects the Richards curves at time $\\thalf$ and shows that $\\thalf$ is a decreasing function of $p$. Units of time are characteristic ($1 / r$). \\textbf{[Right]} Interval incidence, $c(t) - c(t - 1)$.", paste(p, collapse = ", "), r, c0, K, K / 2), echo=FALSE>>=
r <- 1
c0 <- 1
K <- 1000
p <- c(1.3, 1, 0.7)
time <- seq(0, 16, by = 1)
richards <- function(p) {
  K / (1 + ((K / c0)^p - 1) * exp(-r * p * time))^(1 / p)
}
cum_inc <- c(list(c0 * exp(r * time)), lapply(p, richards))
int_inc <- lapply(cum_inc, diff)

ltys <- c(1, 2, 1, 3)
cols <- c("grey80", rep("grey30", 3))
labs <- c("exponential", sprintf("Richards (p = %g)", p))

## Plot
par(
  mfrow = c(1, 2),
  mar = c(3, 4, 1, 3),
  oma = c(0, 0, 0, 5),
  mgp = c(3, 0.7, 0),
  las = 1,
  xaxs = "i",
  yaxs = "i",
  bty = "l"
)

## Panel 1
plot.new()
plot.window(
  xlim = range(time),
  ylim = c(0, max(unlist(cum_inc[-1])) * 1.04)
)
abline(h = K / 2, lty = 2, col = "grey30")
for (i in 1:4) {
  lines(time, cum_inc[[i]], lty = ltys[i], lwd = 3, col = cols[i])
}
box()
axis(side = 1)
axis(side = 2)
title(xlab = "time", line = 2)
title(ylab = "cumulative incidence")

## Panel 2
plot.new()
plot.window(
  xlim = range(time),
  ylim = c(0, max(unlist(int_inc[-1])) * 1.1)
)
for (i in 1:4) {
  lines(time[-1], int_inc[[i]], lty = ltys[i], lwd = 3, col = cols[i])
}
box()
axis(side = 1)
axis(side = 2)
title(xlab = "time", line = 2)
title(ylab = "interval incidence")
legend(
  x = par("usr")[2] * 1.05,
  y = mean(par("usr")[3:4]),
  legend = labs,
  lty = ltys, lwd = 3, col = cols,
  yjust = 0.5, xpd = NA, bty = "n", seg.len = 3, cex = 0.7
)
@


\subsection{Models of observed interval incidence}
\label{sec:models-observed}

Let $\mu_i = c(t_i) - c(t_{i-1})$ be the expected number of cases
observed between times $t_{i-1}$ and $t_i > t_{i-1}$ (expected interval
incidence), and let $X_i$ be the number that is actually observed.

\paragraph{Poisson model.} If $X_i$ is modeled as a
\href{https://en.wikipedia.org/wiki/Poisson_distribution}{Poisson-distributed}
random variable with mean $\mu_i$,
%
\begin{linenomath}
\begin{equation}
\label{eq:pois}
X_i \sim \mathrm{Poisson}(\mu_i)\,,
\end{equation}
\end{linenomath}
%
then the probability mass function of $X_i$ is given by
%
\begin{linenomath}
\begin{equation}
\label{eq:pois-pmf-simple}
f_i(j;\mu_i)
= \mathcal{P}(X_i = j;\mu_i)
= \frac{\mu_i^j e^{-\mu_i}}{j!}\,,\qquad j = 0,1,\ldots\,.
\end{equation}
\end{linenomath}
%
The mean $\mu_i$ is itself conditional on the model chosen
for $c(t)$ (see \cref{sec:models-expected}). To make this
explicit, \cref{eq:pois-pmf-simple} can be rewritten as
%
\begin{linenomath}
\begin{equation}
\label{eq:pois-pmf}
f_i(j;\vec{\xi})
= \frac{\mu_i(\vec{\xi})^j e^{-\mu_i(\vec{\xi})}}{j!}\,\qquad j = 0,1,\ldots\,,
\end{equation}
\end{linenomath}
%
where $\vec{\xi}$ is a parameter vector specifying $c(t)$:
$(r,c_0)$ for the exponential model, $(r,\thalf,K)$ for the
logistic model, and $(r,\thalf,K,p)$ for the Richards model
(see \cref{eq:exponential,eq:logistic,eq:richards}).

\paragraph{Negative binomial model} If $X_i$ is modeled as a
\href{https://en.wikipedia.org/wiki/Negative_binomial_distribution}{negative binomial-distributed}
random variable with mean $\mu_i$ and dispersion $k > 0$,
%
\begin{linenomath}
\begin{equation}
\label{eq:nbinom}
X_i \sim \mathrm{NegativeBinomial}(\mu_i,k)\,,
\end{equation}
\end{linenomath}
%
then the probability mass function of $X_i$ is given by
%
\begin{linenomath}
\begin{equation}
f_i(j;\mu_i,k)
= \mathcal{P}(X_i = j;\mu_i,k)
= \frac{\Gamma(k + j)}{j! \Gamma(k)} \bigg(\frac{k}{k + \mu_i}\bigg)^k \bigg(\frac{\mu_i}{k + \mu_i}\bigg)^j\,\qquad j = 0,1,\ldots\,,
\end{equation}
\end{linenomath}
%
where $\Gamma$ is the
\href{https://en.wikipedia.org/wiki/Gamma_function}{gamma function},
or more explicitly as
%
\begin{linenomath}
\begin{equation}
\label{eq:nbinom-pmf}
f_i(j;\vec{\xi},k)
= \frac{\Gamma(k + j)}{j! \Gamma(k)} \bigg(\frac{k}{k + \mu_i(\vec{\xi})}\bigg)^k \bigg(\frac{\mu_i(\vec{\xi})}{k + \mu_i(\vec{\xi})}\bigg)^j\,\qquad j = 0,1,\ldots\,.
\end{equation}
\end{linenomath}

\cref{fig:models-observed} compares negative binomial and Poisson
distributions with a common mean of 20.

<<models-observed, fig.height=2.5, fig.cap=sprintf("Probability mass functions of three negative binomial random variables ($k = %s$) and a Poisson random variable with a common mean of %g.", paste(k, collapse = ", "), mu), echo=FALSE>>=
val <- 0:60
mu <- 20
k <- 10^{c(0:2)}
pmf <- c(lapply(k, function(x) dnbinom(val, mu = mu, size = x)),
         list(dpois(val, lambda = mu)))

pchs <- c(2, 4, 16, 1)
cols <- c("#DDDDDD", "#44BB99", "#BBCC33", "#EE8866")
labs <- c(sprintf("NegativeBinomial (%g, %g)", mu, k),
          sprintf("Poisson(%g)", mu))

## Plot
par(
  mar = c(3, 4, 1, 1),
  mgp = c(3, 0.7, 0),
  las = 1,
  xaxs = "i",
  yaxs = "i",
  bty = "l"
)
plot.new()
plot.window(xlim = range(val), ylim = c(0, 0.1))
for (i in 1:4) {
  points(val, pmf[[i]], pch = pchs[i], col = cols[i])
}
box()
axis(side = 1)
axis(side = 2)
title(xlab = "value", line = 2)
title(ylab = "probability")
legend(
  x = "topright",
  legend = labs,
  pch = pchs, col = cols,
  bty = "n", cex = 0.7
)
@


\subsection{Baseline growth}
\label{sec:models-baseline}

For many historical epidemics, available data count deaths due to
all causes including the disease of interest, rather than disease
deaths alone. In this setting, growth in disease mortality over
time can be understood from all causes mortality provided that
disease deaths and deaths from other causes (baseline mortality)
are modeled separately. To account for baseline mortality, one can
assume that deaths due to causes other than the disease of interest
occur at a constant rate $b > 0$. Then, for example, the logistic
model (\cref{eq:logistic}) becomes
%
\begin{linenomath}
\begin{equation}
\label{eq:logistic-baseline}
c(t) = b t + \frac{K}{1 + e^{-r (t - \thalf)}}\,,
\end{equation}
\end{linenomath}
%
where $c(t)$ is to be interpreted as cumulative all causes mortality
rather than cumulative disease mortality.

\cref{eq:logistic-baseline} can be applied more generally to model
cumulative multiple (not necessarily all) causes mortality. This
type of data arises in the modeling of influenza, as people who
recover from influenza are prone to later infection with pneumonia,
and, clinically, pneumonia can be difficult to distinguish from
influenza. As such, many influenza deaths are attributed to pneumonia
rather than influenza, and influenza spread must be inferred from
aggregates of the pneumonia and influenza deaths.

Accounting for baseline mortality as in \cref{eq:logistic-baseline}
requires that $b$ is fit in addition to other model parameters.%
\footnote{
  \cref{eq:logistic-baseline} models baseline mortality as linear
  in time. This formulation is valid for short time scales (\eg an
  epidemic occurring over less than one year), over which the
  mortality rate is roughly constant. On longer time scales, more
  sophisticated models of baseline mortality with more than one
  parameter can be considered. For an early example,
  see Serfling~\cite{Serf63}.
}
\cref{fig:models-baseline} displays the effect of baseline growth
on the logistic curve.

<<models-baseline, fig.height=2.5, fig.cap=sprintf("\\textbf{[Left]} Cumulative all causes mortality, $c(t)$. Displayed are logistic curves with $r = %g$, $c_0 = %g$, and $K = %g$ and a linear baseline ($b = %s$); see \\cref{eq:logistic-baseline}). Units of time are characteristic ($1 / r$). \\textbf{[Right]} Interval all causes mortality, $c(t) - c(t - 1)$.", r, c0, K, paste(b, collapse = ", ")), echo=FALSE>>=
r <- 1
c0 <- 1
K <- 1000
b <- c(50, 25, 0)
time <- seq(0, 16, by = 1)
logistic <- function(b) {
  b * time + K / (1 + (K / c0 - 1) * exp(-r * time))
}
cum_inc <- lapply(b, logistic)
int_inc <- lapply(cum_inc, diff)

cols <- c("grey30", "grey30", "grey80")
ltys <- c(3, 2, 1)
labs <- paste0("b = ", b)

## Plot
par(
  mfrow = c(1, 2),
  mar = c(3, 4, 1, 3),
  oma = c(0, 0, 0, 5),
  mgp = c(3, 0.7, 0),
  las = 1,
  xaxs = "i",
  yaxs = "i",
  bty = "l"
)

## Panel 1
plot.new()
plot.window(xlim = range(time), ylim = c(0, max(unlist(cum_inc)) * 1.1))
for (i in 1:3) {
  lines(time, cum_inc[[i]], lty = ltys[i], lwd = 3, col = cols[i])
}
box()
axis(side = 1)
axis(side = 2)
title(xlab = "time", line = 2)
title(ylab = "cumulative AC mortality")

## Panel 2
plot.new()
plot.window(xlim = range(time), ylim = c(0, max(unlist(int_inc)) * 1.1))
for (i in 1:3) {
  lines(time[-1], int_inc[[i]], lty = ltys[i], lwd = 3, col = cols[i])
}
box()
axis(side = 1)
axis(side = 2)
title(xlab = "time", line = 2)
title(ylab = "interval AC mortality")
legend(
  x = par("usr")[2] * 1.05,
  y = mean(par("usr")[3:4]),
  legend = labs,
  lty = ltys, lwd = 3, col = cols,
  yjust = 0.5, xpd = NA, bty = "n", seg.len = 3, cex = 0.7
)
@


\section{Maximum likelihood estimation}
\label{sec:mle}

For $i = 1,\ldots,n$, let $x_i$ be the number of cases observed
between times $t_{i-1}$ and $t_i$ (observed interval incidence).
The primary task of \pkg{epigrowthfit} is to fit one of the epidemic
models outlined in \cref{sec:models} to these data. More precisely,
\pkg{epigrowthfit} assumes that the data $\vec{x}$ were generated
by a vector $\vec{\theta}$ of model parameters and aims to
estimate $\vec{\theta}$ from $\vec{x}$.

The likelihood of an estimate $\vec{\wh{\theta}}$ given data
$\vec{x}$ is the joint probability mass of $\vec{x}$ under the
model specified by $\vec{\wh{\theta}}$, \ie
%
\begin{linenomath}
\begin{equation}
\label{eq:likelihood}
\mathcal{L}(\vec{\wh{\theta}}|\vec{x}) = \prod_{i=1}^{n} f_i(x_i;\vec{\wh{\theta}})\,,
\end{equation}
\end{linenomath}
%
where $f_i$ is the probability mass function of $X_i$
(see \cref{sec:models-observed}) and it is assumed that
the set of random variables $\{X_i\}$ is mutually
independent. If $x_i$ is missing for some (but not all)
$i$, then the product in \cref{eq:likelihood} is taken
over just the indices $i$ for which $x_i$ is known,
hence the likelihood function remains well-defined when
there are missing data.

The negative log likelihood is
%
\begin{linenomath}
\begin{equation}
-\ell(\vec{\wh{\theta}}|\vec{x})
= -\log \mathcal{L}(\vec{\wh{\theta}}|\vec{x})
= -\sum_{i=1}^{n} \log f_i(x_i;\vec{\wh{\theta}})\,.
\end{equation}
\end{linenomath}
%
Under certain regularity conditions, $-\ell$ attains a global minimum
at a unique point $\vec{\wh{\theta}}_\text{mle}$ in its domain, called
the
\href{https://en.wikipedia.org/wiki/Maximum_likelihood_estimation}{maximum likelihood estimate}
of $\vec{\theta}$. As the problem of finding
$\vec{\wh{\theta}}_\text{mle}$ is not tractable analytically except
for simple models, \pkg{epigrowthfit} employs a numerical algorithm
to search for a good approximation. Thus, the task of fitting a
model of epidemic growth to observed incidence data is reduced to a
numerical optimization problem.

\pkg{epigrowthfit} fits all model parameters on an unconstrained
logarithmic scale. That is, the negative log likelihood is recast
as a function of the log-transformed parameter vector
$\vec{\wh{\theta}}^* = \log \vec{\wh{\theta}}$:
%
\begin{linenomath}
\begin{equation}
-\ell^*(\vec{\wh{\theta}}^*|\vec{x})
= -\ell(e^{\vec{\wh{\theta}}^*}|\vec{x})
= -\ell(e^{\log \vec{\wh{\theta}}}|\vec{x})
= -\ell(\vec{\wh{\theta}}|\vec{x})\,,
\end{equation}
\end{linenomath}
%
where it is understood that the exponential and logarithmic
functions have been applied elementwise.
$\vec{\wh{\theta}}_\text{mle}$ is then recovered as
$e^{\vec{\wh{\theta}}_\text{mle}^*}$, where
$\vec{\wh{\theta}}_\text{mle}^*$ is the minimizer of $-\ell^*$.

\pkg{epigrowthfit} writes $-\ell^*$ in a C++ template and uses
R package \cran{TMB} to carry out
\href{https://en.wikipedia.org/wiki/Automatic_differentiation}{automatic differentiation}
of $-\ell^*$ with respect to $\vec{\wh{\theta}}^*$. It employs
gradient-based optimizer \fun{nlminb} (alternatively \fun{nlm}
or one of the optimizers provided through \fun{optim}) from
base R package \pkg{stats} to search for
$\vec{\wh{\theta}}_\text{mle}^*$.

The optimizers are iterative and susceptible to numerical problems
preventing convergence near to $\vec{\wh{\theta}}_\text{mle}^*$.
Poor fits to the data by \fun{nlminb} are sometimes corrected by
trying a different optimizer, such as the Nelder-Mead algorithm
available through \fun{optim}. Nelder-Mead is considered robust,
but as it makes no use of gradients, it is relatively slow.
Speed may not be a concern in practice, as models implemented
in \pkg{epigrowthfit} have at most six parameters.


\section{Choosing a model}
\label{sec:choosing}

\paragraph{Avoid the exponential model.}
The exponential model (\cref{eq:exponential}) implies unbounded
exponential growth of expected cumulative incidence $c(t)$. It
therefore fails to capture saturation of $c(t)$ resulting from
depletion of susceptible individuals. The exponential model can
agree with epidemic data only during the (typically short) initial
exponential growth phase. Indeed, Ma \etal~\cite{Ma+14} showed
that estimates of the initial exponential growth rate $r$ obtained
under the exponential model and the coverage of the associated
95\% confidence intervals, are highly sensitive to the choice of
fitting window. More robust fits to epidemic data are obtained
with the logistic and Richards models (\cref{eq:logistic,eq:richards}),
which allow $c(t)$ to saturate asymptotically.

\paragraph{Start with negative binomial, and switch to Poisson if necessary.}
The negative binomial distribution with mean $\mu$ and dispersion $k$
is well approximated by the Poisson distribution with mean $\mu$ if
$\mu / k \ll 1$~\cite[p.\ 124]{Bolk08}. (Negative binomial variance
exceeds Poisson variance by $\mu^2 / k$. Relative to the mean $\mu$,
the excess variance is $\mu / k$.) Indeed,
%
\begin{linenomath}
\begin{equation}
\mathrm{NegativeBinomial}\big(\mu,k\big) \quad\xrightarrow{k \to \infty}\quad \mathrm{Poisson}(\mu)\,,
\end{equation}
\end{linenomath}
%
as shown graphically in \cref{fig:models-observed} for $\mu = 20$.
This implies that the likelihood (\cref{eq:likelihood}) of the negative
binomial model of observed interval incidence (\cref{eq:nbinom}) is
well approximated by that of the Poisson model (\cref{eq:pois}) when
$k \gg \max_i \{\mu_i\}$. In this sense, the Poisson model is nested
in the negative binomial model.

Without prior knowledge of the data-generating distribution, it
may seem natural to go with the more general negative binomial
model. It turns out that a more careful approach is needed:
\cran{TMB}'s C++ library provides two functions, \code{dnbinom2}
and \code{dnbinom2_robust}, for evaluating the negative binomial
probability mass function, but both are unstable for $k \gg \mu$
(\code{dnbinom2_robust} to a lesser extent). A consequence is
that the computed likelihood of a negative binomial model with
$k \gg \max_i \{\mu_i\}$ is prone to inaccuracy.%
\footnote{
  In an experiment, the computed likelihood of a
  $\mathrm{NegativeBinomial}(1,k)$ model given
  100 $\mathrm{Poisson}(1)$ deviates diverged
  from the computed $\mathrm{Poisson}(1)$ likelihood
  starting near $k = 10^6$.
}
For this reason, one should not necessarily rely on negative binomial
fits with large $k$. Instead, one should set a threshold value of $k$
and refit using a Poisson model if the threshold is exceeded.
By default, \pkg{epigrowthfit} warns users when it returns a
negative binomial fit with $k$ exceeding $100 \max_i \{\mu_i\}$.



\section{Epidemic parameters derived from \tops{$r$}{r}}
\label{sec:extras}


\subsection{Characteristic and doubling times}
\label{sec:doubling-time}

The characteristic time of an epidemic is the time required for
cumulative incidence to increase by a factor of $e$ at the start
of an epidemic, when cumulative incidence grows roughly exponentially
with rate $r$, \ie when $c(t) \sim c_0 e^{r t}$. The characteristic
time corresponding to an exponential growth rate $r$ is simply $1 / r$.

In practice, it is much easier to talk about the doubling time,
the time required for cumulative incidence to increase by a factor
of 2, which is computed as $(\log 2) / r$. \pkg{epigrowthfit}
function \fun{compute_doubling_time} can be used to compute
doubling times for given $r$. See \cref{sec:ontario-doubling-time}
for an example.


\subsection{Basic reproduction number}
\label{sec:R0}

The basic reproduction number of an infectious disease, commonly denoted
by $\R_0$, is the number of people that an infected person is expected
to infect in an otherwise completely susceptible population. Wallinga
and Lipsitch~\cite[Eq 2.7]{WallLips07} showed that $\R_0$ is determined
by the initial exponential growth rate $r$ and the distribution of the
disease generation interval $\tau_\text{gen}$, the time from infection
of a primary case to infection of a secondary case in a chain of
transmission:
%
\begin{linenomath}
\begin{equation}
\label{eq:R0-general}
\frac{1}{\R_0} = \int_{0}^{\infty} e^{-r s} f_\text{gen}(s)\,\text{d}s\,,
\end{equation}
\end{linenomath}
%
where $f_\text{gen}$ is the density function of $\tau_\text{gen}$.

In the special case where
%
\begin{enumerate}
\item $f_\text{gen}$ is supported on the interval $(\tau_0,\tau_m]$, and
\item there are times $\tau_0 < \tau_1 < \cdots < \tau_m$
  such that $f_\text{gen}$ assumes a constant value
  $g_i = p_i / (\tau_i - \tau_{i-1})$
  on the interval $(\tau_{i-1},\tau_i]$ for $i = 1,\ldots,m$,
\end{enumerate}
%
\cref{eq:R0-general} yields
%
\begin{linenomath}
\begin{equation}
\label{eq:R0}
\R_0 = \left. r \middle/ \bigg\{ \sum_{i=1}^{m} \frac{p_i (e^{-r \tau_{i-1}} - e^{-r \tau_i})}{\tau_i - \tau_{i-1}} \bigg\} \right.
\end{equation}
\end{linenomath}
%
This special case arises when $f_\text{gen}$ is an empirical
distribution, and $p_i$ is the observed relative frequency of
$\tau_\text{gen} \in (\tau_{i-1},\tau_i]$~\cite[Eq 3.6]{WallLips07}.
\pkg{epigrowthfit} function \fun{compute_R0} evaluates the
right-hand side of \cref{eq:R0} for given $r$, $\tau_i$, and $p_i$,
allowing users who have estimated $r$ using the package's fitting
machinery to also estimate $\R_0$. See \cref{sec:ontario-R0} for
an example.

The special case described above also arises when one assumes
that the latent%
\footnote{
  The latent period is the time from infection to onset of
  infectiousness.
}
and infectious%
\footnote{
  The infectious period is the time from onset of infectiousness
  to recovery or death from disease.
}
periods $\tau_\text{lat}$ and $\tau_\text{inf}$ are discrete.
To be precise, let $f_\text{lat}$ and $f_\text{inf}$ be the
density functions of $\tau_\text{lat}$ and $\tau_\text{inf}$,
and let $F_\text{inf} = \int f_\text{inf}$ be the distribution
function corresponding to $f_\text{inf}$.
Assuming that infectiousness is constant over the infectious
period, Svensson~\cite[\S 5.1]{Sven07} showed that
%
\begin{linenomath}
\begin{equation}
f_\text{gen} = f_\text{lat} * f_\text{wait}\,,\qquad f_\text{wait} = \scalebox{1.1}{$\mathbbm{1}$}_{(0,\infty)} \frac{1 - F_\text{inf}}{\mathbbm{E}[\tau_\text{inf}]}\,.
\end{equation}
\end{linenomath}
%
Here, $*$ denotes
\href{https://en.wikipedia.org/wiki/Convolution}{convolution}
and $f_\text{wait}$ is the density function of the infectious
\emph{waiting time} $\tau_\text{wait}$, the time from onset
of infectiousness to transmission. One can show that if
$f_\text{lat}$ and $f_\text{inf}$ have discrete support,
then $f_\text{gen}$ satisfies the conditions under which
\cref{eq:R0} is valid. \pkg{epigrowthfit} functions \fun{dgi}
and \fun{pgi} evaluate the density and distribution functions
of $\tau_\text{gen}$ implied by distributions of
$\tau_\text{lat}$ and $\tau_\text{inf}$ supported
on the positive integers (\ie $1,2,\ldots$ days). These are
helpful if one wants to use \fun{compute_R0} and has latent
and infectious period distributions, but not the corresponding
generation interval distribution.


\subsection{Epidemic final size}
\label{sec:final-size}

Once one has computed $\R_0$ as a function of $r$ (see \cref{sec:R0}),
one can compute the expected epidemic final size $Z$ as a function of
$\R_0$. $Z$ is the proportion of the population expected to be infected
over the course of the epidemic and is obtained from $\R_0$ as
%
\begin{linenomath}
\begin{equation}
\label{eq:final-size-general}
Z = S_0 + \frac{1}{\R_0} W\big(-\R_0 S_0 e^{-\R_0 (S_0 + I_0)} \big)\,,
\end{equation}
\end{linenomath}
%
where $W$ denotes the non-elementary
\href{https://en.wikipedia.org/wiki/Lambert_W_function}{Lambert $W$ function}
and $S_0,I_0 \in [0,1]$ are the proportion of the population
that is susceptible and infected, respectively, at the start
of the epidemic. If the epidemic is seeded in a large population
by one infected individual, then $(S_0,I_0) \approx (1,0)$.
In the limit as $(S_0,I_0) \to (1,0)$, \cref{eq:final-size-general}
simplifies to
%
\begin{linenomath}
\begin{equation}
\label{eq:final-size}
Z = 1 + \frac{1}{\R_0} W\big(-\R_0 e^{-\R_0} \big)\,.
\end{equation}
\end{linenomath}
%
Ma and Earn~\cite{MaEarn06} discuss the history and generality
of \cref{eq:final-size-general} and show that it is valid for
a large class of epidemic models, including the SIR model with
multiple infectious stages and arbitrarily distributed infectious
stage durations. \pkg{epigrowthfit} function \fun{compute_final_size}
evaluates the right-hand side of \cref{eq:final-size-general}
for given $\R_0$, $S_0$ and $I_0$. See \cref{sec:ontario-final-size}
for an example.


\section{Example: COVID-19 in Ontario, Canada}
\label{sec:ontario}


\subsection{Loading an epidemic time series}
\label{sec:ontario-data}

\pkg{epigrowthfit} comes with a number of infectious disease
data sets. One of them is \code{canadacovid}, a data frame
listing (mostly) daily confirmations of COVID-19 in Canadian
provinces and territories up to November 8, 2020. The start
date ranges from February 4, 2020 (Ontario), to April 4, 2020
(Nunavut). You can load \code{canadacovid} into your global
environment using \fun{data}.

<<ontario-data-1>>=
data(canadacovid)
@
%
You can access documentation for the data set by running
\code{?canadacovid} and examine the data frame directly
as follows.

<<ontario-data-2>>=
dim(canadacovid) # dimensions
head(canadacovid) # first 6 rows
sapply(canadacovid, class) # variable classes
levels(canadacovid$province) # province and territory labels
@
%
Let's consider just the data from Ontario.

<<ontario-data-3>>=
ontario <- subset(canadacovid, province == "ON")
dim(ontario)
head(ontario)
@
%
The variables in the \code{ontario} data frame most relevant
to \pkg{epigrowthfit} are \code{date} and \code{new_confirmed}.
For \code{i > `11`1}, \code{new_confirmed[i]} lists the number
of COVID-19 cases confirmed between \code{date[i-`11`1]} and
\code{date[i]}, hence \code{date} and \code{new_confirmed}
together specify an interval incidence time series.

In the Ontario time series, the observation interval is fixed
and equal to 1 day, and there are no missing values.

<<ontario-data-4>>=
unique(diff(ontario$date))
sum(is.na(ontario$new_confirmed[-1]))
@
%
\pkg{epigrowthfit} tolerates unequal spacing and missing values,
but it is good practice nonetheless to check data for irregularities
before fitting a model.


\subsection{Graphically inspecting the time series with \tops{\protect\fun{smooth_cases}}{smooth\_cases()}}
\label{sec:ontario-smooth}

Our aim is to use \pkg{epigrowthfit} to fit one of the models outlined
in \cref{sec:models} to each epidemic wave in the Ontario time series.
This will require us to know how many waves there are and to decide
what segments of the time series should be considered when fitting.

In a given wave, of specific interest are the times when interval
incidence starts and stops growing exponentially (linearly on a
logarithmic scale) and the time of peak incidence. Locating these
times requires some care, especially in noisy or unequally spaced
time series, for which notions of growth rate and peak are not
as clearly defined. \pkg{epigrowthfit} function \fun{smooth_cases}
attempts to simplify this task by fitting a smooth cubic
\href{https://en.wikipedia.org/wiki/Spline_(mathematics)}{spline}
to the time series and locating peaks and troughs in the fitted
curve as the times when its first derivative changes sign.

Below, we pass to \fun{smooth_cases} an increasing Date vector
\code{date} and a numeric vector \code{cases} of the same length,
together specifying an interval incidence time series---specifically,
the daily incidence time series in the \code{ontario} data frame.%
\footnote{
  For \code{i > `11`1}, \code{cases[i]} must specify the number
  of cases observed between \code{date[i-`11`1]} and \code{date[i]}.
  \code{cases[`11`1]} is ignored. The time series need not be
  equally spaced, and missing values in \code{cases} are tolerated.
}
We also pass a logical scalar \code{log} and a numeric scalar
\code{spar} telling \fun{smooth_cases} whether to fit
$\code{`9log10`9(`11`1+cases)}$ instead of \code{cases}
and what value in the interval $(0,1]$ to use for the spline
smoothing parameter.%
\footnote{
  Details about \code{spar} can be found under \code{?spline.smooth}.
}

<<ontario-smooth-1>>=
sc <- smooth_cases(
  date = ontario$date,
  cases = ontario$new_confirmed,
  log = TRUE,
  spar = 0.7
)
@

Plotting the resulting \code{smooth_cases} object generates
\cref{fig:ontario-smooth-2}, which shows that \code{spar = `10.7`1}
produces a reasonable fit to the $\log(1+x)$-transformed Ontario data.
In practice, obtaining a such a fit can require some coarse tuning of
\code{spar}. To update a \code{smooth_cases} object with a new value
for \code{spar},
do \code{object <- `9update`9(object, spar = new_value)}.

<<ontario-smooth-2, fig.height=3.5, fig.cap="Result of \\code{`9plot`9(sc)}. A cubic spline fit to interval incidence in the \\code{ontario} data frame. Times of peaks and troughs in the fitted curve are listed in the top margin as integer indices of \\fun{smooth_cases} argument \\code{date = ontario$date}.">>=
plot(sc)
@
%
We learn from \cref{fig:ontario-smooth-2} that the Ontario
time series spans two epidemic waves, the first peaking at
\code{ontario$date[`176`1]} and the second not peaking before
the end of the time series.

Below, we use \pkg{epigrowthfit} function \fun{dline} to mark
roughly when daily incidence starts and stops growing exponentially
(linearly on a logarithmic scale) in each of the two waves.
The resulting plot is shown in \cref{fig:ontario-smooth-3}.
\fun{dline} conveniently rounds specified dates to the nearest
date in \code{date = ontario$date}, converts the result to
numeric user coordinates, then adds vertical lines to the current
\code{smooth_cases} plot at those coordinates.

<<ontario-smooth-3, fig.height=3.5, fig.cap="Duplicate of \\cref{fig:ontario-smooth-2}, but with exponential growth windows marked using \\fun{dline}.", echo=-1>>=
plot(sc)
v <- c("2020-03-01", "2020-03-28", "2020-09-01", "2020-09-26")
dline(v, lty = 2, col = "grey50")
@
%
The dates listed in \code{v} were chosen by inspecting
\cref{fig:ontario-smooth-2} for intervals where the spline
increases roughly linearly, then interpolating by eye the
calendar on the bottom axis to find interval endpoints.
In practice, some iteration with \fun{dline} is typically
required to settle on reasonable endpoints.


\subsection{Initializing the fitting machinery with \tops{\protect\fun{egf_init}}{egf\_init()}}
\label{sec:ontario-init}

We will fit a logistic model with negative binomial observations
(see \cref{sec:models}) to each of the first and second waves in
the Ontario time series (see \cref{fig:ontario-smooth-3}).
The optimization for each wave (see \cref{sec:mle}) is carried
out by \pkg{epigrowthfit} function \fun{egf},
but requires initialization by function \fun{egf_init}.

The first two arguments of \fun{egf_init}, \code{date} and \code{cases},
supply an interval incidence time series. They must be defined exactly
like the so-named arguments of \fun{smooth_cases}
(see \cref{sec:ontario-smooth}). The next three arguments are:

\nolinenumbers\par\vspace{6pt}
\begin{tabular}{rl}
\code{curve} & options \code{`2"exponential"`2}, \code{`2"logistic"`2} (default), or \code{`2"richards"`2} \\
\code{distr} & options \code{`2"pois"`2} or \code{`2"nbinom"`2} (default) \\
\code{include_baseline} & options \code{`1TRUE`1} or \code{`1FALSE`1} (default)
\end{tabular}
\vspace{6pt}\par\linenumbers\noindent
%
Respectively, these specify
the model of expected cumulative incidence
(see \cref{sec:models-expected}),
the model of observed interval incidence
(see \cref{sec:models-observed}),
and whether to account for baseline growth
(see \cref{sec:models-baseline}).

The remaining arguments specify a ``fitting window''
and initial estimates of all model parameters.

\paragraph{Fitting window.}
Only cases observed during the fitting window, a time interval with
endpoints in \code{date}, are considered when fitting an epidemic
model. \fun{egf_init} arguments \code{first} and \code{last} are
used to index the left and right endpoints. Hence the fitting window
starts at \code{date[first]} and ends at \code{date[last]}, and
only the counts \code{cases[(first+`11`1):last]} affect the fitted
model.%
\footnote{
  \code{cases[first]} is excluded because it counts cases observed
  prior to \code{date[first]}, the start of the fitting window.
}
Dates of the form \code{`2"YYYY-MM-DD"`2} can be assigned to
\code{first} and \code{last} instead of integer indices,
in which case conversion to index is accomplished internally
as follows.

<<ontario-init-1, eval=FALSE>>=
## Not run
index <- which.min(abs(date - as.Date("YYYY-MM-DD")))
@

Ma \etal~\cite{Ma+14} examined the sensitivity of estimates of
the initial exponential growth rate $r$ and the coverage of the
associated 95\% confidence intervals to the endpoints of the
fitting window. A reasonable initial approach for the logistic
and Richards models, based on their analysis, is to start the
window at the time when interval incidence begins growing
exponentially (linearly on a logarithmic scale) and to end it
between the time when growth becomes subexponential and the
time of peak incidence.%
\footnote{
  When fitting an exponential model, this approach results
  in underestimation of $r$, because growth becomes subexponential
  before the end of the fitting window. With an exponential model,
  it is typically better for the fitting window to end at or before
  the time when growth becomes subexponential~\cite{Ma+14}.
}

Following \cref{fig:ontario-smooth-3}, we will use
\code{first = `127`1} and \code{last = `176`1}
for the first wave in the Ontario data and
\code{first = `1211`1} and \code{last = `1236`1}
for the second wave.

It is worth noting that \code{first} and \code{last}
are optional arguments. If not set explicitly in the
function call, then \fun{egf_init} will attempt to
choose a reasonable fitting window using an internally
defined algorithm (run \code{?egf_init} for details).
This algorithm relies heavily on an assumption that
the time series specified by \code{date} and \code{cases}
spans exactly one epidemic wave (\ie one rise and one fall).
It is also highly sensitive to noise around the base
and peak of the wave. Hence, except in special cases,
it a good idea to set \code{first} and \code{last}
explicitly based on prior inspection of the time series.

In general, \code{smooth_cases} plots greatly simplify the task
of defining reasonable fitting windows. Generating a plot like
\cref{fig:ontario-smooth-3} before constructing a call to
\fun{egf_init} is usually a good idea.

\paragraph{Initial parameter estimates.}
The optimization algorithms employed by \pkg{epigrowthfit}
(see \cref{sec:mle}) are iterative and require an initial estimate
$\vec{\wh{\theta}}_\text{init}$ of parameter vector $\vec{\theta}$.
The exact choice of $\vec{\wh{\theta}}_\text{init}$ is typically
not critical, but a reasonable initial guess is important
for convergence. \fun{egf_init} argument \code{theta_init}, which
expects a named numeric vector (run \code{?egf_init} for details),
can be used to specify (positive) initial estimates for some or
all relevant model parameters, but is optional, as every parameter
has a reasonable, internally defined default.

The defaults for $r$ (\cref{eq:exponential,eq:logistic,eq:richards})
and $c_0$ (\cref{eq:exponential}) are $\beta_1$ and $e^{\beta_0}$,
respectively, where $\beta_1$ and $\beta_0$ are the slope and intercept
of a linear model fit to $\log(1+x)$-transformed cumulative incidence,
using only observations in the first half of the fitting window, where
growth is closest to exponential.%
\footnote{
  Here, ``cumulative incidence'' refers to the number of cases
  observed since the start of the fitting window, and ``intercept''
  refers to the value of the linear model at the start of the
  fitting window.
}
The defaults for $\thalf$ and $K$ (\cref{eq:logistic,eq:richards})
assume that interval incidence curve during the focal wave is roughly
symmetric about a peak. For $\thalf$, the default is time between
the start of the fitting window and the peak. For $K$, the default is
2 times the number of cases observed in this interval. The default
for $p$ (\cref{eq:richards}) is 1, as this value reduces the Richards
model to a logistic model. Finally, for simplicity, the defaults for
$k$ and $b$ are 1 and 1 day$^{-1}$, respectively.%
\footnote{
  A more reasonable initial estimate of $b$ is supplied by the average
  baseline (\eg all causes minus disease) mortality rate in the years
  preceding and following an epidemic. The default $b = 1$ day$^{-1}$
  can be overridden by setting
  \code{theta_init = `9c`9(b = average_mortality_rate)} explicitly,
  assuming that \code{include_baseline = `1TRUE`1}.
}

Note that the defaults for $\thalf$ and $K$ require \fun{egf_init}
to know the date of the peak in the focal wave. This date can be
specified by assigning argument \code{peak} the index of the peak
time in \code{date} or a date to be coerced to index
(see \code{first} and \code{last} above).

Following \cref{fig:ontario-smooth-3}, we will use
\code{peak = `76`1} for the first wave in the Ontario data and
\code{peak = `2"2020-10-10"`2} for the second wave. Since the
second wave does not actually attain a peak, we have chosen a
date (October 10, 2020) slightly later than the time when growth
becomes subexponential.

\par\nolinenumbers\,\par\linenumbers

We are finally equipped to run \fun{egf_init}. We do this once for
each of the two waves in the Ontario time series.

<<ontario-init-2>>=
## First wave
init1 <- egf_init(
  date = ontario$date,
  cases = ontario$new_confirmed,
  curve = "logistic",
  distr = "nbinom",
  include_baseline = FALSE,
  first = 27,
  last = 76,
  peak = 76
)

## Second wave
init2 <- egf_init(
  date = ontario$date,
  cases = ontario$new_confirmed,
  curve = "logistic",
  distr = "nbinom",
  include_baseline = FALSE,
  first = 211,
  last = 236,
  peak = "2020-10-10"
)
@
%
The output of each initialization is an \code{egf_init} object---%
a list with \Sexpr{length(init1)} elements containing information
used by \fun{egf} and methods for class \code{egf_init}.

<<ontario-init-3>>=
class(init1)
names(init1)
@
%
Elements \code{date}, \code{cases}, \code{curve}, \code{distr},
\code{include_baseline}, \code{first}, and \code{last} are copies
of the arguments of \fun{egf_init} (after conversion to index if
\code{first} and \code{last} are assigned dates in the function
call). Element \code{time} expresses each date in \code{date} as
a number of days since \code{date[first]}.

<<ontario-init-4>>=
with(init1, identical(time, as.integer(date - date[first])))
@
%
Element \code{theta_init} is a numeric vector listing initial
parameter estimates, which in this example were all chosen
internally by \fun{egf_init}. Element \code{log_theta_init}
gives the log-transformed estimates. To retrieve \code{theta_init}
and \code{log_theta_init}, you can use the \fun{coef} method
for class \code{egf_init}.

<<ontario-init-5>>=
coef(init1) # same as `init1$theta_init`
coef(init1, log = TRUE) # same as `init1$log_theta_init`
@
%
Here, $r$ is expressed in units per day and $\thalf$ in days.

Element \code{eval_cum_inc} is a closure%
%
\footnote{
  A closure is formally defined as a pairing of a function and an
  environment binding free variables---variables not defined among
  the formal arguments or in the body of the function. By this
  definition, nearly every function in R is a closure. Nonetheless,
  we use ``closure'' instead of ``function'' here to emphasize that
  function \fun{eval_cum_inc} is not self-contained: its evaluation
  relies on the definition of free variables \code{theta_init},
  \code{curve}, and \code{include_baseline}, as can be seen by
  printing \code{eval_cum_inc}. \fun{eval_cum_inc} retrieves these
  variables from its enclosing environment,
  \code{`9environment`9(eval_cum_inc)},
  which is the execution environment
  of its parent function \fun{egf_init}.
}
%
taking numeric arguments \code{time} and \code{theta} (default is
\code{theta_init}) that specify times in days since \code{date[first]}
and parameter values named as in \code{theta_init}.
\code{`9eval_cum_inc`9(time, theta)} evaluates expected cumulative
incidence since \code{date[first]} at \code{time} days since
\code{date[first]}, conditional on parameter vector \code{theta}.
That is, it evaluates the right-hand side of
\cref{eq:exponential,eq:logistic,eq:richards}  (\cref{eq:logistic}
in this example). Since \cref{eq:exponential,eq:logistic,eq:richards}
model growth \emph{within} the fitting window but not necessarily
outside of it, the output of \code{`9eval_cum_inc`9(time, theta)}
must be interpreted carefully when argument \code{time} contains
elements less than \code{object$time[object$first]} (zero) or greater
than \code{object$time[object$last+`11`1]}, where \code{object} is
the \code{egf_init} object.

The \fun{predict} method for class \code{egf_init} provides a
convenient interface to \fun{eval_cum_inc}.

<<ontario-init-6>>=
predict(init1, time = with(init1, time[first:last]))
@
%
The output of \fun{predict} is a list with elements \code{time},
\code{refdate}, \code{cum_inc}, and \code{int_inc}. Element
\code{time} is a copy of the \code{predict} argument. Element
\code{refdate} gives the date from which times are measured,
namely \code{date[first]}. Element \code{cum_inc} is the value
of \code{`9eval_cum_inc`9(time)}, and element \code{int_inc}
is just \code{`9c`9(`1NA`1, `9diff`9(cum_inc))}, the interval
incidence time series associated with \code{cum_inc}.
The \fun{predict} method should be preferred over direct use
of \fun{eval_cum_inc}, as it will warn when you pass times
outside of the fitting window:

<<ontario-init-7>>=
invisible(predict(init1, time = with(init1, time[first] - 0.1)))
invisible(predict(init1, time = with(init1, time[last] + 0.1)))
@

Finally, element \code{call} of the \code{egf_init} object is
the call to \fun{egf_init}. Its inclusion allows the
\code{egf_init} object to be updated using \pkg{stats} function
\fun{update}. This is convenient when you want to change the
fitting window but not the details of the model being fit.
For example, \code{init2} could have been defined by updating
the call to \fun{egf_init} that generated \code{init1}.

<<ontario-init-8>>=
init2_redo <- update(init1,
  first = 211,
  last = 236,
  peak = "2020-10-10"
)
identical(init2, init2_redo, ignore.environment = TRUE)
@
%
For details, run \code{?update}.


\subsection{Plotting and printing \tops{\protect\code{egf_init}}{egf\_init} objects}
\label{sec:ontario-init-plot}

The \fun{plot} method for class \code{egf_init} displays together
the main components of an \code{egf_init} object. It takes an
argument \code{inc} indicating the type of incidence to plot:
\code{`2"cumulative"`2} or \code{`2"interval"`2} (default).
Conveniently, it takes an \code{add} argument, allowing one to
plot multiple \code{egf_init} objects together.

<<ontario-init-plot-1, fig.height=7, fig.cap='Result of \\code{`9plot`9(init, inc)}: \\textbf{[Top]} cumulative incidence (\\code{inc = `2"cumulative"`2}) and \\textbf{[Bottom]} interval incidence (\\code{inc = `2"interval"`2}). Points are observed incidence. Lines are expected incidence conditional on initial parameter estimates. Initial parameter estimates are guesses, hence the lines are \\emph{not} fit to the data. Incidence is plotted on a logarithmic scale, with zeros plotted directly on the horizontal axis. The shaded rectangle displays the selected fitting window.'>>=
par(mfrow = c(2, 1)) # create a two-panel plot
plot(init1, inc = "cumulative")
plot(init2, inc = "cumulative", add = TRUE)
plot(init1, inc = "interval")
plot(init2, inc = "interval", add = TRUE)
@
%
Do not be alarmed if the curves in an \code{egf_init} plot do not
fit the data particularly well, as in \cref{fig:ontario-init-plot-1}.
Poor fits are to be expected from crude initial estimates of model
parameters. An \code{egf_init} plot should be used mainly as a final
check that the fitting window chosen for each wave is reasonable.

The exact endpoints of the chosen fitting windows will not be
obvious from an \code{egf_init} plot. The \fun{print} method for
class \code{egf_init} will tell you more about a fitting window
and remind you that actually fitting a model requires a call to
\fun{egf} (see \cref{sec:ontario-fit}).

<<ontario-init-plot-2>>=
init1 # same as `print(init1)`
@


\subsection{Fitting the model with \tops{\protect\fun{egf}}{egf()}}
\label{sec:ontario-fit}

\cref{fig:ontario-init-plot-1} shows that our initial guesses
$\vec{\wh{\theta}}_\text{init}$ of the model parameter vectors
$\vec{\theta}$ produce poor fits to the waves in the Ontario
time series. Function \fun{egf} uses these initial guesses as
a starting point in an optimization algorithm to obtain superior
estimates---more precisely, numerical approximations of the
maximum likelihood estimates $\vec{\wh{\theta}}_\text{mle}$
(see \cref{sec:mle}).

The first argument of \fun{egf}, \code{init}, expects an
\code{egf_init} object. The second argument \code{method}
lets you specify an optimization algorithm. The options are
\code{`2"nlminb"`2} (default), \code{`2"nlm"`2},
\code{`2"Nelder-Mead"`2}, \code{`2"BFGS"`2},
\code{`2"L-BFGS-S"`2}, and \code{`2"CG"`2}.
The third argument \code{na_action} indicates whether \fun{egf}
should throw an error if it finds missing values in the fitting
window, or otherwise ignore them when calculating likelihoods
(see \cref{sec:mle}). The options are \code{`2"fail"`2} and
\code{`2"exclude"`2} (default). An optional fourth argument
\code{nbdisp_tol} defines a threshold $\kappa$ (default is 100)
on the negative binomial dispersion parameter $k$.
If $k > \kappa \max_i \{\mu_i\}$ in the fitted model,
then \fun{egf} will issue a warning suggesting to refit using
a Poisson model (see \cref{sec:choosing}). Further optional
arguments will be passed directly to \fun{nlminb}, \fun{nlm},
or \fun{optim}, depending on \code{method}. Below, we pass
\code{init1} and \code{init2}, in turn, to \fun{egf}, each
time accepting the defaults for \code{method}, \code{na_action},
and \code{nbdisp_tol}.

<<ontario-fit-1>>=
fit1 <- egf(init1)
fit2 <- egf(init2)
@

\fun{egf} returns an \code{egf} object---a list with
\Sexpr{length(fit1)} elements specifying the fitted model
and other information used by methods for class \code{egf}.

<<ontario-fit-2>>=
class(fit1)
names(fit1)
@
%
Elements \code{init} and \code{method} are copies of the arguments
of \fun{egf}. Element \code{theta_fit} is a numeric vector listing
\emph{fitted} parameter estimates, and element \code{log_theta_fit}
gives the log-transformed estimates. To retrieve \code{theta_fit}
and \code{log_theta_fit}, you can use the \fun{coef} method for
class \code{egf}.

<<ontario-fit-3>>=
coef(fit1) # same as `fit1$theta_fit`
coef(fit1, log = TRUE) # same as `fit1$log_theta_fit`
@
%
Once again, $r$ is expressed in units per day and $\thalf$ in days.

Element \code{nll} is the negative log likelihood of
\code{log_theta_fit}.%
\footnote{Recall from \cref{sec:mle} that \pkg{epigrowthfit} defines the
negative log likelihood as a function of log-transformed parameters.}
Elements \code{nll_func} and \code{nll_grad} are closures taking
a single numeric argument \code{log_theta} (default is
\code{log_theta_fit}) specifying log-transformed parameter values in
the order used by \code{log_theta_fit}. They evaluate the negative log
likelihood function and its gradient with respect to log-transformed
parameters at \code{log_theta}.

<<ontario-fit-4>>=
## Temporarily add list subset to search path
attach(fit1[c("log_theta_fit", "nll", "nll_func", "nll_grad")],
  name = "tmp"
)

nll_func(log_theta_fit)
nll_grad(log_theta_fit)
identical(nll, nll_func(log_theta_fit))
@
%
It is a good idea to check that:
%
\begin{enumerate}
\item the norm of the gradient evaluated at \code{log_theta_fit}
  is some orders of magnitude less than one;
\item that small, random perturbations of \code{log_theta_fit}
  have lower likelihood (greater negative log likelihood).
\end{enumerate}
%
These checks provide some assurance that \code{log_theta_fit}
approximates a local maximum of the likelihood surface. They do
\emph{not} rule out the possibility of multiple local maxima.
Hence \code{log_theta_fit} may still not approximate the global
maximum. Such apprehensions are typically resolved by plotting
the model to assess goodness of fit (see \cref{sec:ontario-fit-plot}).

<<ontario-fit-5>>=
## Check 1
norm(nll_grad(log_theta_fit), "2")

## Check 2
n <- 1e04
ptb <- replicate(n, rnorm(log_theta_fit, mean = 0, sd = 0.05))
ltf_ptb <- log_theta_fit + ptb
nll_ptb <- apply(ltf_ptb, 2, nll_func)
head(nll_ptb)
sum(nll_ptb > nll) / n # should be close to 1

## Remove list subset from search path
detach("tmp")
@

Element \code{eval_cum_inc} of the \code{egf} object is a
closure identical to that found in the \code{egf_init} object
(see \cref{sec:ontario-init}), except that the default value
of argument \code{theta} is \code{theta_fit}, not \code{theta_init}.
When \code{theta = theta_fit} is desired, use the \fun{predict}
method for class \code{egf} instead of calling \fun{eval_cum_inc}
directly.

<<ontario-fit-6>>=
predict(fit1, time = with(init1, time[first:last]))
@
%
The output should be interpreted like that of the
method for class \code{egf_init} (see again \cref{sec:ontario-init}).

Elements \code{madf_out} and \code{optim_out} of the \code{egf}
object save the output of \cran{TMB} function \fun{MakeADFun}
and the output of the \pkg{stats} optimizer called by \fun{egf}
(\fun{nlminb}, \fun{nlm}, or \fun{optim}). These are retained
in the \code{egf} object to allow the user to diagnose poor fits,
as well as warnings and errors issued by the optimizer.
For details, consult the relevant documentation
(\eg by running \code{?TMB::MakeADFun}).

Element \code{large_nbdisp_flag}, present only if
\code{init$distr = `2"nbinom`2"} as in this example,
is a logical scalar indicating whether $k$ in the fitted model
(\ie \code{theta_fit[[`2"nbdisp"`2]]}) exceeds the threshold
defined by argument \code{nbdisp_tol} (see above).

<<ontario-fit-7>>=
fit1$large_nbdisp_flag
@

Finally, element \code{call} saves the call to \fun{egf},
allowing one to repeat the call with different arguments
using \pkg{stats} function \fun{update}. For example,
one can verify whether a different optimization algorithm
arrives at a different fit as follows.

<<ontario-fit-8>>=
fit1_redo <- update(fit1, method = "Nelder-Mead")
norm(coef(fit1) - coef(fit1_redo), "2") / norm(coef(fit1), "2")
fit1$nll - fit1_redo$nll
@


\subsection{Plotting and printing \tops{\protect\code{egf}}{egf} objects}
\label{sec:ontario-fit-plot}

The \fun{plot} and \fun{print} methods for class \code{egf}
replicate the methods for class \code{egf_init}
(see \cref{sec:ontario-init-plot}), except that what is plotted
and printed is no longer an initial guess but a fit to the data
obtained by via optimization.

<<ontario-fit-plot-1, fig.height=7, fig.cap='Result of \\code{`9plot`9(fit, inc)}: \\textbf{[Top]} cumulative incidence (\\code{inc = `2"cumulative"`2}) and \\textbf{[Bottom]} interval incidence (\\code{inc = `2"interval"`2}). Points are observed incidence. Lines are expected incidence conditional on fitted parameter estimates. Doubling time estimates and their 95\\% confidence intervals are printed next to the lines. Incidence is plotted on a logarithmic scale, with zeros plotted directly on the horizontal axis. The shaded rectangle displays the selected fitting window.'>>=
par(mfrow = c(2, 1)) # create a two-panel plot
plot(fit1, inc = "cumulative")
plot(fit2, inc = "cumulative", add = TRUE)
plot(fit1, inc = "interval")
plot(fit2, inc = "interval", add = TRUE)
@
%
Comparing \cref{fig:ontario-init-plot-1,fig:ontario-fit-plot-1}
shows that the fits are much better than the initial guesses
(as one might hope!).

<<ontario-fit-plot-2>>=
fit1 # same as `print(fit1)`
@


\subsubsection{Exceptional points in interval incidence plots}

Plots of interval incidence
by \fun{plot.egf_init} (see \cref{fig:ontario-init-plot-1})
and \fun{plot.egf} (see \cref{fig:ontario-fit-plot-1})
should be interpreted with care. The displayed curve is supported on
a grid of equally spaced time points, and the spacing chosen is the
median observation interval. However, the displayed data may not be
equally spaced. This is not the case in the Ontario data, which
\emph{are} equally spaced, but it is the case in the Alberta data,
in which most observations are counts over one day, but some are
counts over longer intervals.

<<ontario-unequal-spacing-1>>=
alberta <- subset(canadacovid, province == "AB")
dt <- as.integer(diff(alberta$date))

## Median
median(dt)

## Frequency table
dtt <- table(dt)
setNames(dtt, paste0(names(dtt), "d"))
@
%
Counts over fewer or more days than the median observation interval
will tend to deviate from the plotted curve not due to chance, but
due to scale. These exceptional counts should be noted when visually
assessing whether the plotted curve fits the data. The \fun{plot}
method for classes \code{egf_init} and \code{egf} highlight exceptional
counts according to optional argument \code{tol}, though this
feature is not demonstrated in \cref{fig:ontario-init-plot-1} or in
\cref{fig:ontario-fit-plot-1}, which show the equally spaced Ontario
data.


\subsubsection{Customizing plots}

The \fun{plot} methods for classes \code{egf_init} and \code{egf}
takes an optional argument \code{style} allowing for easy customization
of various plot elements (\eg line type, width, and colour).
In addition to \code{style}, one can set \code{xlim}, \code{ylim},
\code{xlab}, \code{ylab}, and \code{main} to adjust the axis limits,
axis titles, and plot title. (Conveniently, for \code{xlim}, you can
specify a date range instead of numeric coordinates.) For a complete
list of available options, run \code{?plot.egf}.

To demonstrate some of what is possible, we will recreate the bottom
panel of \cref{fig:ontario-fit-plot-1}, zooming in on the first wave.

<<ontario-custom-plot-1, fig.height=3.5, fig.cap="Customization of the bottom panel of \\cref{fig:ontario-fit-plot-1}, showing only the first wave.">>=
plot(fit1, inc = "interval",
  xlim = c("2020-03-01", "2020-04-19"),
  xlab = "",
  ylab = "daily confirmations",
  main = "COVID-19 in Ontario, Canada",
  style = list(
    points_main = list(pch = 18, col = "plum", cex = 1.5),
    lines = list(col = "seagreen", lwd = 4, lty = 2),
    window = NULL,
    date = list(
      mgp2 = c(0.3, 2),
      cex.axis = c(0.85, 1.15),
      col.axis = c("black", "red")
    ),
    text_dbl = list(
      x = "2020-04-14", y = 10, adj = 1,
      col = "steelblue", cex = 1.25, font = 2
    )
  )
)
@


\subsection{Confidence intervals on parameter estimates}

Under construction. For now, take a look at \code{?confint.egf}.


\subsection{Simulating the fitted model}
\label{sec:ontario-fit-simulate}

Simulations of a fitted model can be used to assess the robustness
of the model to observation error (see \cref{sec:models-observed}).
One can replace the observed data with simulated data, repeat the
fitting process, and assess how much the new fit differs from the
original over many replications.

You can perform simulations using the \fun{simulate} method for class
\code{egf}. The first argument, \code{object}, expects an \code{egf}
object. Arguments \code{nsim} and \code{seed} specify a number of
simulations and an optional seed for random number generation, used
to make simulations reproducible. You can specify time points with
optional argument \code{time}, but the default,
\code{`9with`9(object$init, time[first:last])}, is typically what
you want, as it matches the time points in your simulations to the
time points in your data, restricted to the fitting window.

<<ontario-fit-simulate-1>>=
sim <- simulate(fit1, nsim = 6, seed = 1257)
@

\fun{simulate} returns an \code{egf_sim} object---a list containing
copies of \fun{simulate} arguments \code{object} and \code{time};
an element \code{refdate} storing the reference date
\code{`9with`9(object$init, date[first])}; and elements \code{int_inc}
and \code{cum_inc}, both matrices with \code{`9length`9(time)} rows
and \code{nsim} columns storing simulations.

<<ontario-fit-simulate-2>>=
names(sim)
length(sim$time) # number of time points
t(sapply(sim[c("int_inc", "cum_inc")], dim)) # dimensions of each matrix
@

For \code{i > 1}, \code{int_inc[i, j]} is the number of cases
observed between \code{time[i-`11`1]} and \code{time[i]} in
simulation \code{j} (simulated interval incidence). Each element
of row vector \code{int_inc[i, ]} is sampled independently from
a Poisson or negative binomial distribution (depending on
\code{object$init$distr}, in this case \code{`2"nbinom"`2})
with mean equal to \code{`9predict`9(object, time)$int_inc[i]}.
The negative binomial dispersion parameter, if used,
takes its fitted value \code{object$theta_fit[[`2"nbdisp"`2]]}.

\code{cum_inc[i, j]} is the number of cases observed up to
\code{time[i]} in simulation \code{j} (simulated cumulative
incidence). Column vector \code{cum_inc[, j]} is computed as
\code{c0 + `9cumsum`9(`9c`9(`10`1, int_inc[, j]))}, where
\code{c0 = `9predict`9(object, time)$cum_inc[`11`1]} is the
value of expected cumulative incidence at \code{time[`11`1]}.

<<ontario-fit-simulate-3>>=
lapply(sim[c("time", "int_inc", "cum_inc")], head)
@

The \fun{plot} method for class \code{egf_sim} plots the predicted curve
on top of the simulated time series. Optional arguments \code{col_pred}
and \code{col_sim} can be used to specify alternative colours, which may
be desirable when the number of simulations is large and greater
transparency is required to properly visualize their distribution
(see, for example, function \fun{alpha} in the \cran{scales} package).

<<ontario-fit-simulate-4, fig.height=2.5, fig.cap='Result of \\code{`9plot`9(sim, inc)}: \\textbf{[Left]} cumulative incidence (\\code{inc = `2"cumulative"`2}) and \\textbf{[Right]} interval incidence (\\code{inc = `2"interval"`2}). Simulated time series are plotted in grey with transparency. Predicted curves are plotted in colour.'>>=
par(mfrow = c(1, 2))
plot(sim, inc = "cumulative")
plot(sim, inc = "interval")
@


\subsection{Epidemic parameters derived from \tops{$r$}{r}}
\label{sec:ontario-extras}

Here, we demonstrate \pkg{epigrowthfit} functions
\fun{compute_doubling_time}, \fun{compute_R0}, and
\fun{compute_final_size}, which can be used to compute
salient epidemic parameters (see \cref{sec:extras})
from initial exponential growth rates $r$. We will
make use of the \code{egf} objects btained in
\cref{sec:ontario-fit}, namely \code{fit1}.
These contain our estimates of $r$ for the first and
second waves in the Ontario time series.


\subsubsection{Doubling time}
\label{sec:ontario-doubling-time}

\fun{compute_doubling_time} is a generic function taking a single
argument \code{x}, which can be a numeric vector listing initial
exponential growth rates $r$ in units day$^{-1}$, an \code{egf_init}
object, or an \code{egf} object. If \code{x} is a numeric vector,
then it returns a \code{doubling_time} object---a numeric vector
listing doubling times in corresponding to the elements of \code{x},
with a \fun{print} method that specifies the units (always days).
If \code{x} is an \code{egf_init} object, then it applies the
method for numeric vectors to the initial estimate of $r$, namely
\code{x$theta_init[[`2"r"`2]]}. If \code{x} is an \code{egf} object,
then it applies the method for numeric vectors to the \emph{fitted}
estimate of $r$, namely \code{x$theta_fit[[`2"r"`2]]}.

<<ontario-doubling-time-1>>=
compute_doubling_time(10^(-2:0)) # method for class "numeric"
compute_doubling_time(fit1) # method for class "egf"
compute_doubling_time(fit2)
@

The doubling time estimates from \code{fit1} and \code{fit2} are
conveniently displayed in \cref{fig:ontario-fit-plot-1}, along with
their 95\% confidence intervals.


\subsubsection{Basic reproduction number}
\label{sec:ontario-R0}

\fun{compute_R0} is a generic function that uses \cref{eq:R0}
to calculate the basic reproduction number $\R_0$ as a function
of the initial exponential growth rate $r$. This calculation
depends on interval endpoints $\tau_i$ and bin probabilities
$p_i$ defining a binned generation interval distribution
(see \cref{sec:R0}).

Function \fun{compute_R0} takes numeric arguments \code{x},
\code{breaks}, and \code{probs}, specifying $r$ in units
day$^{-1}$, $\tau_i$ in days, and $p_i$, respectively.
It is vectorized in \code{x}, so \code{x} can have length
greater than 1. Alternatively, \code{x} can be an \code{egf_init}
or \code{egf} object, in which case a value for $r$ is
taken from \code{object$theta_init[[`2"r"`2]]} or
\code{object$theta_fit[[`2"r"`2]]}, respectively.
\code{probs} must have length \code{`9length`9(breaks)-`11`1},
and \code{probs[i]} should define the probability that the
generation interval is between \code{breaks[i]} and
\code{breaks[i+1]} days.%
\footnote{It is actually sufficient for \code{probs} to give
probability \emph{weights}, because \fun{compute_R0} normalizes
for you, \ie it assigns \code{probs <- probs / `9sum`9(probs)}
internally.}

For the purpose of demonstration, we assume that the generation
interval in days follows the
\href{https://en.wikipedia.org/wiki/Gamma_distribution}{gamma distribution}
fitted by Ganyani \etal~\cite{Gany+20} to line list data from
the COVID-19 epidemic in Tianjin, China (see \cref{fig:dgamma}).
\pkg{epigrowthfit} data set \code{covid_generation_interval} supplies
the parameters of this distribution,
called \code{shape} and \code{scale}, and values for \code{breaks} and
\code{probs} that can be passed directly to \fun{compute_R0}.

<<ontario-R0-1>>=
data(covid_generation_interval)
covid_generation_interval
@
%
Note that \code{probs} is reproduced by evaluating the
gamma distribution function at \code{breaks} with \pkg{stats}
function \fun{pgamma}, then differencing the result:

<<ontario-R0-2>>=
## Temporarily add list to search path
attach(covid_generation_interval, name = "tmp")

identical(probs, diff(pgamma(breaks, shape = shape, scale = scale)))
@
%
We are now equipped to call \fun{compute_R0}.

<<ontario-R0-3>>=
compute_R0(10^(-2:0), breaks, probs) # method for class "numeric"
compute_R0(fit1, breaks, probs) # method for class "egf"
compute_R0(fit2, breaks, probs)

## Remove list from search path
detach("tmp")
@

<<dgamma, fig.height=2.5, fig.cap="Estimated distribution of the COVID-19 generation interval in days, from Ganyani \\etal~\\cite{Gany+20}, Table 4, Scenario 2. The mean, 2.57 days, is indicated with a dashed line.", echo=FALSE>>=
x <- 10^seq(-1, log10(20), length.out = 150)
fx <- with(covid_generation_interval,
  dgamma(x, shape = shape, scale = scale)
)

## Setup
par(mar = c(3, 4, 1, 1), mgp = c(3, 0.7, 0), las = 1)

## Plot
plot.new()
plot.window(xlim = range(x), ylim = c(0, max(fx[-1]) * 1.04),
            xaxs = "i", yaxs = "i")
lines(x, fx, lwd = 3, col = "grey80")
abline(v = with(covid_generation_interval, shape * scale), lty = 2)
box(bty = "l")
axis(side = 1)
axis(side = 2)
title(xlab = "generation interval, days", line = 2)
title(ylab = "probability density")
@


\subsubsection{Epidemic final size}
\label{sec:ontario-final-size}

\fun{compute_final_size} uses \cref{eq:final-size-general}
to calculate the expected epidemic final size $Z$ as a
function of the basic reproduction number $\R_0$ and the initial
susceptible and infected proportions $S_0,I_0 \in [0,1]$.
Accordingly, it takes numeric arguments \code{R0}, \code{S0},
and \code{I0}. \fun{compute_final_size} is vectorized in every
argument. To make this possible, \code{R0}, \code{S0}, and
\code{I0} are recycled up to the length of the longest of the
three vectors, and $Z$ is computed elementwise.

Below, we accept the defaults for \code{S0} and \code{I0},
which are 1 and 0. These are valid for epidemics seeded in
a large population by one infected individual.

<<ontario-final-size-1>>=
R0 <- 10^seq(0, 1, by = 0.05)
compute_final_size(R0)
@

\section{Missing functionality (to be implemented soon)}

\begin{itemize}
\item Confidence intervals on fitted incidence curves.
\item Regularization to penalize extreme parameter vectors.
\item Mixed effects models accounting for heterogeneity between
  jurisdictions in a principled way.
\end{itemize}


\end{document}

