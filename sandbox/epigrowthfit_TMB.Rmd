---
title: "implementing epigrowthfit in TMB"
bibliography: "../inst/REFERENCES.bib"
---

The `epigrowthfit` package, based on @Ma+14, is useful but has grown organically and has a variety of annoying features. Some of our ideas for improvement include:

- better estimation via
     - reparameterization (cf. @Smir+17, @DeCamp17), in particular substituting a half-time $\tau$ for $x_0$
	 - implementation in TMB (see below)
- cleaner implementation of link functions, relabeling of parameters based on whether they are on the constrained or unconstrained scale
- replace S4 with S3 methods?
- better/more flexible methods for choosing the *end* of the fitting window
- handle Poisson/NB distinction better; maybe use generalized Poisson (which can be over/underdisp)? Or fit $1/\theta$ and use a constraint at 0?
- allow for random effects (e.g. fitting many regions simultaneously)

```{r pkgs,message=FALSE}
library(TMB)
library(epigrowthfit)
library(tidyverse)
library(bbmle)
library(broom.mixed)
```

Get Canadian COVID data and reduce it to a simple format:
```{r get_data}
ontario <- (dplyr::filter(canada_covid,Province=="ON")
    ## need time to be numeric
    %>% mutate(time=lubridate::decimal_date(Date))
    %>% select(time, Date, x=newConfirmations)
    ## NAs mess us up
    %>% drop_na(x)
)
```

Use the `epigrowthfit` machinery to decide on a fitting window and derive initial parameter estimates

```{r inits}
i <- with(ontario,
          getInits(time, x, model="richards", distrib="nbinom"))
```


Select data up to the peak and set time to start at 0:

```{r data2}
dd <- (ontario[i$first:i$peak,]
    %>% mutate_at("time", ~ . -min(.))
)
```

Set up parameter vector: the peak is a good initial guess for the half-maximum-time of the cumulative incidence curve. (We're using `p` internally in the TMB code based on the presentation in @Smir+17; @Ma+14 used `s`.)

```{r params}
params <- with(i, list(log_thalf=log(tail(dd$time,1))
                     , log_K=log(theta0[["K"]])
                     , log_r=log(theta0[["r"]])
                     , log_p=log(theta0[["s"]])
                     , log_nb_disp=log(theta0[["ll.k"]])
))
```

These are the essential steps for going from a TMB (C++) file to a set of functions usable in R.

```{r comp,message=FALSE}
compile("epigrowthfit_TMB.cpp")
dyn.load(dynlib("epigrowthfit_TMB"))
## translate from x0 to t_half?
dt <- diff(dd$time[1:2]) ## assume equal time steps
ad_data <- list(t=c(dd$time[1]-dt,dd$time), x=dd$x, debug=0)
lapply(ad_data,length)
m <- MakeADFun(data=ad_data, parameters=params,
               DLL="epigrowthfit_TMB",
               silent=TRUE)
lapply(environment(m$fn)$data, length)
```

Test the negative log-likelihood function and gradient function:

```{r tests}
m$fn(unlist(params)) ## NLL
m$gr(unlist(params)) ## gradient
```

We can drop the starting values, objective function, and gradient straight into any derivative-based optimizer:
```{r nlminb}
(nlminbfit <- nlminb(start=unlist(params), objective=m$fn, gradient=m$gr))
## default: Nelder-Mead
(optimfit <- optim(par=unlist(params), fn=m$fn, gr=m$gr,
      method="BFGS"))
```

We can also use `bbmle::mle2()`, provided that we specify the parameter names as an attribute of the objective function
(the BFGS optimizer is used by default).

```{r mle2}
parnames(m$fn) <- names(params)
mle2fit <- mle2(minuslogl=m$fn,
                start=params, gr=m$gr,
                vecpar=TRUE)
```

```{r}
cbind(mle2=coef(mle2fit), optim=optimfit$par, nlminb=nlminbfit$par)
```

TMB functions carry along the best-fit parameters in their environment, so we don't actually need to substitute them in.
When we set up the model we asked for the incidence curve (and its delta-method/Wald CIs) to be reported in addition to the parameters.

```{r results}
par(las=1,bty="l")
## plot data
plot(x~time,data=dd,log="y",xlab="time (years)",ylab="new confirmations")
## retrieve params from TMB object
## pp <- environment(m$fn)$last.par.best
## or from optimizer fit
pp <- nlminbfit$par
lapply(environment(m$fn)$data, length)
## plot estimated inc curve at the MLE (last params evaluated)
lines(dd$time,m$report(pp)$inccurve)
## plot estimated inc curve at the starting values
lines(dd$time,m$report(unlist(params))$inccurve,col=2)
## get CIs from sdreport (delta method)
with(sdreport(m),
     matlines(dd$time,cbind(value,value-1.96*sd,
                            value+1.96*sd),lty=c(1,2,2),
                          col=1))
legend("topleft",lty=c(1,1,2), bty="n",
       col=c(1,2,1),
       legend=c("fit","initial params","95% CI (Wald/delta)"))
```

The bump at the second time step occurs because the elapsed time is twice as long, so the
expected incidence is (approximately) doubled. We don't know why 7 March 2020 is actually
missing in the data, but the model is behaving consistently ...

Some utilities for parameters/CIs:

```{r tidy,cache=TRUE}
class(m) <- c("TMB", class(m))
tt <- function(x,...) {
    (tidy(x,...)
        %>% as_tibble()
        %>% select(-std.error)
        %>% mutate_if(is.numeric,exp)
        %>% mutate_if(is.character,~gsub("log_","",.))
    )
}
tt(m, conf.int=TRUE, conf.method="wald")
tt(m, conf.int=TRUE, conf.method="uniroot") ## needs work/fragile
## "profile" not implemented but could be?
```

## digression: multiple models in one template (`.cpp`) file

We often want the flexibility to fit several different models, with different (possibly non-nested) sets of parameters. 

### Repeat yourself 

 The simplest way to do this is to write separate `.cpp` files for every model, but this risks lots of code repetition (violating [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself))
 
### Unpack parameter vectors

A second alternative is to pass parameters as *vectors* of parameters, whose length is determined at run-time (??). This is convenient, for example, if all of the model complexity can be set up as linear sub-models of various components (as in the `params` argument to `gnls` or the `parameters` argument to `bbmle::mle2`). In this case you can use `model.matrix()` or `sparse.model.matrix()` to set up a model matrix that you pass as a data object, which effectively defines the order and meaning of the parameters in the associated parameter vector.
    * if your models do not fit into the linear model framework, then you need to unpack the parameter vectors inside the template file. Furthermore, you need some way to pass information about which model is being fitted. Probably the simplest way to do this is to pass an `enum` as a data object; this is the closest you can get to passing a factor variable. Ideally you should have a way to keep your `enum`s in sync between your R and C++ code; `glmmTMB` does this by running a shell script on the R code, but perhaps it could be done by operating on a factor?
	
For example, here is a snippet of code from a template that calculates an attack rate as constant ($a(i)=c$); as a Ricker function  of size ($a(i)=c*S(i)/d*exp(1-S(i)/d)$); or as a power-Ricker function of size ($a(i)=c*pow(S(i)/d,g)*exp(1-S(i)/d)$). Going through the model from least to most specific, we increment a parameter counter (using `p++`) for each parameter we take from the parameter vector ...

```{c++,eval=FALSE}
PARAMETER_VECTOR(consume_parms);  // parameter vector
DATA_INTEGER(size_model_flag);    // integer code for size model
// ...
enum size_model_type {
  constant  = 0,
  ricker    = 1,
  powricker = 2
};
// ...
int p=0;  // parameter counter
if(size_model_flag==constant || size_model_flag==ricker || size_model_flag==powricker) {
    c = exp(consume_parms(p++)); // max attack rate
 }
if(size_model_flag==ricker || size_model_flag==powricker) {
    d = exp(consume_parms(p++)); // size at max attack rate
 }
  
if(size_model_flag==powricker){
    g = exp(consume_parms(p++)); // shape of power ricker curve
 }
```

### map parameters

An alternative in this case is to *always* pass all of the parameters, fixing some to special values; in the example above, setting `g` to 1 reduces the power-Ricker to the Ricker. We have a problem making the constant model a nested case of the Ricker, because we need $d \to \infty$ (R can handle `Inf` values, but I don't know if TMB/C++ can - it seems risky). One alternative is to reparameterize the model so that we use `f = 1/d`; then we have $a(i)=c \textrm{pow}(f*S(i),g)\exp(1-fS(i))$, and the Ricker converges to constant when $f=0$.

There's a minor technical problem here (which is shared by a lot of model types): when the model converges to a parameter value on the boundary (i.e. only values of $f \ge 0$ make sense, not $f<0$), then a bunch of derivative-based machinery (such as the estimate of standard deviations based on the curvature of the log-likelihood surface) no longer works properly. We can use bounded (box-constrained) optimization to prevent the parameter from going negative, *or* we can fit the parameter on the log scale. However, if we fit $\log(f)$ instead of $f$, then we're still not out of the woods, because $\log(f) \to -\infty$ and we will still run into trouble with curvatures etc. (It's also possible that R and TMB would handle `Inf` values consistently, but convergence to `Inf` [e.g. in the case where a constant really is a better fit than the Ricker] is harder than convergence to zero ...)

The other issue is that TMB/optim will get confused if we have a parameter that doesn't affect the likelihood; its curvature will be zero, which will make TMB think something has gone wrong. However, we can use the `map=` argument to `TMB::MakeADFun` to specify that a parameter is fixed.

```{c++,eval=FALSE}
PARAMETER(log_c);
PARAMETER(dinv);
PARAMETER(log_g);

for (i=1; i<S.size(); i++) {
	a(i)=exp(log_c)*pow(dinv*S(i),g)*exp(1-dinv*S(i));
 }
```

Then we would (1) call `nlminb()` (or `optim(..., method="L-BFGS-B")`) with an approporiate `lower` argument (e.g. `lower=c(log_c-Inf,dinv=0,log_g-Inf)`) and call `MakeADFun()` with `parameters=list(log_c=0,dinv=0,log_g=0)` and `map=list(dinv=NA, log_g=NA)` to fit a constant model.

If we have incommensurate models we might want to set the starting parameters to `NA` (and use the `isNA` helper function defined below) as a flag for which model/parameter set to use internally ...

```{c++,eval=FALSE}
template<class Type>
bool isNA(Type x){
  return R_IsNA(asDouble(x));
}
```

Similar arguments apply to Poisson/negative binomial fits: we can use a parameterization with $1/k$ as the negative binomial parameter, fit with $1/k$ bounded at zero, and use a Poisson model instead if $1/k$ is mapped to zero in `MakeADFun`. (**n.b.** we have to be very careful with `if` statements! Can we use `CppAD::CondExpEq(left,right,trueCase,falseCase)`? Or safer/clearer to use `enum`? Or use `CppAD::CondExpLt()` with a very small cutoff?

## to do / fix me

- modify `epigrowthfit_TMB.cpp` to allow all three models (exp, logistic, Richards) and at least Poisson/nbinom responses
- check Richards parameterization (esp., make sure that our value of $r$ corresponds to growth rate at zero); re-read @Ma+14, @Smir+17?
- discuss:
    - pros and cons of `bbmle`
    - delta method vs likelihood profiles vs. importance sampling
- blue sky:
    - process error via Kalman filter??
    - *generalized* Richards?
- compute $R_0$



## References
