---
title: "implementing epigrowthfit in TMB"
bibliography: "../inst/REFERENCES.bib"
---

The `epigrowthfit` package, based on @Ma+14, is useful but has grown organically and has a variety of annoying features. Some of our ideas for improvement include:

- better estimation via
     - reparameterization (cf. @Smir+17, @DeCamp17), in particular substituting a half-time $\tau$ for $x_0$
	 - implementation in TMB (see below)
- cleaner implementation of link functions, relabeling of parameters based on whether they are on the constrained or unconstrained scale
- replace S4 with S3 methods?
- better/more flexible methods for choosing the *end* of the fitting window
- handle Poisson/NB distinction better; maybe use generalized Poisson (which can be over/underdisp)? Or fit $1/\theta$ and use a constraint at 0?
- allow for random effects (e.g. fitting many regions simultaneously)

```{r pkgs,message=FALSE}
library(TMB)
library(epigrowthfit)
library(tidyverse)
theme_set(theme_bw())
library(colorspace)
scale_colour_discrete <- scale_colour_discrete_qualitative ## default colo(u)rs
library(bbmle)
library(broom.mixed)
library(numDeriv)
```

```{r enum, echo=FALSE}
factor_to_enum <- function(f,enum_name="curve_type",fn="",append=FALSE) {
    if (!append) unlink(fn) ## careful??
    if (!is.factor(f)) {  ## factorize, in order
        f <- factor(f, levels=f)
    }
    ff <- file(fn, open = if (append) "a" else "w")
    on.exit(close(ff))
    writeLines(sprintf("enum %s {",enum_name), con=ff)
    ## zero-indexed enum: not sure I need it but ???
    ss <- sprintf("   %s = %d",levels(f),seq_along(levels(f)))
    ss <- paste(ss,collapse=",\n")
    writeLines(ss, con=ff)
    writeLines("};", con=ff)
}
curve_f <- c("exponential","logistic","richards")
factor_to_enum(curve_f, fn="curve_enum.h")
distr_f <- c("poisson","nbinom2")
factor_to_enum(distr_f, enum_name="distr_type", fn="distr_enum.h")
```

Get Canadian COVID data and reduce it to a simple format:
```{r get_data}
ontario <- (dplyr::filter(canada_covid,Province=="ON")
    ## need time to be numeric
    %>% mutate(time=lubridate::decimal_date(Date))
    %>% select(time, Date, x=newConfirmations)
    ## NAs mess us up
    %>% drop_na(x)
)
```

Use the `epigrowthfit` machinery to decide on a fitting window and derive initial parameter estimates

```{r inits}
i <- with(ontario,
          getInits(time, x, model="richards", distrib="nbinom"))
```


Select data up to the peak and set time to start at 0:

```{r data2}
dd <- (ontario[i$first:i$peak,]
    %>% mutate_at("time", ~ . -min(.))
)
```

Set up parameter vector: the peak is a good initial guess for the half-maximum-time of the cumulative incidence curve. (We're using `p` internally in the TMB code based on the presentation in @Smir+17; @Ma+14 used `s`.)

```{r params}
all_params <- with(i, list(log_thalf=log(tail(dd$time,1))
                     , log_x0=log(theta0[["x0"]])
                     , log_K=log(theta0[["K"]])
                     , log_r=log(theta0[["r"]])
                     , log_p=log(theta0[["s"]])
                     , log_nbdisp=log(theta0[["ll.k"]])
))
```

These are the essential steps for going from a TMB (C++) file to a set of functions usable in R.

```{r comp,message=FALSE,results="hide"}
compile("epigrowthfit_TMB.cpp")
dyn.load(dynlib("epigrowthfit_TMB"))
exclude_params <- "log_x0"
params <- all_params[!names(all_params) %in% exclude_params]
dt <- diff(dd$time[1:2]) ## assume equal time steps
ad_data <- list(t=c(dd$time[1]-dt,dd$time), x=dd$x, debug=0,
                curve_flag=match("richards",curve_f),
                distr_flag=match("nbinom2",distr_f))
## lapply(ad_data,length)
m <- MakeADFun(data=ad_data, parameters=all_params,
               DLL="epigrowthfit_TMB",
               silent=TRUE,
               ## exclude x0 because we're not fitting an exponential model
               ## richards: ignore x0
               ## logistic: ignore x0 and p
               ## exponential: ignore K and thalf
               map=list(log_x0=factor(NA))  
               )
```

Test the negative log-likelihood function and gradient function:

```{r tests}
m$fn(unlist(params))
m$gr(unlist(params)) ## gradient
```

We can drop the starting values, objective function, and gradient straight into any derivative-based optimizer:
```{r nlminb}
(nlminbfit <- nlminb(start=unlist(params), objective=m$fn, gradient=m$gr))
```

We can use `optim` instead as long as we switch from the default Nelder-Mead method (which doesn't use gradient information, so will waste TMB's advantages) to BFGS:

```{r optim}
optimfit <- optim(par=unlist(params), fn=m$fn, gr=m$gr, method="BFGS")
```

We can also use `bbmle::mle2()`, provided that we specify the parameter names as an attribute of the objective function and specify `vecpar=TRUE` (the BFGS optimizer is used by default).

```{r mle2}
parnames(m$fn) <- names(params)
mle2fit <- mle2(minuslogl=m$fn,
                start=params, gr=m$gr,
                vecpar=TRUE)
```

```{r comp_coefs}
cbind(mle2=coef(mle2fit), optim=optimfit$par, nlminb=nlminbfit$par)
```

TMB functions carry along the best-fit parameters in their environment, so we don't actually need to substitute them in.
When we set up the model we asked for the incidence curve (and its delta-method/Wald CIs) to be reported in addition to the parameters.

```{r results}
par(las=1,bty="l")
## plot data
plot(x~time,data=dd,log="y",xlab="time (years)",ylab="new confirmations")
## retrieve params from TMB object
## pp <- environment(m$fn)$last.par.best
## or from optimizer fit
pp <- nlminbfit$par
lapply(environment(m$fn)$data, length)
## plot estimated inc curve at the MLE (last params evaluated)
lines(dd$time,exp(m$report(pp)$log_inccurve))
## plot estimated inc curve at the starting values
lines(dd$time,exp(m$report(unlist(params))$log_inccurve),col=2)
## get CIs from sdreport (delta method)
with(sdreport(m),
     matlines(dd$time,exp(cbind(value,value-1.96*sd,
                            value+1.96*sd)),lty=c(1,2,2),
                          col=1))
legend("topleft",lty=c(1,1,2), bty="n",
       col=c(1,2,1),
       legend=c("fit","initial params","95% CI (Wald/delta)"))
```


The bump at the second time step occurs because the elapsed time is twice as long, so the
expected incidence is (approximately) doubled. We don't know why 7 March 2020 is actually
missing in the data, but the model is behaving consistently ...

```{r tidy_def,echo=FALSE}
## some utilities
class(m) <- c("TMB", class(m))
tt <- function(x,...) {
    (tidy(x,...)
        %>% as_tibble()
        %>% select(-std.error)
        %>% mutate_if(is.numeric,exp)
        %>% mutate_if(is.character,~gsub("log_","",.))
    )
}
```

```{r tidy,warning=FALSE}
(tt1 <- tt(m, conf.int=TRUE, conf.method="wald"))
(tt2 <- tt(m, conf.int=TRUE, conf.method="uniroot")) ## needs work/fragile
## "profile" not implemented but could be?
```

Let's look a little more deeply into what's going on with the uniroot parameters.
(**fixme**: based on what's below, could probably write a profile CI method in `broom.mixed::tidy.TMB` that would do better ...)

```{r calc_prof,echo=FALSE,cache=TRUE}
vars <- c("log_thalf","log_r","log_p")
all_vars <- names(m$env$last.par.best)
prof0 <- purrr:::map_dfr(setNames(all_vars,all_vars),
                         ~ setNames(tmbprofile(m,name=.,trace=FALSE),c("focal","value")),
                         .id="param") %>%
    dplyr::filter(focal>-10)
## more precise profiles on problematic variables
prof1 <- purrr:::map2_dfr(setNames(vars,vars),
                 list(c(-10,3),c(2,8),c(-6,1)),
                 ~ setNames(tmbprofile(m,name=.x,parm.range=.y,trace=FALSE, ystep=0.025),c("focal","value")),
                 .id="param")
```

```{r plot_prof,echo=FALSE}
ggplot(prof0,aes(focal,sqrt(2*(value-min(value))))) + geom_point(alpha=0.5)+ geom_line() + facet_wrap(~param,scale="free_x") +
    geom_point(data=prof1,colour="red",alpha=0.3) + geom_line(data=prof1,colour="red") +
    labs(x="",y="sqrt(deviance)")
```

## digression: negative binomial models

We would often like to fit models without knowing in advance whether a negative binomial or a Poisson response is more appropriate. In principle this is easy because the Poisson is *nested* within the negative binomial (i.e., NB converges to Poisson for a particular choice of models). However, there are several problems with this "easy" solution;

* in the standard parameterization the NB converges to Poisson when the overdispersion parameter goes to infinity. R can handle infinite values:

```{r}
identical(dpois(1,lambda=1),dnbinom(1,mu=1,size=Inf))
```

but it's not clear, even though C++ can in principle handle infinite values, that this would go smoothly.

* We could change the parameterization, i.e. fit the parameter on the inverse scale, so that it converges to Poisson when the parameter goes to 0 rather than infinity. However, we still have some problems: when a model converges to a parameter value on the boundary (i.e. only values of $f \ge 0$ make sense, not $f<0$), then a bunch of derivative-based machinery (such as the estimate of standard deviations based on the curvature of the log-likelihood surface) no longer works properly. We can use bounded (box-constrained) optimization to prevent the parameter from going negative, *or* we can fit the parameter on the log scale. However, if we fit $\log(f)$ instead of $f$, then we're still not out of the woods, because $\log(f) \to -\infty$ and we will still run into trouble with curvatures etc. 

* There's another problem: there's no guarantee that the negative binomial likelihood is *numerically* stable as the dispersion parameter goes to infinity.

```{r binomtest,echo=FALSE,message=FALSE,results="hide"}
compile("dnbinom_test.cpp")
dyn.load(dynlib("dnbinom_test"))
compile("dnbinom_robust_test.cpp")
dyn.load(dynlib("dnbinom_robust_test"))
set.seed(101)
x <- rpois(100, lambda=1)
m_nb <- MakeADFun(data=list(x=x), parameters=list(log_mu=0,inv_nbdisp=1),
                  DLL="dnbinom_test",
                  silent=TRUE)
m_nbr <- MakeADFun(data=list(x=x), parameters=list(log_mu=0,log_nbdisp=0),
                   DLL="dnbinom_robust_test",
                   silent=TRUE)
kvec <- 10^seq(12,4,length=501)
TMB_nbfun <- Vectorize(function(k) { m_nb$fn(c(0,1/k)) })
TMB_nbrfun <- Vectorize(function(k) { m_nbr$fn(c(0,log(k))) })
R_nbfun <- Vectorize(function(k) { -sum(dnbinom(x,mu=1,size=k,log=TRUE)) })
fun_list <- list(TMB_nb=TMB_nbfun,
                 TMB_nbr=TMB_nbrfun,
                 R_nb=R_nbfun)
dffuns <- map(fun_list,
              ~ function(kvec) data.frame(k=kvec, val=.(kvec)))
res <- purrr::map_dfr(dffuns,~.(kvec),.id="model") %>%
    mutate(val2=val+sum(dpois(x,lambda=1,log=TRUE)))
```

```{r dnbtest_plot1,echo=FALSE}
gg0 <- ggplot(res,aes(k,val2,colour=model)) +
    scale_x_log10() +
    geom_point() +
    geom_line() +
    scale_colour_discrete(labels=c("R, dnbinom2","TMB, dnbinom2","TMB dnbinom2_robust")) +
    labs(x="dispersion parameter",y="diff from Poisson neg log-likelihood") +
    geom_hline(yintercept=0,lty=2)
print(gg0)
```

Uh-oh ...
TMB has two different functions (`dnbinom2` and `dnbinom_robust`) for parameterizing the NB by mean and variance; the first uses mean and variance, the second uses *log* mean and *log excess* variance (the variance is $\mu(1+\mu/k)$, the excess variance is $\mu^2/k$).

Zoom in on a smaller part of the range and limit the y-axis scale.

```{r dnbtest_plot3,echo=FALSE}
rng2 <- res %>% filter(model=="R_nb") %>% pull(val2) %>% range()
gg0 %+% (dplyr::filter(res, k<1e8)) + scale_y_continuous(limits=rng2,oob=scales::squish)
```

Zoom in farther, and leave out `dnbinom2` entirely ...

```{r dnbtest_plot4,echo=FALSE}
gg0 %+% (dplyr::filter(res, 1e6<k, k<1e8, model != "TMB_nb"))
```

If you want to look [inside the R code](https://github.com/wch/r-source/blob/f8d4d7d48051860cc695b99db9be9cf439aee743/src/nmath/dnbinom.c#L80-L96) you can see the careful processing that R does to try to get accurate answers across a huge range of parameter scales ... all TMB does, in contrast, is to carefully translate from the log mu/log excess var scale ... see [here](https://github.com/kaskr/adcomp/blob/12b47bec0391ce48b04475c1f814856210f204a0/TMB/inst/include/tiny_ad/robust/distributions.hpp#L26-L63).

Bottom line: even with `dnbinom_robust`, in cases where we think we are approaching Poisson, we will have to set a threshold and switch to Poisson if the dispersion parameter is unrealistically large


## digression: multiple models in one template (`.cpp`) file

We often want the flexibility to fit several different models, with different (possibly non-nested) sets of parameters. 

### Repeat yourself 

 The simplest way to do this is to write separate `.cpp` files for every model, but this risks lots of code repetition (violating [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself))
 
### Unpack parameter vectors

A second alternative is to pass parameters as *vectors* of parameters, whose length is determined at run-time (??). This is convenient, for example, if all of the model complexity can be set up as linear sub-models of various components (as in the `params` argument to `gnls` or the `parameters` argument to `bbmle::mle2`). In this case you can use `model.matrix()` or `sparse.model.matrix()` to set up a model matrix that you pass as a data object, which effectively defines the order and meaning of the parameters in the associated parameter vector.
    * if your models do not fit into the linear model framework, then you need to unpack the parameter vectors inside the template file. Furthermore, you need some way to pass information about which model is being fitted. Probably the simplest way to do this is to pass an `enum` as a data object; this is the closest you can get to passing a factor variable. Ideally you should have a way to keep your `enum`s in sync between your R and C++ code; `glmmTMB` does this by running a shell script on the R code, but perhaps it could be done by operating on a factor?
	
For example, here is a snippet of code from a template that calculates an attack rate as constant ($a(i)=c$); as a Ricker function  of size ($a(i)=c*S(i)/d*exp(1-S(i)/d)$); or as a power-Ricker function of size ($a(i)=c*pow(S(i)/d,g)*exp(1-S(i)/d)$). Going through the model from least to most specific, we increment a parameter counter (using `p++`) for each parameter we take from the parameter vector ...

```{c++,eval=FALSE}
PARAMETER_VECTOR(consume_parms);  // parameter vector
DATA_INTEGER(size_model_flag);    // integer code for size model
// ...
enum size_model_type {
  constant  = 0,
  ricker    = 1,
  powricker = 2
};
// ...
int p=0;  // parameter counter
if(size_model_flag==constant || size_model_flag==ricker || size_model_flag==powricker) {
    c = exp(consume_parms(p++)); // max attack rate
 }
if(size_model_flag==ricker || size_model_flag==powricker) {
    d = exp(consume_parms(p++)); // size at max attack rate
 }
  
if(size_model_flag==powricker){
    g = exp(consume_parms(p++)); // shape of power ricker curve
 }
```

### map parameters

An alternative in this case is to *always* pass all of the parameters, fixing some to special values; in the example above, setting `g` to 1 reduces the power-Ricker to the Ricker. We have a problem making the constant model a nested case of the Ricker, because we need $d \to \infty$ (R can handle `Inf` values, but I don't know if TMB/C++ can - it seems risky). One alternative is to reparameterize the model so that we use `f = 1/d`; then we have $a(i)=c \textrm{pow}(f*S(i),g)\exp(1-fS(i))$, and the Ricker converges to constant when $f=0$.

There's a minor technical problem here (which is shared by a lot of model types): when the model converges to a parameter value on the boundary (i.e. only values of $f \ge 0$ make sense, not $f<0$), then a bunch of derivative-based machinery (such as the estimate of standard deviations based on the curvature of the log-likelihood surface) no longer works properly. We can use bounded (box-constrained) optimization to prevent the parameter from going negative, *or* we can fit the parameter on the log scale. However, if we fit $\log(f)$ instead of $f$, then we're still not out of the woods, because $\log(f) \to -\infty$ and we will still run into trouble with curvatures etc. (It's also possible that R and TMB would handle `Inf` values consistently, but convergence to `Inf` [e.g. in the case where a constant really is a better fit than the Ricker] is harder than convergence to zero ...)

The other issue is that TMB/optim will get confused if we have a parameter that doesn't affect the likelihood; its curvature will be zero, which will make TMB think something has gone wrong. However, we can use the `map=` argument to `TMB::MakeADFun` to specify that a parameter is fixed.

```{c++,eval=FALSE}
PARAMETER(log_c);
PARAMETER(dinv);
PARAMETER(log_g);

for (i=1; i<S.size(); i++) {
	a(i)=exp(log_c)*pow(dinv*S(i),g)*exp(1-dinv*S(i));
 }
```

Then we would (1) call `nlminb()` (or `optim(..., method="L-BFGS-B")`) with an approporiate `lower` argument (e.g. `lower=c(log_c-Inf,dinv=0,log_g-Inf)`) and call `MakeADFun()` with `parameters=list(log_c=0,dinv=0,log_g=0)` and `map=list(dinv=NA, log_g=NA)` to fit a constant model.

If we have incommensurate models we might want to set the starting parameters to `NA` (and use the `isNA` helper function defined below) as a flag for which model/parameter set to use internally ...

```{c++,eval=FALSE}
template<class Type>
bool isNA(Type x){
  return R_IsNA(asDouble(x));
}
```

Similar arguments apply to Poisson/negative binomial fits: we can use a parameterization with $1/k$ as the negative binomial parameter, fit with $1/k$ bounded at zero, and use a Poisson model instead if $1/k$ is mapped to zero in `MakeADFun`. (**n.b.** we have to be very careful with `if` statements! Can we use `CppAD::CondExpEq(left,right,trueCase,falseCase)`? Or safer/clearer to use `enum`? Or use `CppAD::CondExpLt()` with a very small cutoff?

## to do / fix me

- modify `epigrowthfit_TMB.cpp` to allow all three models (exp, logistic, Richards) and at least Poisson/nbinom responses
- check Richards parameterization (esp., make sure that our value of $r$ corresponds to growth rate at zero); re-read @Ma+14, @Smir+17?
- discuss:
    - pros and cons of `bbmle`
    - delta method vs likelihood profiles vs. importance sampling
- blue sky:
    - process error via Kalman filter??
    - *generalized* Richards?
- compute $R_0$



## References
