---
title: "implementing epigrowthfit in TMB"
bibliography: "../inst/REFERENCES.bib"
---

The `epigrowthfit` package, based on @Ma+14, is useful but has grown organically and has a variety of annoying features. Some of our ideas for improvement include:

- better estimation via
     - reparameterization (cf. @Smir+17, @DeCamp17), in particular substituting a half-time $\tau$ for $x_0$
	 - implementation in TMB (see below)
- cleaner implementation of link functions, relabeling of parameters based on whether they are on the constrained or unconstrained scale
- replace S4 with S3 methods?
- better/more flexible methods for choosing the *end* of the fitting window
- handle Poisson/NB distinction better; maybe use generalized Poisson (which can be over/underdisp)? Or fit $1/\theta$ and use a constraint at 0?
- allow for random effects (e.g. fitting many regions simultaneously)

```{r pkgs,message=FALSE}
library(TMB)
library(epigrowthfit)
library(tidyverse)
theme_set(theme_bw())
library(colorspace)
library(ggstance) ## position_dodgev
scale_colour_discrete <- scale_colour_discrete_qualitative ## default colo(u)rs
library(bbmle)
library(broom.mixed)
library(numDeriv)
library(nloptr)
```

```{r checkversion,echo=FALSE}
if (packageVersion("broom.mixed")<"0.2.7") {
    warning("for best results, install the latest broom.mixed via 'remotes::install_github(\"bbolker/broom.mixed\")'")
}
```

```{r enum, echo=FALSE}
## utility function for making sure model/curve coding is synchronized between
##  TMB and R code
factor_to_enum <- function(f,enum_name="curve_type",fn="",append=FALSE) {
    if (!append) unlink(fn) ## careful??
    if (!is.factor(f)) {  ## factorize, in order
        f <- factor(f, levels=f)
    }
    ff <- file(fn, open = if (append) "a" else "w")
    on.exit(close(ff))
    writeLines(sprintf("enum %s {",enum_name), con=ff)
    ## zero-indexed enum: not sure I need it but ???
    ss <- sprintf("   %s = %d",levels(f),seq_along(levels(f)))
    ss <- paste(ss,collapse=",\n")
    writeLines(ss, con=ff)
    writeLines("};", con=ff)
}
curve_f <- c("exponential","logistic","richards")
factor_to_enum(curve_f, fn="curve_enum.h")
distr_f <- c("poisson","nbinom2")
factor_to_enum(distr_f, enum_name="distr_type", fn="distr_enum.h")
```

Get Canadian COVID data and reduce it to a simple format:
```{r get_data}
ontario <- (dplyr::filter(canada_covid,Province=="ON")
    ## need time to be numeric
    %>% mutate(time=lubridate::decimal_date(Date))
    %>% select(time, Date, x=newConfirmations)
    ## NAs mess us up
    %>% drop_na(x)
)
```

Use the `epigrowthfit` machinery to decide on a fitting window and derive initial parameter estimates

```{r inits}
i <- with(ontario,
          getInits(time, x, model="richards", distrib="nbinom"))
```


Select data up to the peak and set time to start at 0:

```{r data2}
dd <- (ontario[i$first:i$peak,]
    %>% mutate_at("time", ~ . -min(.))
)
```

Set up parameter vector: the peak is a good initial guess for the half-maximum-time of the cumulative incidence curve. (We're using `p` internally in the TMB code based on the presentation in @Smir+17; @Ma+14 used `s`.)

```{r params}
all_params <- with(i, list(log_thalf=log(tail(dd$time,1))
                         , log_x0=log(theta0[["x0"]])
                         , log_K=log(theta0[["K"]])
                         , log_r=log(theta0[["r"]])
                         , log_p=log(theta0[["s"]])
                         , log_nbdisp=log(theta0[["ll.k"]])
))
```

These are the essential steps for going from a TMB (C++) file to a set of functions usable in R.

```{r comp,message=FALSE,results="hide"}
compile("epigrowthfit_TMB.cpp")
dyn.load(dynlib("epigrowthfit_TMB"))
curve_str <- "richards"
distr_str <- "nbinom2"
exclude_params=switch(curve_str,
                      richards="log_x0",
                      logistic=c("log_x0","log_p"),
                      exponential=c("log_thalf","log_p"))
if (distr_str=="poisson") {
    exclude_params <- c(exclude_params,"log_nbdisp")
}

params <- all_params[!names(all_params) %in% exclude_params]
dt <- diff(dd$time[1:2]) ## assume equal time steps
ad_data <- list(t=c(dd$time[1]-dt,dd$time), x=dd$x, debug=0,
                curve_flag=match("richards",curve_f),
                distr_flag=match("nbinom2",distr_f))
## lapply(ad_data,length)
mapvals <- replicate(length(exclude_params),
                     factor(NA), simplify=FALSE)
names(mapvals) <- exclude_params
m <- MakeADFun(data=ad_data,
               parameters=all_params,
               DLL="epigrowthfit_TMB",
               silent=TRUE,
               map=mapvals
               )
```

Test the negative log-likelihood function and gradient function:

```{r tests}
m$fn(unlist(params))
m$gr(unlist(params)) ## gradient
```

We can drop the starting values, objective function, and gradient straight into any derivative-based optimizer:
```{r nlminb}
(nlminbfit <- nlminb(start=unlist(params), objective=m$fn, gradient=m$gr))
```

We can use `optim` instead as long as we switch from the default Nelder-Mead method (which doesn't use gradient information, so will waste TMB's advantages) to BFGS:

```{r optim}
optimfit <- optim(par=unlist(params), fn=m$fn, gr=m$gr, method="BFGS")
```

We can also use `bbmle::mle2()`, provided that we specify the parameter names as an attribute of the objective function and specify `vecpar=TRUE` (the BFGS optimizer is used by default).

```{r mle2}
parnames(m$fn) <- names(params)
mle2fit <- mle2(minuslogl=m$fn,
                start=params, gr=m$gr,
                vecpar=TRUE)
```

Finally, it may be useful to try methods from the `nloptr` package, but 

```{r nloptr}
tmpfn <- function(x) { r <- m$fn(x); cat(x,r,"\n"); return(r) }
tmpgrad <- function(x) { r <- m$gr(x); cat(x,r,"\n"); return(r) }
n1 <- nloptr(unname(unlist(params)), tmpfn, tmpgrad, opts=list(algorithm="NLOPT_LD_LBFGS",
                                                       ftol_rel=1e-8))
```

```{r comp_coefs}
cbind(mle2=coef(mle2fit), optim=optimfit$par, nlminb=nlminbfit$par,
      nloptr=n1$solution)
```

TMB functions carry along the best-fit parameters in their environment, so we don't actually need to substitute them in.
When we set up the model we asked for the incidence curve (and its delta-method/Wald CIs) to be reported in addition to the parameters.

```{r results}
par(las=1,bty="l")
## plot data
plot(x~time,data=dd,log="y",xlab="time (years)",ylab="new confirmations")
## retrieve params from TMB object
## pp <- environment(m$fn)$last.par.best
## or from optimizer fit
pp <- nlminbfit$par
lapply(environment(m$fn)$data, length)
## plot estimated inc curve at the MLE (last params evaluated)
lines(dd$time,exp(m$report(pp)$log_inccurve))
## plot estimated inc curve at the starting values
lines(dd$time,exp(m$report(unlist(params))$log_inccurve),col=2)
## get CIs from sdreport (delta method)
with(sdreport(m),
     matlines(dd$time,exp(cbind(value,value-1.96*sd,
                            value+1.96*sd)),lty=c(1,2,2),
                          col=1))
legend("topleft",lty=c(1,1,2), bty="n",
       col=c(1,2,1),
       legend=c("fit","initial params","95% CI (Wald/delta)"))
```


The bump at the second time step occurs because the elapsed time is twice as long, so the
expected incidence is (approximately) doubled. We don't know why 7 March 2020 is actually
missing in the data, but the model is behaving consistently ...

```{r tidy_def,echo=FALSE}
## some utilities
class(m) <- c("TMB", class(m))
tidyfun <- function(x,...) {
    (tidy(x,...)
        %>% as_tibble()
        %>% select(-std.error)
        %>% mutate_if(is.numeric,exp)
        %>% mutate_if(is.character,~gsub("log_","",.))
    )
}
```

Comparing confidence intervals: all still a bit fragile ...

```{r tidy,warning=FALSE}
ci_methods <- c("wald","uniroot","profile")
tt <- purrr::map_dfr(setNames(ci_methods,ci_methods),
              ~tidyfun(m, conf.int=TRUE, conf.method=.),
              .id="ci_method") %>%
    mutate_at("conf.low", replace_na, 0) %>%
    mutate_at("conf.high", replace_na, Inf)

## https://github.com/tidyverse/ggplot2/issues/3617
pv <- position_dodgev(0.2)
ggplot(tt, aes(y=term,x=estimate,xmin=conf.low, xmax=conf.high, colour=ci_method)) +
    geom_pointrange(position=pv) +
    ## would like to draw half-
    ## geom_segment(aes(x=estimate,xend=conf.high,y=term,yend=term),
    ##  position=pv) +
    scale_x_log10() +
    facet_wrap(~term,scale="free")
```

CIs that go all the way to the edge of the plot are `NA`, either because the negative log-likelihood never goes above the critical value or because something else is wrong with the CI estimate ("uniroot" looks fragile ...)

## digression: negative binomial models

We would often like to fit models without knowing in advance whether a negative binomial or a Poisson response is more appropriate. In principle this is easy because the Poisson is *nested* within the negative binomial (i.e., NB converges to Poisson for a particular choice of models). However, there are several problems with this "easy" solution;

* in the standard parameterization the NB converges to Poisson when the overdispersion parameter goes to infinity. R can handle infinite values:

```{r}
identical(dpois(1,lambda=1),dnbinom(1,mu=1,size=Inf))
```

but it's not clear, even though C++ can in principle handle infinite values, that this would go smoothly.

* We could change the parameterization, i.e. fit the parameter on the inverse scale, so that it converges to Poisson when the parameter goes to 0 rather than infinity. However, we still have some problems: when a model converges to a parameter value on the boundary (i.e. only values of $f \ge 0$ make sense, not $f<0$), then a bunch of derivative-based machinery (such as the estimate of standard deviations based on the curvature of the log-likelihood surface) no longer works properly. We can use bounded (box-constrained) optimization to prevent the parameter from going negative, *or* we can fit the parameter on the log scale. However, if we fit $\log(f)$ instead of $f$, then we're still not out of the woods, because $\log(f) \to -\infty$ and we will still run into trouble with curvatures etc. 

* There's another problem: there's no guarantee that the negative binomial likelihood is *numerically* stable as the dispersion parameter goes to infinity.

```{r binomtest,echo=FALSE,message=FALSE,results="hide"}
compile("dnbinom_test.cpp")
dyn.load(dynlib("dnbinom_test"))
compile("dnbinom_robust_test.cpp")
dyn.load(dynlib("dnbinom_robust_test"))
set.seed(101)
x <- rpois(100, lambda=1)
m_nb <- MakeADFun(data=list(x=x), parameters=list(log_mu=0,inv_nbdisp=1),
                  DLL="dnbinom_test",
                  silent=TRUE)
m_nbr <- MakeADFun(data=list(x=x), parameters=list(log_mu=0,log_nbdisp=0),
                   DLL="dnbinom_robust_test",
                   silent=TRUE)
kvec <- 10^seq(12,4,length=501)
TMB_nbfun <- Vectorize(function(k) { m_nb$fn(c(0,1/k)) })
TMB_nbrfun <- Vectorize(function(k) { m_nbr$fn(c(0,log(k))) })
R_nbfun <- Vectorize(function(k) { -sum(dnbinom(x,mu=1,size=k,log=TRUE)) })
fun_list <- list(TMB_nb=TMB_nbfun,
                 TMB_nbr=TMB_nbrfun,
                 R_nb=R_nbfun)
dffuns <- map(fun_list,
              ~ function(kvec) data.frame(k=kvec, val=.(kvec)))
res <- purrr::map_dfr(dffuns,~.(kvec),.id="model") %>%
    mutate(val2=val+sum(dpois(x,lambda=1,log=TRUE)))
```

```{r dnbtest_plot1,echo=FALSE}
gg0 <- ggplot(res,aes(k,val2,colour=model)) +
    scale_x_log10() +
    geom_point() +
    geom_line() +
    scale_colour_discrete(labels=c("R, dnbinom2","TMB, dnbinom2","TMB dnbinom2_robust")) +
    labs(x="dispersion parameter",y="diff from Poisson neg log-likelihood") +
    geom_hline(yintercept=0,lty=2)
print(gg0)
```

Uh-oh ...
TMB has two different functions (`dnbinom2` and `dnbinom_robust`) for parameterizing the NB by mean and variance; the first uses mean and variance, the second uses *log* mean and *log excess* variance (the variance is $\mu(1+\mu/k)$, the excess variance is $\mu^2/k$).

Zoom in on a smaller part of the range and limit the y-axis scale.

```{r dnbtest_plot3,echo=FALSE}
rng2 <- res %>% filter(model=="R_nb") %>% pull(val2) %>% range()
gg0 %+% (dplyr::filter(res, k<1e8)) + scale_y_continuous(limits=rng2,oob=scales::squish)
```

Zoom in farther, and leave out `dnbinom2` entirely ...

```{r dnbtest_plot4,echo=FALSE}
gg0 %+% (dplyr::filter(res, 1e6<k, k<1e8, model != "TMB_nb"))
```

If you want to look [inside the R code](https://github.com/wch/r-source/blob/f8d4d7d48051860cc695b99db9be9cf439aee743/src/nmath/dnbinom.c#L80-L96) you can see the careful processing that R does to try to get accurate answers across a huge range of parameter scales ... all TMB does, in contrast, is to carefully translate from the log mu/log excess var scale ... see [here](https://github.com/kaskr/adcomp/blob/12b47bec0391ce48b04475c1f814856210f204a0/TMB/inst/include/tiny_ad/robust/distributions.hpp#L26-L63).

Bottom line: even with `dnbinom_robust`, in cases where we think we are approaching Poisson, we will have to set a threshold and switch to Poisson if the dispersion parameter is unrealistically large



## to do / fix me

- check Richards parameterization (esp., make sure that our value of $r$ corresponds to growth rate at zero); re-read @Ma+14, @Smir+17?
- discuss:
    - pros and cons of `bbmle`
    - delta method vs likelihood profiles vs. importance sampling
- blue sky:
    - process error via (extended) Kalman filter
    - *generalized* Richards?
- compute $R_0$



## References
