\documentclass[12pt]{article}

\usepackage[top=1in,bottom=1.5in,left=1in,right=1in]{geometry}
\usepackage{amsmath,amssymb}
\allowdisplaybreaks
\usepackage[colorlinks=true,allcolors=magenta]{hyperref}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\newcommand{\transpose}[1]{#1^{\mathrm{T}}}

\begin{document}
\setlength{\parskip}{5mm}
\setlength{\parindent}{0mm}

\section{Correlation matrix distributions}

Let $X$ and $S$ be $n \times n$ real symmetric positive definite matrices
with unit diagonal elements, and let $\nu \in (n - 1, \infty)$.

\subsection{Lewandowski-Kurowicka-Joe (LKJ) distribution}

If $X$ follows an
\href{https://mc-stan.org/docs/2_27/functions-reference/lkj-correlation.html}{LKJ distribution}
with shape $\eta > 0$,
\begin{equation}
X \sim \mathrm{LKJ}(\eta)\,,
\end{equation}
then its probability density is
\begin{equation}
f(X;\eta) = g(\eta) \det(X)^{\eta - 1}\,,
\end{equation}
where $g(\eta)$ is a normalization constant.

If $\eta = 1$, then the distribution is uniform on the space of
$n \times n$ real symmetric positive definite matrices with unit
diagonal elements.
%
If $\eta > 1$, then the density is an increasing function of $\det(X)$,
attaining a maximum at $\det(X) = 1$ (the identity matrix $I_n$)
and approaching $0$ as $\det(X) \to 0$.
%
If $\eta < 1$, then the density is a decreasing function of $\det(X)$,
attaining a minimum at $\det(X) = 1$
and approaching $\infty$ as $\det(X) \to 0$.

\subsection{Computation}

Note that $X$ can be factorized as
\begin{equation}
X = D^{-1/2} L \transpose{L} D^{-1/2}\,,
\end{equation}
where $L$ is a lower triangular matrix with unit diagonal elements
and
\begin{equation}
D = \diag(\diag(L \transpose{L}))\,.
\end{equation}
If the normalization constant can be ignored,
then computing $f(X;\eta)$ amounts to computing $\det(X)$.
Recalling that
$\det(A B) = \det(A) \det(B)$ for matrices $A$ and $B$
and that
$\det(C) = \prod_{k} (C)_{k,k}$ for triangular matrices $C$,
one can write
\begin{equation}
\begin{aligned}
\det(X)
  &= \det(D^{-1/2}) \det(L) \det(\transpose{L}) \det(D^{-1/2}) \\
  &= \det(D)^{-1/2} \cdot 1 \cdot 1 \cdot \det(D)^{-1/2} \\
  &= \det(D)^{-1}\,.
\end{aligned}
\end{equation}


\section{Covariance matrix distributions}

Let $X$ and $S$ be $n \times n$ real symmetric positive definite matrices,
and let $\nu \in (n - 1, \infty)$.

\subsection{Wishart distribution}

If $X$ follows a
\href{https://mc-stan.org/docs/2_27/functions-reference/wishart-distribution.html}{Wishart distribution}
with degrees of freedom $\nu$ and scale $S$,
\begin{equation}
X \sim \mathrm{Wishart}(\nu,S)\,,
\end{equation}
then its probability density is
\begin{equation}
f(X;\nu,S) = \frac{\det(S)^{-\nu / 2} \det(X)^{-(-\nu + n + 1) / 2}}{2^{n \nu / 2} \Gamma_{n}(\nu / 2)} \exp\Big(-\frac{1}{2} \tr(S^{-1} X)\Big)\,,
\end{equation}
where $\Gamma_{n}$ denotes the
\href{https://en.wikipedia.org/wiki/Multivariate_gamma_function}{$n$-variate gamma function},
given by
\begin{equation}
\Gamma_{n}(z) = \pi^{n (n - 1) / 4} \prod_{i=0}^{n-1} \Gamma\Big(z - \frac{i}{2}\Big)\,,\qquad \Re(z) > \frac{1}{2}(n - 1)\,.
\end{equation}

\subsection{Inverse Wishart distribution}

If $X$ follows an
\href{https://mc-stan.org/docs/2_27/functions-reference/inverse-wishart-distribution.html}{inverse Wishart distribution}
with degrees of freedom $\nu$ and scale $S$,
\begin{equation}
X \sim \mathrm{InverseWishart}(\nu,S)\,,
\end{equation}
then its probability density is
\begin{equation}
f(X;\nu,S) = \frac{\det(S)^{\nu / 2} \det(X)^{-(\nu + n + 1) / 2}}{2^{n \nu / 2} \Gamma_{n}(\nu / 2)} \exp\Big(-\frac{1}{2} \tr(S X^{-1})\Big)\,.
\end{equation}

\subsection{Computation}

Note that $X$ and $S$ can be factorized as
\begin{equation}
X = D_{\sigma,X} D_{\theta,X}^{-1/2} L_{X} \transpose{L_{X}} D_{\theta,X}^{-1/2} D_{\sigma,X}\,,\qquad S = D_{\sigma,S} D_{\theta,S}^{-1/2} L_{S} \transpose{L_{S}} D_{\theta,S}^{-1/2} D_{\sigma,S}\,,
\end{equation}
where $L_{X}$ and $L_{S}$ are lower triangular matrices
with unit diagonal elements,
\begin{equation}
D_{\sigma,X} = \diag(\diag(X))^{1/2}\,,\qquad D_{\sigma,S} = \diag(\diag(S))^{1/2}\,,
\end{equation}
and
\begin{equation}
D_{\theta,X} = \diag(\diag(L_{X} \transpose{L_{X}}))\,,\qquad D_{\theta,S} = \diag(\diag(L_{S} \transpose{L_{S}}))\,.
\end{equation}

To compute $\det(X)$ and $\det(S)$, recall that
$\det(A B) = \det(A) \det(B)$ for matrices $A$ and $B$
and that
$\det(C) = \prod_{k} (C)_{k,k}$ for triangular matrices $C$.
Hence
\begin{equation}
\begin{aligned}
\det(X)
  &= \det(D_{\sigma,X}) \det(D_{\theta,X}^{-1/2}) \det(L_{X}) \det(\transpose{L_{X}}) \det(D_{\theta,X}^{-1/2}) \det(D_{\sigma,X}) \\
  &= \det(D_{\sigma,X}) \det(D_{\theta,X})^{-1/2} \cdot 1 \cdot 1 \cdot \det(D_{\theta,X})^{-1/2} \det(D_{\sigma,X}) \\
  &= \det(D_{\sigma,X})^{2} \det(D_{\theta,X})^{-1}\,.
\end{aligned}
\end{equation}
Similarly,
\begin{equation}
\det(S) = \det(D_{\sigma,S})^{2} \det(D_{\theta,S})^{-1}\,.
\end{equation}

To compute $\tr(S^{-1} X)$, recall that
\begin{equation}
\tr(A B) = \sum_{i,j} (A \odot \transpose{B})_{i,j} = \sum_{i,j} (\transpose{A} \odot B)_{i,j}\,,
\end{equation}
where $\odot$ denotes elementwise multiplication. Hence
\begin{equation}
\begin{aligned}
\tr(S^{-1} X)
  &= \sum_{i,j} (S^{-1} \odot \transpose{X})_{i,j} \\
  &= \sum_{i,j} (S^{-1} \odot X)_{i,j} \\
  &= \sum_{i,j} (D_{\sigma,S}^{-1} D_{\theta,S}^{1/2} \transpose{(L_{S}^{-1})} L_{S}^{-1} D_{\theta,S}^{1/2} D_{\sigma,S}^{-1} \odot D_{\sigma,X} D_{\theta,X}^{-1/2} L_{X} \transpose{L_{X}} D_{\theta,X}^{-1/2} D_{\sigma,X})_{i,j} \\
  &= \sum_{i,j} (D_{1} A_{1} \transpose{D_{1}})_{i,j}\,,
\end{aligned}
\end{equation}
where
\begin{equation}
A_{1} = \transpose{(L_{S}^{-1})} L_{S}^{-1} \odot L_{X} \transpose{L_{X}}\,,\qquad D_{1} = D_{\sigma,S}^{-1} D_{\theta,S}^{1/2} D_{\sigma,X} D_{\theta,X}^{-1/2}\,.
\end{equation}

Similarly,
\begin{equation}
\begin{aligned}
\tr(S X^{-1})
  &= \sum_{i,j} (\transpose{S} \odot X^{-1})_{i,j} \\
  &= \sum_{i,j} (S \odot X^{-1})_{i,j} \\
  &= \sum_{i,j} (D_{\sigma,S} D_{\theta,S}^{-1/2} L_{S} \transpose{L_{S}} D_{\theta,S}^{-1/2} D_{\sigma,S} \odot D_{\sigma,X}^{-1} D_{\theta,X}^{1/2} \transpose{(L_{X}^{-1})} L_{X}^{-1} D_{\theta,X}^{1/2} D_{\sigma,X}^{-1})_{i,j} \\
  &= \sum_{i,j} (D_{2} A_{2} \transpose{D_{2}})_{i,j}\,,
\end{aligned}
\end{equation}
where
\begin{equation}
A_{2} = L_{S} \transpose{L_{S}} \odot \transpose{(L_{X}^{-1})} L_{X}^{-1}\,,\qquad D_{2} = D_{\sigma,S} D_{\theta,S}^{-1/2} D_{\sigma,X}^{-1} D_{\theta,X}^{1/2}\,.
\end{equation}

\end{document}
